<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <!--头部信息-->
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <!--title keywords description 请改为自己的-->
    <title>低调奋进</title>

    <!--网站favicon可以没有或者改为自己的-->
    <!--<link rel="shortcut icon" type="image/x-icon" href="http://www.bituplink.com/wp-content/uploads/favicon.png"/>-->

    <!--CSS 若不需要变动样式不用改-->
    <link href="plugin/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/zui/1.8.1/css/zui.min.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" type="text/css" href="../css/common.css" />
    <link href="../img/logo.ico" rel="shortcut icon" />
    <script src="plugin/jquery.min.js"></script>
    <script src="plugin/bootstrap/js/bootstrap.min.js"></script>
</head>
<body id="nav_body">
<!--[if lt IE 10]>
<div class="alert alert-danger">
    您正在使用 
    <strong>过时的</strong> 浏览器. 请更换一个更好的浏览器来提升用户体验.
</div>
<![endif]--><!--头部导航条-->
<div id="content">
    <div class="w_header">
      <div class="container">
        <div class="w_header_top">
          <a href="../index.html" class="w_logo"></a>
          <span class="w_header_nav">
              <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="speech.html" class="active">Speech & ML</a></li>
                <li><a href="pro.html">Programming</a></li>
                <li><a href="moodList.html">Life</a></li>
                <li><a href="tools.html">Tool</a></li>
                <li><a href="about.html">About</a></li>
            </ul>
        </span>
    </div>
</div>
</div>

<!--左侧Director，导航跳转-->
<div class="left-bar">
    <div class="header">
        <h2>Director</h2>
    </div>
    <div class="menu" id="menu">
        <ul class="scrollcontent">
            <!--左侧Director，按照需要修改和添加，参考已有的修改名称和href-->
            <li><a href="#row-1">Hybrid ASR & General</a></li>
            <li><a href="#row-2">RNN-T</a></li>
            <li><a href="#row-3">CTC</a></li>
            <li><a href="#row-4">AED</a></li>
            <li><a href="#row-5">Unified & Rescoing</a></li>
            <li><a href="#row-6">Data Aug</a></li>
            <li><a href="#row-7">Other</a></li>
        </ul>
    </div>
</div>
<!--内容-->
<div class="main">
    <div class="container content-box">
        <!--导航分类范例1，请根据自己的需求进行修改-->
        <section class="item card-box" id="row-1">
            <div class="container-fluid">
                <div class="row">
                    <div class="item-tit">
                        <strong>Journal and conference on speech</strong>
                        <table width="1150" border="1">
                            <tr>
                                <td width="150" align="center">CCF-A</a></td>
                                <td width="1000">NeuraIPS&nbsp;&nbsp;&nbsp;AAAI&nbsp;&nbsp;&nbsp;IJAI&nbsp;&nbsp;&nbsp;ACMMM </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">CCF-B</a></td>
                                <td width="1000">ICASSP&nbsp;&nbsp;&nbsp;COLING&nbsp;&nbsp;&nbsp;SpeechCom&nbsp;&nbsp;&nbsp;TSLP&nbsp;&nbsp;&nbsp;TASLP&nbsp;&nbsp;&nbsp;JSLHR&nbsp;&nbsp;&nbsp;TMM&nbsp;&nbsp;&nbsp;TOMCCAP&nbsp;&nbsp;&nbsp;ICME </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">CCF-C</a></td>
                                <td width="1000">INTERSPEECH&nbsp;&nbsp;&nbsp;ICPR </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">other</a></td>
                                <td width="1000">ICLR </td>
                            </tr>
                        </table>
                    </div>
                    <div class="item-tit">
                        <strong>Hybrid & General ASR</strong>
                    </div>
                    <!--获取内容列表-->
                    <h3> 2021 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">The History of Speech Recognition to the Year 2030</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.00084.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Multilingual Speech Recognition using Knowledge Transfer across Learning Processes</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Efficient domain adaptation of language models in ASR systems using Prompt-tuning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06502.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Word Order Does Not Matter For Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05994.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05354.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Interactive Feature Fusion for End-to-End Noise-Robust Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05267.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04612.pdf">pdf</a></td>
                        </tr>
                     </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">On the Comparison of Popular End-to-End Models for Large Scale Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.14327.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Conformer: Convolution-augmented Transformer for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.08100v1.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.03191.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Improved Noisy Student Training for Automatic Speech Recognition(</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.09629.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">CIF: Continuous Integrate-And-Fire for End-To-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1905.11235.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">A Comparison of Label-Synchronous and Frame-Synchronous End-to-End Models for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.10113.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Imputer: Sequence modelling via imputation and dynamic programming</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2002.08926.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Automatic Speech Recognition Errors Detection and Correction: A Review</td>
                            <td width="200"><a href="http://icnlsp.org/IMG/pdf/-12.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">A review of on-device fully neural end-to-end automatic speech recognition algorithms </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.07974.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Accelerating recurrent neural network language model based online speech recognition system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1801.09866.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Towards Language-Universal End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1711.02207.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Reducing Bias in Production Speech Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1705.04400.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1711.05747.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
        </div>
    </div>
</section>
<section class="item card-box" id="row-2">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>RNN-T</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Improved Neural Language Model Fusion for Streaming Recurrent Neural Network Transducer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.13878.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Streaming End-to-End Multi-Talker Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.13148.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800"> A Better and Faster End-to-End Model for Streaming ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.10798.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Tied & Reduced RNN-T Decoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.07513.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Tiny Transducer: A Highly-efficient Speech Recognition Model on Edge Devices </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.06856.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800"> Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin Speech Recognition with a Syllable-to-Character Converter</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08469.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">On Language Model Integration for RNN Transducer based Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06841.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">RNN-T For Latency Controlled ASR With Improved Beam Search </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1911.01629.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Transformer Transducer: A Streamable Speech Recognition Model With Transformer Encoders And RNN-T Loss</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2002.02562.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">A Streaming On-Device End-to-End Model Surpassing Server-Side Conventional Model Quality and Latency</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2003.12710.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Towards Fast And Accurate Streaming E2E ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2004.11544.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Knowledge Distillation from Offline to Streaming RNN Transducer for End-to-end Speech Recognition</td>
                            <td width="200"><a href="https://indico2.conference4me.psnc.pl/event/35/contributions/3144/attachments/457/482/Wed-1-5-3.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800"> Transfer Learning Approaches for Streaming End-to-End Speech Recognition System</td>
                            <td width="200"><a href=https://arxiv.org/pdf/2008.05086.pdf"">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Analyzing the Quality and Stability of a Streaming End-to-End On-Device Speech Recognizer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.01416.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Alignment Restricted Streaming Recurrent Neural Network Transducer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.03072.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Benchmarking LF-MMI, CTC and RNN-T Criteria for Streaming ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.04785.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Improving RNN transducer with normalized jointer network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.01576.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800"> Improved Neural Language Model Fusion for Streaming Recurrent Neural Network Transducer </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.13878.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Improving Streaming Automatic Speech Recognition With Non-Streaming Model Distillation On Unsupervised Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.12096.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.11148.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Analyzing the Quality and Stability of a Streaming End-to-End On-Device Speech Recognizer </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.01416.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Parallel Rescoring with Transformer for Streaming On-Device Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2008.13093.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Self-Attention Transducers for End-to-End Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1909.13037.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Streaming E2E Speech Recognition For Mobile Devices</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1811.06621.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-3">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>CTC</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Why does CTC result in peaky behavior?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.14849.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.15025.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">CASS-NAT: CTC Alignment-based Single Step Non-autoregressive Transformer for Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.14725.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800"> Improved Mask-CTC for Non-Autoregressive End-to-End ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.13270.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800"> Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.08700.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Automatic Spelling Correction with Transformer for CTC-based End-to-End Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1904.10045.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">An improved hybrid CTC-Attention model for speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1810.12020.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Residual Convolutional CTC Networks for Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1702.07793.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1703.00096.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-4">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>AED</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05571.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Emformer: Efficient Memory Transformer Based Acoustic Model For Low Latency Streaming Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.10759.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">High Performance Sequence-to-Sequence Model for Streaming Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2003.10022.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Streaming Chunk-Aware Multihead Attention for Online End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.01712.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Streaming Transformer-based Acoustic Models Using Self-attention with Augmented Memory </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.08042.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">CTC-synchronous Training for Monotonic Attention Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.04712.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">Low Latency End-to-End Streaming Speech Recognition with a Scout Network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2003.10369.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</td>
                            <td width="800">Synchronous Transformers For E2E Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1912.02958.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</td>
                            <td width="800">Transformer Online CTC/Attention E2E Speech Recognition Architecture</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2001.08290.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</td>
                            <td width="800">Streaming Automatic Speech Recognition With The Transformer Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2001.02674.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</td>
                            <td width="800">Minimum Latency Training Strategies For Streaming seq-to-seq ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2004.05009.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</td>
                            <td width="800">Enhancing Monotonic Multihead Attention for Streaming ASR </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.09394.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00120.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Insertion-Based Modeling for End-to-End Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.13211.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.07903.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Listen Attentively, and Spell Once: Whole Sentence Generation via a Non-Autoregressive Architecture for Low-Latency Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.04862.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1910.13923.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Streaming Transformer ASR with Blockwise Synchronous Inference </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.14941.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Triggered Attention for End-to-End Speech Recognition </td>
                            <td width="200"><a href="https://www.merl.com/publications/docs/TR2019-015.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Listen and Fill in the Missing Letters: Non-Autoregressive Transformer for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1911.04908.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800"> Spelling Correction Model For E2E Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1902.07178.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800"> An Empirical Study Of Efficient ASR Rescoring With Transformers </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1910.11450.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Correction of Automatic Speech Recognition with Transformer Sequence-To-Sequence Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1910.10697.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">State-of-the-art Speech Recognition With Sequence-to-Sequence Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1712.01769.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Montonic Chunkwise Attention</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1712.05382.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Multilingual Speech Recognition With A Single End-To-End Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1711.01694.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Attention-Based End-to-End Speech Recognition in Mandarin</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1707.07167.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping</td>
                            <td width="200"><a href="https://www.isca-speech.org/archive_v0/Interspeech_2017/pdfs/1705.PDF">pdf</a></td>
                        </tr>
             </table>
            <h3> 2016 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1609.03193.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2015 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1508.01211.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-5">
    <div class="container-fluid">
        </div>
    </div>
</section>
<section class="item card-box" id="row-5">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Unified & Rescoring</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.05481.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">One In A Hundred: Select The Best Predicted Sequence from Numerous Candidates for Streaming Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.14791.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Have best of both worlds: two-pass hybrid and E2E cascading framework for speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04891.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.01547.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.05642.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Transformer Transducer: One Model Unifying Streaming And Non-Streaming Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.03192.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Universal ASR: Unify And Improve Streaming ASR With Full-Context Modeling </td>
                            <td width="200"><a href="https://openreview.net/pdf?id=Pz_dcqfcKW8">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Cascaded encoders for unifying streaming and non-streaming ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.14606.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Dynamic latency speech recognition with asynchronous revision</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.01570.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.05481.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Hybrid CTC-Attention based End-to-End Speech Recognition using Subword Units</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1807.04978.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1706.02737.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-6">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Data Aug</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">MixSpeech: Data Augmentation for Low-resource Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.12664.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800"></td>
                            <td width="200"><a href="">pdf</a></td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1904.08779.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-7">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Other</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06909.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>

<!--页脚-->
<footer class="footer">
    <div class="container">
        <div class="rwo">
            <div class="col-md-12">
                <p>
                    本站内容源自互联网，如有内容侵犯了你的权益，请联系删除相关内容，联系邮箱：yongqiangli@alumni.hust.edu.cn
                </p>
                <!--代码源自小呆导航的开源代码，遵循MIT协议，此处保留源代码的声明-->
                <p>
                    Copyright © 2018-2021 li yongqiang All Rights Reserved
                </p>
            </div>
        </div>
    </div>
</footer>
</div>
<!--内容区域-->
</div>
<div id="get-top" title="回到顶部">
    <i class="icon icon-arrow-up"></i>
</div>

<!-- jQuery (ZUI中的Javascript组件依赖于jQuery) -->
<script src="http://code.jquery.com/jquery-1.11.0.min.js"></script>

<script>
    window.onscroll = function(){
//回到顶部
var sllTop = document.documentElement.scrollTop||document.body.scrollTop;
if(sllTop>240){
  $('#get-top').css('display','block')
}else{
  $('#get-top').css('display','none')
}
}
$('#get-top').click(function(){ 
  $('body,html').animate({
    scrollTop: 0
  }, 800);//点击回到顶部按钮，数字越小越快
})
//判断用户使用的设备
var deviceVal  = browserRedirect();
function browserRedirect() {
  var sUserAgent = navigator.userAgent.toLowerCase();
  var bIsIpad = sUserAgent.match(/ipad/i) == "ipad";
  var bIsIphoneOs = sUserAgent.match(/iphone os/i) == "iphone os";
  var bIsMidp = sUserAgent.match(/midp/i) == "midp";
  var bIsUc7 = sUserAgent.match(/rv:1.2.3.4/i) == "rv:1.2.3.4";
  var bIsUc = sUserAgent.match(/ucweb/i) == "ucweb";
  var bIsAndroid = sUserAgent.match(/android/i) == "android";
  var bIsCE = sUserAgent.match(/windows ce/i) == "windows ce";
  var bIsWM = sUserAgent.match(/windows mobile/i) == "windows mobile";
  if (bIsIpad || bIsIphoneOs || bIsMidp || bIsUc7 || bIsUc || bIsAndroid || bIsCE || bIsWM) {
    return 'phone';
} else {
    return 'pc';
}
}
$('.nav-btn').on('click', function () {
    $('.nav').toggleClass('showNav');
    $(this).toggleClass('animated2');
});

</script>
</div>
</body>
</html>
