<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <!--头部信息-->
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <!--title keywords description 请改为自己的-->
    <title>低调奋进</title>

    <!--网站favicon可以没有或者改为自己的-->
    <!--<link rel="shortcut icon" type="image/x-icon" href="http://www.bituplink.com/wp-content/uploads/favicon.png"/>-->

    <!--CSS 若不需要变动样式不用改-->
    <link href="plugin/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/zui/1.8.1/css/zui.min.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" type="text/css" href="../css/common.css" />
    <link href="../img/logo.ico" rel="shortcut icon" />
    <script src="plugin/jquery.min.js"></script>
    <script src="plugin/bootstrap/js/bootstrap.min.js"></script>
</head>
<body id="nav_body">
<!--[if lt IE 10]>
<div class="alert alert-danger">
    您正在使用 
    <strong>过时的</strong> 浏览器. 请更换一个更好的浏览器来提升用户体验.
</div>
<![endif]--><!--头部导航条-->
<div id="content">
    <div class="w_header">
      <div class="container">
        <div class="w_header_top">
          <a href="../index.html" class="w_logo"></a>
          <span class="w_header_nav">
              <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="speech.html" class="active">Speech & ML</a></li>
                <li><a href="pro.html">Programming</a></li>
                <li><a href="moodList.html">Life</a></li>
                <li><a href="tools.html">Tool</a></li>
                <li><a href="about.html">About</a></li>
            </ul>
        </span>
    </div>
</div>
</div>

<!--左侧Director，导航跳转-->
<div class="left-bar">
    <div class="header">
        <h2>Director</h2>
    </div>
    <div class="menu" id="menu">
        <ul class="scrollcontent">
            <!--左侧Director，按照需要修改和添加，参考已有的修改名称和href-->
            <li><a href="#row-1">Hybrid ASR & General</a></li>
            <li><a href="#row-2">RNN-T</a></li>
            <li><a href="#row-3">CTC</a></li>
            <li><a href="#row-4">AED</a></li>
            <li><a href="#row-5">Unified & Rescoring</a></li>
            <li><a href="#row-6">Data Aug</a></li>
            <li><a href="#row-7">LM</a></li>
            <li><a href="#row-8">Unsupervised</a></li>
            <li><a href="#row-9">Multilingual</a></li>
            <li><a href="#row-10">Personal</a></li>
            <li><a href="#row-11">Accent</a></li>
            <li><a href="#row-12">Dataset</a></li>
            <li><a href="#row-13">Rebust</a></li>
            <li><a href="#row-14">Other</a></li>
        </ul>
    </div>
</div>
<!--内容-->
<div class="main">
    <div class="container content-box">
        <!--导航分类范例1，请根据自己的需求进行修改-->
        <section class="item card-box" id="row-1">
            <div class="container-fluid">
                <div class="row">
                    <div class="item-tit">
                        <strong>Journal and conference on speech</strong>
                        <table width="1150" border="1">
                            <tr>
                                <td width="150" align="center">CCF-A</a></td>
                                <td width="1000">NeuraIPS&nbsp;&nbsp;&nbsp;AAAI&nbsp;&nbsp;&nbsp;IJAI&nbsp;&nbsp;&nbsp;ACMMM </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">CCF-B</a></td>
                                <td width="1000">ICASSP&nbsp;&nbsp;&nbsp;COLING&nbsp;&nbsp;&nbsp;SpeechCom&nbsp;&nbsp;&nbsp;TSLP&nbsp;&nbsp;&nbsp;TASLP&nbsp;&nbsp;&nbsp;JSLHR&nbsp;&nbsp;&nbsp;TMM&nbsp;&nbsp;&nbsp;TOMCCAP&nbsp;&nbsp;&nbsp;ICME </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">CCF-C</a></td>
                                <td width="1000">INTERSPEECH&nbsp;&nbsp;&nbsp;ICPR </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">other</a></td>
                                <td width="1000">ICLR </td>
                            </tr>
                        </table>
                    </div>
                    <div class="item-tit">
                        <strong>Hybrid & General ASR</strong>
                    </div>
                    <!--获取内容列表-->
                    <h3> 2021 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">The History of Speech Recognition to the Year 2030</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.00084.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Multilingual Speech Recognition using Knowledge Transfer across Learning Processes</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Efficient domain adaptation of language models in ASR systems using Prompt-tuning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06502.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Word Order Does Not Matter For Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05994.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05354.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Interactive Feature Fusion for End-to-End Noise-Robust Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05267.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04612.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">A Study of Low-Resource Speech Commands Recognition based on Adversarial Reprogramming</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03894.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">AequeVox: Automated Fairness Testing of Speech Recognition Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09843.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04590.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">synchronous Decentralized Distributed Training of Acoustic Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.11199.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Beyond Lp clipping: Equalization-based Psychoacoustic Attacks against ASRs</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.13250.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Continual learning using lattice-free MMI for speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07055.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Explaining the Attention Mechanism of End-to-End Speech Recognition Using Decision Trees</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03879.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Exploring Heterogeneous Characteristics of Layers in ASR Models for More Efficient Training</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04267.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.14420.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">improving Character Error Rate Is Not Equal to Having Clean Speech: Speech Enhancement for ASR Systems with Black-box Acoustic Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05968.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03327.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Improving Pseudo-label Training For End-to-end Speech Recognition Using Gradient Mask</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04056.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Integrating Categorical Features in End-to-End ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03047.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Interactive Feature Fusion for End-to-End Noise-Robust Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05267.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Multi-Modal Pre-Training for Automated Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09890.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08352.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">Optimizing Alignment of Speech and Language Latent Spaces for End-to-End Speech Recognition and Understanding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.12138.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">Parallel Composition of Weighted Finite-State Transducers</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02848.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">SCaLa: Supervised Contrastive Learning for End-to-End Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04187.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Speech Pattern based Black-box Model Watermarking for Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09814.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.00678.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">Spell my name: keyword boosted speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02791.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Towards efficient end-to-end speech recognition with biologically-inspired neural networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02743.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03151.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs for Robust Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04934.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">Word Order Does Not Matter For Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05994.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Voice Conversion Can Improve ASR in Very Low-Resource Settings</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.02674.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">Towards Building ASR Systems for the Next Billion Users</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.03945.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">Scaling ASR Improves Zero and Few Shot Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.05948.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">Romanian Speech Recognition Experiments from the ROBIN Project</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.12028.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">Retrieving Speaker Information from Personalized Acoustic Models for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.04194.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">Recent Advances in End-to-End Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.01690.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">40</a></td>
                            <td width="800">Privacy attacks for automatic speech recognition acoustic models in a federated learning framework</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.03777.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">41</a></td>
                            <td width="800">Multi-Channel Multi-Speaker ASR Using 3D Spatial Feature</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.11023.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">42</a></td>
                            <td width="800">Mixed Precision DNN Qunatization for Overlapped Speech Separation and Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.14479.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">43</a></td>
                            <td width="800">Integrated Semantic and Phonetic Post-correction for Chinese Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.08400.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">44</a></td>
                            <td width="800">Effect of noise suppression losses on speech distortion and ASR performance</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.11606.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">45</a></td>
                            <td width="800">Do We Still Need Automatic Speech Recognition for Spoken Language Understanding?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.14842.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">46</a></td>
                            <td width="800">Conformer-based Hybrid ASR System for Switchboard Dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.03442.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">47</a></td>
                            <td width="800">A comparison of streaming models and data augmentation methods for robust speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10043.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">48</a></td>
                            <td width="800">Are E2E ASR models ready for an industrial usage?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.12572.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">49</a></td>
                            <td width="800">Voice Quality and Pitch Features in Transformer-Based Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.11391.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">50</a></td>
                            <td width="800">Investigation of Densely Connected Convolutional Networks with Domain Adversarial Learning for Noise Robust Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.10108.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">51</a></td>
                            <td width="800">Continual Learning for Monolingual End-to-End Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.09427.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">52</a></td>
                            <td width="800">Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.08718.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">53</a></td>
                            <td width="800">Speech frame implementation for speech analysis and recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.08027.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">54</a></td>
                            <td width="800">Improving Speech Recognition on Noisy Speech via Speech Enhancement with Multi-Discriminators CycleGAN</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.06309.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">55</a></td>
                            <td width="800">Directed Speech Separation for Automatic Speech Recognition of Long Form Conversational Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.05863.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">56</a></td>
                            <td width="800">Revisiting the Boundary between ASR and NLU in the Age of Conversational Dialog Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.05842.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">57</a></td>
                            <td width="800">Training end-to-end speech-to-text models on mobile phones</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.03871.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">58</a></td>
                            <td width="800">Robust Speech Representation Learning via Flow-based Embedding Regularization</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.03454.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">59</a></td>
                            <td width="800">A Mixture of Expert Based Deep Neural Network for Improved ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.01025.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">60</a></td>
                            <td width="800">A higher order Minkowski loss for improved prediction ability of acoustic model in ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.01023.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">61</a></td>
                            <td width="800">X-Vector based voice activity detection for multi-genre broadcast speech-to-text</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.05016.pdf">pdf</a></td>
                        </tr>
                     </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">On the Comparison of Popular End-to-End Models for Large Scale Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.14327.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Conformer: Convolution-augmented Transformer for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.08100v1.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.03191.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Improved Noisy Student Training for Automatic Speech Recognition(</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.09629.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">CIF: Continuous Integrate-And-Fire for End-To-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1905.11235.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">A Comparison of Label-Synchronous and Frame-Synchronous End-to-End Models for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.10113.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Imputer: Sequence modelling via imputation and dynamic programming</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2002.08926.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Automatic Speech Recognition Errors Detection and Correction: A Review</td>
                            <td width="200"><a href="http://icnlsp.org/IMG/pdf/-12.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">A review of on-device fully neural end-to-end automatic speech recognition algorithms </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.07974.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Accelerating recurrent neural network language model based online speech recognition system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1801.09866.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Towards Language-Universal End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1711.02207.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Reducing Bias in Production Speech Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1705.04400.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1711.05747.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-2">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>RNN-T</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Improved Neural Language Model Fusion for Streaming Recurrent Neural Network Transducer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.13878.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Streaming End-to-End Multi-Talker Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.13148.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800"> A Better and Faster End-to-End Model for Streaming ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.10798.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Tied & Reduced RNN-T Decoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.07513.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Tiny Transducer: A Highly-efficient Speech Recognition Model on Edge Devices </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.06856.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800"> Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin Speech Recognition with a Syllable-to-Character Converter</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08469.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">On Language Model Integration for RNN Transducer based Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06841.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">A Unified Speaker Adaptation Approach for ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08545.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Factorized Neural Transducer for Efficient Language Model Adaptation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.01500.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Input Length Matters: An Empirical Study Of RNN-T And MWER Training For Long-form Telephony Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03841.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Knowledge Distillation for Neural Transducers from Large Self-Supervised Pre-trained Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03334.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">On Language Model Integration for RNN Transducer based Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06841.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Streaming Transformer Transducer Based Speech Recognition Using Non-Causal Convolution</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05241.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Word-level confidence estimation for RNN transducers</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.15222.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Sequence Transduction with Graph-based Supervision</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.01272.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Joint AEC AND Beamforming with Double-Talk Detection using RNN-Transformer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.04904.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Context-Aware Transformer Transducer for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.03250.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Deliberation of Streaming RNN-Transducer by Non-autoregressive Decoding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.11442.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Multi-turn RNN-T for streaming recognition of multi-party speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.10200.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Investigation of Training Label Error Impact on RNN-T</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.00350.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">RNN-T For Latency Controlled ASR With Improved Beam Search </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1911.01629.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Transformer Transducer: A Streamable Speech Recognition Model With Transformer Encoders And RNN-T Loss</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2002.02562.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">A Streaming On-Device End-to-End Model Surpassing Server-Side Conventional Model Quality and Latency</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2003.12710.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Towards Fast And Accurate Streaming E2E ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2004.11544.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Knowledge Distillation from Offline to Streaming RNN Transducer for End-to-end Speech Recognition</td>
                            <td width="200"><a href="https://indico2.conference4me.psnc.pl/event/35/contributions/3144/attachments/457/482/Wed-1-5-3.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800"> Transfer Learning Approaches for Streaming End-to-End Speech Recognition System</td>
                            <td width="200"><a href=https://arxiv.org/pdf/2008.05086.pdf"">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Analyzing the Quality and Stability of a Streaming End-to-End On-Device Speech Recognizer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.01416.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Alignment Restricted Streaming Recurrent Neural Network Transducer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.03072.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Benchmarking LF-MMI, CTC and RNN-T Criteria for Streaming ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.04785.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Improving RNN transducer with normalized jointer network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.01576.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800"> Improved Neural Language Model Fusion for Streaming Recurrent Neural Network Transducer </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.13878.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Improving Streaming Automatic Speech Recognition With Non-Streaming Model Distillation On Unsupervised Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.12096.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.11148.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Analyzing the Quality and Stability of a Streaming End-to-End On-Device Speech Recognizer </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.01416.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Parallel Rescoring with Transformer for Streaming On-Device Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2008.13093.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Self-Attention Transducers for End-to-End Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1909.13037.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Streaming E2E Speech Recognition For Mobile Devices</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1811.06621.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-3">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>CTC</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Why does CTC result in peaky behavior?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.14849.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.15025.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">CASS-NAT: CTC Alignment-based Single Step Non-autoregressive Transformer for Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.14725.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800"> Improved Mask-CTC for Non-Autoregressive End-to-End ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.13270.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">An Investigation of Enhancing CTC Model for Triggered Attention-based Streaming ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.10402.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Back from the future: bidirectional CTC decoding using future information in speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03326.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">CTC Variations Through New WFST Topologies</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03098.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Hierarchical Conditional End-to-End ASR with CTC and Multi-Granular Subword Units</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04109.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800"> Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.08700.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Automatic Spelling Correction with Transformer for CTC-based End-to-End Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1904.10045.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">An improved hybrid CTC-Attention model for speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1810.12020.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Residual Convolutional CTC Networks for Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1702.07793.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1703.00096.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-4">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>AED</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05571.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05172.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05571.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Attention based end to end Speech Recognition for Voice Search in Hindi and English</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10208.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">A Conformer-based ASR Frontend for Joint Acoustic Echo Cancellation, Speech Enhancement and Speech Separation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09935.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Consistent Training and Decoding For End-to-end Speech Recognition Using Lattice-free MMI</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.02498.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Emformer: Efficient Memory Transformer Based Acoustic Model For Low Latency Streaming Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.10759.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">High Performance Sequence-to-Sequence Model for Streaming Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2003.10022.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Streaming Chunk-Aware Multihead Attention for Online End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.01712.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Streaming Transformer-based Acoustic Models Using Self-attention with Augmented Memory </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.08042.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">CTC-synchronous Training for Monotonic Attention Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.04712.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">Low Latency End-to-End Streaming Speech Recognition with a Scout Network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2003.10369.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</td>
                            <td width="800">Synchronous Transformers For E2E Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1912.02958.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</td>
                            <td width="800">Transformer Online CTC/Attention E2E Speech Recognition Architecture</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2001.08290.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</td>
                            <td width="800">Streaming Automatic Speech Recognition With The Transformer Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2001.02674.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</td>
                            <td width="800">Minimum Latency Training Strategies For Streaming seq-to-seq ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2004.05009.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</td>
                            <td width="800">Enhancing Monotonic Multihead Attention for Streaming ASR </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.09394.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00120.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Insertion-Based Modeling for End-to-End Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.13211.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.07903.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Listen Attentively, and Spell Once: Whole Sentence Generation via a Non-Autoregressive Architecture for Low-Latency Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2005.04862.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1910.13923.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Streaming Transformer ASR with Blockwise Synchronous Inference </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.14941.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Triggered Attention for End-to-End Speech Recognition </td>
                            <td width="200"><a href="https://www.merl.com/publications/docs/TR2019-015.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Listen and Fill in the Missing Letters: Non-Autoregressive Transformer for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1911.04908.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800"> Spelling Correction Model For E2E Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1902.07178.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800"> An Empirical Study Of Efficient ASR Rescoring With Transformers </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1910.11450.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Correction of Automatic Speech Recognition with Transformer Sequence-To-Sequence Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1910.10697.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">State-of-the-art Speech Recognition With Sequence-to-Sequence Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1712.01769.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Montonic Chunkwise Attention</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1712.05382.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Multilingual Speech Recognition With A Single End-To-End Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1711.01694.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Attention-Based End-to-End Speech Recognition in Mandarin</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1707.07167.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping</td>
                            <td width="200"><a href="https://www.isca-speech.org/archive_v0/Interspeech_2017/pdfs/1705.PDF">pdf</a></td>
                        </tr>
             </table>
            <h3> 2016 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1609.03193.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2015 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1508.01211.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-5">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Unified & Rescoring</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.05481.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">One In A Hundred: Select The Best Predicted Sequence from Numerous Candidates for Streaming Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.14791.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Have best of both worlds: two-pass hybrid and E2E cascading framework for speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04891.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.01547.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.05642.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">An Investigation of Enhancing CTC Model for Triggered Attention-based Streaming ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.10402.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">ASR Rescoring and Confidence Estimation with ELECTRA</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.01857.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Have best of both worlds: two-pass hybrid and E2E cascading framework for speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04891.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Lattention: Lattice-attention in ASR rescoring</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10157.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Improving Hybrid CTC/Attention End-to-end Speech Recognition with Pretrained Acoustic and Language Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.07254.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</td>
                            <td width="800">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">ASR Rescoring and Confidence Estimation with ELECTRA</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.01857.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Have best of both worlds: two-pass hybrid and E2E cascading framework for speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04891.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Lattention: Lattice-attention in ASR rescoring</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10157.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Transformer Transducer: One Model Unifying Streaming And Non-Streaming Speech Recognition </td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.03192.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Universal ASR: Unify And Improve Streaming ASR With Full-Context Modeling </td>
                            <td width="200"><a href="https://openreview.net/pdf?id=Pz_dcqfcKW8">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Cascaded encoders for unifying streaming and non-streaming ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.14606.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Dynamic latency speech recognition with asynchronous revision</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.01570.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.05481.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Hybrid CTC-Attention based End-to-End Speech Recognition using Subword Units</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1807.04978.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1706.02737.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-6">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Data Aug</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">MixSpeech: Data Augmentation for Low-resource Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.12664.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Data Augmentation with Locally-time Reversed Speech for Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04511.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Significance of Data Augmentation for Improving Cleft Lip and Palate Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.00797.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Synt++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.11479.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Data Augmentation for Speech Recognition in Maltese: A Low-Resource Perspective</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07793.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Data Augmentation based Consistency Contrastive Pre-training for Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.12522.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">PM-MMUT: Boosted Phone-mask Data Augmentation using Multi-modeing Unit Training for Robust Uyghur E2E Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.06721.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800"></td>
                            <td width="200"><a href="">pdf</a></td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1904.08779.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-7">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>LM</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Private Language Model Adaptation for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.10026.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Disambiguation-BERT for N-best Rescoring in Low-Resource Conversational ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02267.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05354.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Learning Domain Specific Language Models for Automatic Speech Recognition through Machine Translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.10261.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Private Language Model Adaptation for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.10026.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">ViraPart: A Text Refinement Framework for ASR and NLP Tasks in Persian</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09086.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Conversational speech recognition leveraging effective fusion methods for cross-utterance language modeling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.03333.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Mixed Precision of Quantization of Transformer Language Models for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.11540.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Mixed Precision Low-bit Quantization of Neural Network Language Models for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.11438.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-8">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Unsupervised</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Private Language Model Adaptation for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.10026.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Analyzing the Robustness of Unsupervised Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03509.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Combining Unsupervised and Text Augmented Semi-Supervised Learning for Low Resourced Autoregressive Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.15836.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.00165.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Wav2vec-S: Semi-Supervised Pre-Training for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04484.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.13900.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Unsupervised Speech Enhancement with speech recognition embedding and disentanglement losses</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.08678.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Semi-supervised transfer learning for language expansion of end-to-end speech recognition models to low-resource languages</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10047.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Self-Supervised Learning for speech recognition with Intermediate layer supervision</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.08778.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-9">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Multilingual</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Magic dust for cross-lingual adaptation of monolingual wav2vec-2.0</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03560.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Mandarin-English Code-switching Speech Recognition with Self-supervised Speech Representation Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03504.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Minimum word error training for non-autoregressive Transformer-based code-switching ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03573.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Multilingual Speech Recognition using Knowledge Transfer across Learning Processes</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Joint Unsupervised and Supervised Training for Multilingual ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.08137.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Joint Modeling of Code-Switched and Monolingual ASR via Conditional Factorization</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.08137.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Bilingual Speech Recognition by Estimating Speaker Geometry from Video Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.13463.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Integrating Knowledge in End-to-End Automatic Speech Recognition for Mandarin-English Code-Switching</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.10202.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.05820.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-10">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Personal</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Fast Contextual Adaptation with Neural Associative Memory for On-Device Personalized Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02220.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04612.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Personalizing ASR with limited data using targeted subset selection</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04908.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Prompt-tuning in ASR systems for efficient domain-adaptation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06502.pdf">pdf</a></td>
                        </tr>
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Accent</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Accent-Robust Automatic Speech Recognition Using Supervised and Unsupervised Wav2vec Embeddings</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06502.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Multi-Dialect Arabic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.14678.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-12">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Dataset</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06909.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Building a Noisy Audio Dataset to Evaluate Machine Learning Approaches for Automatic Speech Recognition Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.01425.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">CORAA: a large corpus of spontaneous and prepared speech manually validated for speech recognition in Brazilian Portuguese</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.15731.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03370.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Towards Measuring Fairness in Speech Recognition: Casual Conversations Dataset Transcriptions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09983.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">The People's Speech: A Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09344.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">JTubeSpeech: corpus of Japanese speech collected from YouTube for speech recognition and speaker verification</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.09323.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-13">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Rebust</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Robustifying automatic speech recognition by extracting slowly varying features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.07400.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Perceptual Loss with Recognition Model for Single-Channel Enhancement and Robust ASR</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.06068.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Sequential Randomized Smoothing for Adversarially Robust Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.03000.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-14">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Other</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05376.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Speech recognition for air traffic control via feature learning and end-to-end training</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.02654.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">A study on native American English speech recognition by Indian listeners with varying word familiarity level</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.04151.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Blackbox Untargeted Adversarial Testing of Automatic Speech Recognition Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.01821.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>

<!--页脚-->
<footer class="footer">
    <div class="container">
        <div class="rwo">
            <div class="col-md-12">
                <p>
                    本站内容源自互联网，如有内容侵犯了你的权益，请联系删除相关内容，联系邮箱：yongqiangli@alumni.hust.edu.cn
                </p>
                <!--代码源自小呆导航的开源代码，遵循MIT协议，此处保留源代码的声明-->
                <p>
                    Copyright © 2018-2021 li yongqiang All Rights Reserved
                </p>
            </div>
        </div>
    </div>
</footer>
</div>
<!--内容区域-->
</div>
<div id="get-top" title="回到顶部">
    <i class="icon icon-arrow-up"></i>
</div>

<!-- jQuery (ZUI中的Javascript组件依赖于jQuery) -->
<script src="http://code.jquery.com/jquery-1.11.0.min.js"></script>

<script>
    window.onscroll = function(){
//回到顶部
var sllTop = document.documentElement.scrollTop||document.body.scrollTop;
if(sllTop>240){
  $('#get-top').css('display','block')
}else{
  $('#get-top').css('display','none')
}
}
$('#get-top').click(function(){ 
  $('body,html').animate({
    scrollTop: 0
  }, 800);//点击回到顶部按钮，数字越小越快
})
//判断用户使用的设备
var deviceVal  = browserRedirect();
function browserRedirect() {
  var sUserAgent = navigator.userAgent.toLowerCase();
  var bIsIpad = sUserAgent.match(/ipad/i) == "ipad";
  var bIsIphoneOs = sUserAgent.match(/iphone os/i) == "iphone os";
  var bIsMidp = sUserAgent.match(/midp/i) == "midp";
  var bIsUc7 = sUserAgent.match(/rv:1.2.3.4/i) == "rv:1.2.3.4";
  var bIsUc = sUserAgent.match(/ucweb/i) == "ucweb";
  var bIsAndroid = sUserAgent.match(/android/i) == "android";
  var bIsCE = sUserAgent.match(/windows ce/i) == "windows ce";
  var bIsWM = sUserAgent.match(/windows mobile/i) == "windows mobile";
  if (bIsIpad || bIsIphoneOs || bIsMidp || bIsUc7 || bIsUc || bIsAndroid || bIsCE || bIsWM) {
    return 'phone';
} else {
    return 'pc';
}
}
$('.nav-btn').on('click', function () {
    $('.nav').toggleClass('showNav');
    $(this).toggleClass('animated2');
});

</script>
</div>
</body>
</html>
