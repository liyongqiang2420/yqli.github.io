<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <!--头部信息-->
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <!--title keywords description 请改为自己的-->
    <title>低调奋进</title>

    <!--网站favicon可以没有或者改为自己的-->
    <!--<link rel="shortcut icon" type="image/x-icon" href="http://www.bituplink.com/wp-content/uploads/favicon.png"/>-->

    <!--CSS 若不需要变动样式不用改-->
    <link href="plugin/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/zui/1.8.1/css/zui.min.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" type="text/css" href="../css/common.css" />
    <link href="../img/logo.ico" rel="shortcut icon" />
    <script src="plugin/jquery.min.js"></script>
    <script src="plugin/bootstrap/js/bootstrap.min.js"></script>
</head>
<body id="nav_body">
<!--[if lt IE 10]>
<div class="alert alert-danger">
    您正在使用 
    <strong>过时的</strong> 浏览器. 请更换一个更好的浏览器来提升用户体验.
</div>
<![endif]--><!--头部导航条-->
<div id="content">
    <div class="w_header">
      <div class="container">
        <div class="w_header_top">
          <a href="../index.html" class="w_logo"></a>
          <span class="w_header_nav">
              <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="speech.html" class="active">Speech & ML</a></li>
                <li><a href="aigc.html">AIGC</a></li>
            <li><a href="https://yqlibook.readthedocs.io/en/latest/">yqliBook</a></li>
                <li><a href="pro.html">Programming</a></li>
                <li><a href="moodList.html">Life</a></li>
                <li><a href="tools.html">Tool</a></li>
                <li><a href="about.html">About</a></li>
            </ul>
        </span>
    </div>
</div>
</div>

<!--左侧Director，导航跳转-->
<div class="left-bar">
    <div class="header">
        <h2>Director</h2>
    </div>
    <div class="menu" id="menu">
        <ul class="scrollcontent">
            <!--左侧Director，按照需要修改和添加，参考已有的修改名称和href-->
            <li><a href="#row-1">General TTS</a></li>
            <li><a href="#row-2">Vocoder</a></li>
            <li><a href="#row-3">Adap & multi-lingual</a></li>
            <li><a href="#row-4">Expressive TTS</a></li>
            <li><a href="#row-5">Voice Conversion</a></li>
            <li><a href="#row-6">Sing Synthesis</a></li>
            <li><a href="#row-7">Talking Head</a></li>
            <li><a href="#row-8">Robust TTS</a></li>
            <li><a href="#row-9">Front End</a></li>
            <li><a href="#row-10">Alignment</a></li>
            <li><a href="#row-11">Dual learning</a></li>
            <li><a href="#row-12">EEG</a></li>
            <li><a href="#row-13">S2S</a></li>
            <li><a href="#row-14">Other</a></li>
        </ul>
    </div>
</div>
<!--内容-->
<div class="main">
    <div class="container content-box">
        <!--导航分类范例1，请根据自己的需求进行修改-->
        <section class="item card-box" id="row-1">
            <div class="container-fluid">
                <div class="row">
                    <div class="item-tit">
                        <strong>Journal and conference on speech</strong>
                        <table width="1150" border="1">
                            <tr>
                                <td width="150" align="center">CCF-A</a></td>
                                <td width="1000">NeuraIPS&nbsp;&nbsp;&nbsp;AAAI&nbsp;&nbsp;&nbsp;IJAI&nbsp;&nbsp;&nbsp;ACMMM </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">CCF-B</a></td>
                                <td width="1000">ICASSP&nbsp;&nbsp;&nbsp;COLING&nbsp;&nbsp;&nbsp;SpeechCom&nbsp;&nbsp;&nbsp;TSLP&nbsp;&nbsp;&nbsp;TASLP&nbsp;&nbsp;&nbsp;JSLHR&nbsp;&nbsp;&nbsp;TMM&nbsp;&nbsp;&nbsp;TOMCCAP&nbsp;&nbsp;&nbsp;ICME </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">CCF-C</a></td>
                                <td width="1000">INTERSPEECH&nbsp;&nbsp;&nbsp;ICPR </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">other</a></td>
                                <td width="1000">ICLR </td>
                            </tr>
                        </table>
                    </div>
                    <div class="item-tit">
                        <strong>On going</strong>
                        <table width="1150" border="1">
                            <tr>
                                <td width="150" align="center">2021</a></td>
                                <td width="800">speech synthese </td>
                                <td width="200"><a href="https://docs.google.com/spreadsheets/d/1wLjhN1RITE39NjrmvP2zKNP7JmilImPgPANCvXl8BK8/edit?usp=sharing">pdf</a></td>
                            </tr>
                            <td width="150" align="center">2022</a></td>
                            <td width="800">speech synthesis</td>
                            <td width="200"><a href="https://docs.google.com/spreadsheets/d/11YYOg6i6UXw19_g1JRaXGNhvt1zhG24RgOXCzZlqZGE/edit?usp=sharing">pdf</a></td>
                        </tr>
                    </table>
                        </table>
                    </div>
                    <div class="item-tit">
                        <strong>General TTS</strong>
                    </div>
                    <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.11972.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">The MSXF TTS System for ICASSP 2022 ADD Challenge</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.11400.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">MHTTS: Fast multi-head text-to-speech for spontaneous speech with imperfect transcription</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.07438.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.11755.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.07199.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Unsupervised word-level prosody tagging for controllable speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.07200.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">FAAG: Fast Adversarial Audio Generation through Interactive Attack Optimisation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.05416.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Building Synthetic Speaker Profiles in Text-to-Speech Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.03125.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Revisiting Over-Smoothness in Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.13066.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.01080.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.11562.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.0969.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Applying Syntax–Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15276.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.13508.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Differentiable Duration Modeling for End-to-End Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.11049.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15683.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">ECAPA-TDNN for Multi-speaker Text-to-speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.10473.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Improve few-shot voice cloning using multi-modal learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.09708.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.16852.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.17190.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Nix-TTS: An Incredibly Lightweight End-to-End Text-to-Speech Model via Non End-to-End Distillation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15643.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15796.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Variational Auto-Encoder based Mandarin Speech Cloning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15796.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">Vocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.10637.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">vTTS: visual-text to speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.14725.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">WavThruVec: Latent speech representation as intermediate features for neural speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.16930.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Regotron: Regularizing the Tacotron2 architecture via monotonic alignment loss</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.13437.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.11792.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">Hierarchical and Multi-Scale Variational Autoencoder for Diverse and Natural Non-Autoregressive Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.04004.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Unsupervised Quantized Prosody Representation for Controllable Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.03238.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Simple and Effective Unsupervised Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.02524.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">AILTTS: Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.02172.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.00768.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Universal Adaptor: Converting Mel-Spectrograms Between Different Configurations for Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.00170.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.04421.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.04120.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.08039.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.02147.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">NatiQ: An End-to-end Text-to-Speech System for Arabic</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.07373.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">40</a></td>
                            <td width="800">R-MelNet: Reduced Mel-Spectral Modeling for Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.15276.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">41</a></td>
                            <td width="800">TTS-by-TTS 2: Data-selective augmentation for neural speech synthesis using ranking support vector machine with variational autoencoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.14984.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">42</a></td>
                            <td width="800">UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.02512.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">43</a></td>
                            <td width="800">Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.02246.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">44</a></td>
                            <td width="800">Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.14607.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">45</a></td>
                            <td width="800">Diffsound: Discrete Diffusion Model for Text-to-sound Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.09983.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">46</a></td>
                            <td width="800">LIP: Lightweight Intelligent Preprocessor for meaningful text-to-speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.07118.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">47</a></td>
                            <td width="800">ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.06389.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">48</a></td>
                            <td width="800">DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.04646.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">49</a></td>
                            <td width="800">Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.06088.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">50</a></td>
                            <td width="800">SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.06011.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">51</a></td>
                            <td width="800">BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.01718.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">52</a></td>
                            <td width="800">Unify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS)</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.01547.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">53</a></td>
                            <td width="800">Mix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS)</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.01507.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">54</a></td>
                            <td width="800">Computer-assisted Pronunciation Training -- Speech synthesis is almost all you need</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.00774.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">55</a></td>
                            <td width="800">Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.13183.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">56</a></td>
                            <td width="800">Visualising Model Training via Vowel Space for Text-To-Speech Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.09775.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">57</a></td>
                            <td width="800">A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.02189.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">58</a></td>
                            <td width="800">EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.1089.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">59</a></td>
                            <td width="800">A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.10887.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">60</a></td>
                            <td width="800">Controllable Accented Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.10804.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">61</a></td>
                            <td width="800">Deep Speech Synthesis from Articulatory Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.06337.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">62</a></td>
                            <td width="800">AudioGen: Textually Guided Audio Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.15352.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2021 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Triple M: A Practical Neural Text-to-speech System With Multi-guidance Attention And Multi-band Multi-time Lpcnet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00247.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">VARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep VAE with Residual Attention</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.06431.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">LightSpeech: Lightweight and Fast Text to Speech with Neural Architecture Search</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.04040.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech </td>
                            <td width="200"><a href="https://openreview.net/pdf?id=o3iritJHLfO">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">ADASPEECH: ADAPTIVE TEXT TO SPEECH FOR CUSTOM VOICE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.00993.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Building Multilingual TTS using Cross-Lingual Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.14039.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Supervised and Unsupervised Approaches for Controlling Narrow Lexical Focus in Sequence-to-Sequence Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.09940.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Mixture Density Network for Phone-Level Prosody Modelling in Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00851.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.09914.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Data-Efficient Training Strategies for Neural TTS Systems</td>
                            <td width="200"><a href="https://dl.acm.org/doi/abs/10.1145/3430984.3431034">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Multilingual Byte2Speech Text-To-Speech Models Are Few-shot Spoken Language Learners</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.03541.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Text-to-speech for the hearing impaired</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.02174.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Continual Speaker Adaptation for Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.14512.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.14574.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.15060.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.05557.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Fast DCTTS: Efficient Deep Convolutional Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00624.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01409.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Multi-rate attention architecture for fast streamable Text-to-speech spectrum modeling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00705.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Flavored Tacotron: Conditional Learning for Prosodic-linguistic Features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.04050.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Speech Resynthesis from Discrete Disentangled Self-Supervised Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00355.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Dependency Parsing based Semantic Representation Learning with Graph Neural Network for Enhancing Expressiveness of Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06835.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Review of end-to-end speech synthesis technology based on deep learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.09995.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">dependency Parsing based Semantic Representation Learning with Graph Neural Network for Enhancing Expressiveness of Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06835.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">TalkNet 2: Non-Autoregressive Depth-Wise Separable Convolutional Model Stanislav Beliaev, Boris Ginsburgfor Speech Synthesis with Explicit Pitch and Duration Prediction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.08189.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Signal Representations for Synthesizing Audio Textures with Generative Adversarial Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.07390.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">SpeechNet: A Universal Modularized Model for Speech Processing Tasks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.03070.pdf">pdf</a>
                            &nbsp;&nbsp;<a href="https://mp.weixin.qq.com/s/858zn1pkTrL8mgN7GGw1Ug">blog</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">How do Voices from Past Speech Synthesis Challenges Compare Today?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.02373.pdf">pdf</a>
                            &nbsp;&nbsp;<a href="https://mp.weixin.qq.com/s?__biz=MzAxNjY3NjQwOQ==&mid=2247485167&idx=1&sn=159993dd0921ac04baec32d055ca0738&chksm=9bf065b9ac87ecafbb1d7d8e311494f8583d77a47ea9753c6afd22ca6ea0fb007a4bf9d60101&cur_album_id=1595929281711390722&scene=190#rd">blog</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Learning Robust Latent Representations for Controllable Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.04458.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">MASS: Multi-task Anthropomorphic Speech Synthesis Framework</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.04124.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">VQCPC-GAN: Variable-length Adversarial Audio Synthesis using Vector-Quantized Contrastive Predictive Coding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.01531.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">SpeechNet: A Universal Modularized Model for Speech Processing Tasks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.03070.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.06337.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">Ito^TTS and Ito^Wave: Linear Stochastic Differential Equation Is All You Need For Audio Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.07583.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">Diverse and Controllable Speech Synthesis with GMM-Based Phone-Level Prosody Modelling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.13086.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">A learned conditional prior for the VAE acoustic space of a TTS system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10229.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">A Survey on Neural Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15561.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">An objective evaluation of the effects of recording conditions and speaker characteristics in multi-speaker deep neural speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.01812.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">40</a></td>
                            <td width="800">Byakto Speech: Real-time long speech synthesis with convolutional neural network: Transfer learning from English to Bangla</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.03937.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">41</a></td>
                            <td width="800">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06103.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">42</a></td>
                            <td width="800">Controllable Context-aware Conversational Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10828.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">43</a></td>
                            <td width="800">Multi-Scale Spectrogram Modelling for Neural Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15649.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">44</a></td>
                            <td width="800">Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08352.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">45</a></td>
                            <td width="800">FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15123.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">46</a></td>
                            <td width="800">GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15153.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">47</a></td>
                            <td width="800">Hierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15144.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">48</a></td>
                            <td width="800">Improving multi-speaker TTS prosody variance with a residual encoder and normalizing flows</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.05762.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">49</a></td>
                            <td width="800">Non-native English lexicon creation for bilingual speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10870.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">50</a></td>
                            <td width="800">Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.02830.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">51</a></td>
                            <td width="800">Speaker verification-derived loss and data augmentation for DNN-based multispeaker speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.01789.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">52</a></td>
                            <td width="800">Speech BERT Embedding For Improving Prosody in Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.04312.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">53</a></td>
                            <td width="800">WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.09660.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">54</a></td>
                            <td width="800">Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.13479.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">55</a></td>
                            <td width="800">VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.03298.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">56</a></td>
                            <td width="800">Location, Location: Enhancing the Evaluation of Text-to-Speech Synthesis Using the Rapid Prosody Transcription Paradigm</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.02527.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">57</a></td>
                            <td width="800">Federated Learning with Dynamic Transformer for Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.08795.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">58</a></td>
                            <td width="800">Effective and Differentiated Use of Control Information for Multi-speaker Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/abs/2107.03065">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">59</a></td>
                            <td width="800">End to End Bangla Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.00500.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">60</a></td>
                            <td width="800">Perceptually Guided End-to-End Text-to-Speech With MOS Prediction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.01174.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">61</a></td>
                            <td width="800">One TTS Alignment To Rule Them All</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.10447.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">62</a></td>
                            <td width="800">Combining speakers of multiple languages to improve quality of neural voices</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.07737.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">63</a></td>
                            <td width="800">DeepEigen: Learning-based Modal Sound Synthesis with Acoustic Transfer Maps</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.07425.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">64</a></td>
                            <td width="800">Neural HMMs are all you need (for high-quality attention-free TTS)</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.13320.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">65</a></td>
                            <td width="800">PortaSpeech: Portable and High-Quality Generative Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.15166.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">66</a></td>
                            <td width="800">Nana-HDR: A Non-attentive Non-autoregressive Hybrid Model for TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.13673.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">67</a></td>
                            <td width="800">Low-Latency Incremental Text-to-Speech Synthesis with Distilled Context Prediction Network</td>
                            <td width="200"><a href=https://arxiv.org/pdf/2109.10724.pdf"">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">68</a></td>
                            <td width="800">An Audio Synthesis Framework Derived from Industrial Process Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.10455.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">69</a></td>
                            <td width="800">On-device neural speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.08710.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">70</a></td>
                            <td width="800">fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.06912.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">71</a></td>
                            <td width="800">A study on the efficacy of model pre-training in developing neural text-to-speech system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03857.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">72</a></td>
                            <td width="800">DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.12612.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">73</a></td>
                            <td width="800">Discrete acoustic space for an efficient sampling in neural text-to-speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.12539.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">74</a></td>
                            <td width="800">EdiTTS: Score-based Editing for Controllable Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02584.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">75</a></td>
                            <td width="800">Emphasis control for parallel neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03012.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">76</a></td>
                            <td width="800">Environment Aware Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03887.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">77</a></td>
                            <td width="800">ESPnet2-TTS: Extending the Edge of TTS Research</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07840.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">78</a></td>
                            <td width="800">FedSpeech: Federated Text-to-Speech with Continual Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07216.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">79</a></td>
                            <td width="800">Hierarchical prosody modeling and control in non-autoregressive parallel neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02952.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">80</a></td>
                            <td width="800">Mixer-TTS: non-autoregressive, fast and compact text-to-speech model conditioned on language model embeddings</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03584.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">81</a></td>
                            <td width="800">Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09698.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">82</a></td>
                            <td width="800">On the Interplay Between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.01147.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">83</a></td>
                            <td width="800">PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04486.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">84</a></td>
                            <td width="800">Prosody-TTS: An end-to-end speech synthesis system with prosody control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02854.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">85</a></td>
                            <td width="800">A study on the efficacy of model pre-training in developing neural text-to-speech system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03857.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">86</a></td>
                            <td width="800">Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10882.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">87</a></td>
                            <td width="800">Guided-TTS:Text-to-Speech with Untranscribed Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.11755.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">88</a></td>
                            <td width="800">Improved Prosodic Clustering for Multispeaker and Speaker-independent Phoneme-level Prosody Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10168.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">89</a></td>
                            <td width="800">Improving Prosody for Unseen Texts in Speech Synthesis by Utilizing Linguistic Information and Noisy Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07549.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">90</a></td>
                            <td width="800">More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10139.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">91</a></td>
                            <td width="800">Prosodic Clustering for Phoneme-level Prosody Control in End-to-End Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10177.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">92</a></td>
                            <td width="800">RefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.00962.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">93</a></td>
                            <td width="800">Speaker Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.05095.pdf">pdf</a></td>
                        </tr>
                    </table>

                    <h3> 2020 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">INTERACTIVE TEXT-TO-SPEECH VIA SEMI-SUPERVISED STYLE TRANSFER LEARNING</td>
                            <td width="200"><a href="../pdf/tts_paper/INTERACTIVE TEXT-TO-SPEECH VIA SEMI-SUPERVISED STYLE TRANSFER LEARNING.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">SQUEEZEWAVE EXTREMELY LIGHTWEIGHT VOCODERS FOR ON DEVICE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/SQUEEZEWAVE EXTREMELY LIGHTWEIGHT VOCODERS FOR ON DEVICE SPEECH SYNTHESIS.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://tianrengao.github.io/SqueezeWaveDemo/">demo</a>
                                &nbsp;&nbsp;<a href="https://github.com/tianrengao/SqueezeWave">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">LOCATION RELATIVE ATTENTION MECHANISMS FOR ROBUST LONG FORM SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/LOCATION RELATIVE ATTENTION MECHANISMS FOR ROBUST LONG FORM SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">End to End Adversarial Text to Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/End to End Adversarial Text to Speech.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://deepmind.com/research/publications/End-to-End-Adversarial-Text-to-Speech">demo</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">FastSpeech 2 Fast and High Quality End to End Text to Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/FastSpeech 2 Fast and High Quality End to End Text to Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Deep Representation Learning in Speech Processing Challenges Recent Advances and Future Trends</td>
                            <td width="200"><a href="../pdf/tts_paper/Deep Representation Learning in Speech Processing Challenges Recent Advances and Future Trends.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Flowtron an Autoregressive Flowbased Generative Network for TexttoSpeech Synthesis.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/Flowtron an Autoregressive Flowbased Generative Network for TexttoSpeech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">JDI-T- Jointly trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment</td>
                            <td width="200"><a href="../pdf/tts_paper/JDI-T- Jointly trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">FastPitch- Parallel Text-to-speech with Pitch Prediction.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/FastPitch- Parallel Text-to-speech with Pitch Prediction.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Glow-TTS- A Generative Flow for Text-to-Speech via Monotonic Alignment Search.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/Glow-TTS- A Generative Flow for Text-to-Speech via Monotonic Alignment Search.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">FLOW-TTS: A NON-AUTOREGRESSIVE NETWORK FOR TEXT TO SPEECH BASED ON FLOW</td>
                            <td width="200"><a href="../pdf/tts_paper/FLOW-TTS: A NON-AUTOREGRESSIVE NETWORK FOR TEXT TO SPEECH BASED ON FLOW.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">SpeedySpeech- Efficient Neural Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/SpeedySpeech- Efficient Neural Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">End-to-End Adversarial Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/End-to-End Adversarial Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Controllable Neural Prosody Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Controllable Neural Prosody Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling</td>
                            <td width="200"><a href="../pdf/tts_paper/Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Exploring TTS without T Using Biologically/Psychologically Motivated Neural Network Modules (ZeroSpeech 2020)</td>
                            <td width="200"><a href="../pdf/tts_paper/Exploring TTS without T Using Biologically/Psychologically Motivated Neural Network Modules (ZeroSpeech 2020).pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">From Speaker Verification to Multispeaker Speech Synthesis, Deep Transfer with Feedback Constraint</td>
                            <td width="200"><a href="../pdf/tts_paper/From Speaker Verification to Multispeaker Speech Synthesis, Deep Transfer with Feedback Constraint.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Incremental Text to Speech for Neural Sequence-to-Sequence Models using Reinforcement Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/Incremental Text to Speech for Neural Sequence-to-Sequence Models using Reinforcement Learning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit</td>
                            <td width="200"><a href="../pdf/tts_paper/Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages</td>
                            <td width="200"><a href="../pdf/tts_paper/Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Speaking Speed Control of End-to-End Speech Synthesis using Sentence-Level Conditioning</td>
                            <td width="200"><a href="../pdf/tts_paper/Speaking Speed Control of End-to-End Speech Synthesis using Sentence-Level Conditioning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">NON-ATTENTIVE TACOTRON- ROBUST AND CONTROLLABLE NEURAL TTS SYNTHESIS INCLUDING UNSUPERVISED DURATION MODELING</td>
                            <td width="200"><a href="../pdf/tts_paper/NON-ATTENTIVE TACOTRON- ROBUST AND CONTROLLABLE NEURAL TTS SYNTHESIS INCLUDING UNSUPERVISED DURATION MODELING.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">PARALLEL TACOTRON- NON-AUTOREGRESSIVE AND CONTROLLABLE TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/PARALLEL TACOTRON- NON-AUTOREGRESSIVE AND CONTROLLABLE TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">TTS-BY-TTS- TTS-DRIVEN DATA AUGMENTATION FOR FAST AND HIGH-QUALITY SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/TTS-BY-TTS- TTS-DRIVEN DATA AUGMENTATION FOR FAST AND HIGH-QUALITY SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">SPEECH SYNTHESIS AND CONTROL USING DIFFERENTIABLE DSP</td>
                            <td width="200"><a href="../pdf/tts_paper/SPEECH SYNTHESIS AND CONTROL USING DIFFERENTIABLE DSP.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">FEATHERTTS- ROBUST AND EFFICIENT ATTENTION BASED NEURAL TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/FEATHERTTS- ROBUST AND EFFICIENT ATTENTION BASED NEURAL TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">GRAPHSPEECH: SYNTAX-AWARE GRAPH ATTENTION NETWORK FOR NEURAL SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/GRAPHSPEECH- SYNTAX-AWARE GRAPH ATTENTION NETWORK FOR NEURAL SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">HIERARCHICAL PROSODY MODELING FOR NON-AUTOREGRESSIVE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/HIERARCHICAL PROSODY MODELING FOR NON-AUTOREGRESSIVE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">DEVICETTS: A SMALL-FOOTPRINT, FAST, STABLE NETWORK FOR ON-DEVICE TEXT-TO-SPEECH</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.15311.pdf">pdf</a></td>
                        </tr>
                        </tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">PRETRAINING STRATEGIES, WAVEFORM MODEL CHOICE, AND ACOUSTIC CONFIGURATIONS FOR MULTI-SPEAKER END-TO-END SPEECH SYNTHESIS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.04839.pdf">pdf</a></td>
                        </tr>
                        </tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">Fast and lightweight on-device TTS with Tacotron2 and LPCNet</td>
                            <td width="200"><a href="https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2169.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2019 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">2019</td>
                            <td width="800"> isca 2019 speech </td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2019/">papers</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Deep Text-to-Speech System with Seq2Seq Model</td>
                            <td width="200"><a href="../pdf/tts_paper/Deep_Text-to-Speech_System_with_Seq2Seq_Model.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">FastSpeech: Fast, Robust and Controllable Text to Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/FastSpeech_Fast_Robust_and_Controllable_Text_to_Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Neural Speech Synthesis with Transformer Network</td>
                            <td width="200"><a href="../pdf/tts_paper/Neural_Speech_Synthesis_with_Transformer_Network.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="../pdf/tts_paper/tts transformer .pptx">ppt</a>
                                &nbsp;&nbsp;<a href="https://neuraltts.github.io/transformertts/">demo</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Parallel Neural Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/Parallel Neural Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/libriTTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</td>
                            <td width="800">Forward-Backward Decoding for Regularizing End-to-End TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Forward-Backward Decoding for Regularizing End-to-End TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</td>
                            <td width="800">Self-attention Based Prosodic Boundary Prediction for Chinese Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Self-attention Based Prosodic Boundary Prediction for Chinese Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Guide to Speech Synthesis with Deep Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/Guide to Speech Synthesis with Deep Learning.pdf">ppt</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">tts tutorial part1 part2</td>
                            <td width="200"><a href="../pdf/tts_paper/tts tutorial part1.pdf">ppt1</a>
                                &nbsp;&nbsp;<a href="../pdf/tts_paper/tts_tutorial part2.pdf">ppt2</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">maximizing mutual information for tacotron</td>
                            <td width="200"><a href="../pdf/tts_paper/MAXIMIZING MUTUAL INFORMATION FOR TACOTRON.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</td>
                            <td width="800">durlan</td>
                            <td width="200"><a href="../pdf/tts_paper/DURIAN DURATION INFORMED ATTENTION NETWORK FOR MULTIMODAL SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</td>
                            <td width="800">Non-Autoregressive Neural Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/Non-Autoregressive Neural Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Tacotron-based acoustic model using phoneme alignment for practical neural text-to-speech systems</td>
                            <td width="200"><a href="../pdf/tts_paper/Tacotron-based acoustic model using phoneme alignment for practical neural text-to-speech systems.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2018 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">2018</td>
                            <td width="800"> isca 2018 speech </td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2018/">papers</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Deep voice 3: Scaling text-to-speech with convolutional sequence learning</td>
                            <td width="200"><a href="../pdf/tts_paper/deep_voice3.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">ClariNet Parallel Wave Generation in End-to-End Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/ClariNet Parallel Wave Generation in End-to-End Text-to-Speech.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Linear Networks Based Speaker Adaptation For Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Linear Networks Based Speaker Adaptation For Speech Synthesis.pdf">pdf</a>
                            </td>
                        </tr>
                    </table>    
                    <h3> 2017 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">2017</td>
                            <td width="800"> isca 2017 speech </td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2017/">papers</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Tacotron: Towards End-to-End Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/tacotron.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://google.github.io/tacotron/index.html">page</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Char2Wav: End-to-End Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Char2Wav-End-to-End_Speech_Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Deep Voice: Real-time Neural Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/Deep_Voice-Real-time_Neural_Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Deep Voice 2: Multi-Speaker Neural Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/DeepVoice2-Multi-Speaker_Neural_Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">VoiceLoop voice fitting and synthesis via a phonological loop</td>
                            <td width="200"><a href="../pdf/tts_paper/VOICELOOP- VOICE FITTING AND SYNTHESIS VIA A PHONOLOGICAL LOOP.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Attention Is All You Need</td>
                            <td width="200"><a href="../pdf/tts_paper/Attention Is All You Need.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2016 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">2016</td>
                            <td width="800"> isca 2016 speech </td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2016/">papers</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices</td>
                            <td width="200"><a href="../pdf/tts_paper/Fast_Compact_and_High_Quality_LSTM-RNN_Based_Statistical_Parametric.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Merlin: An Open Source Neural Network Speech Synthesis System</td>
                            <td width="200"><a href="../pdf/tts_paper/Merlin_An_Open_Source_Neural_Network_Speech_Synthesis_System.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2015 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Acoustic modeling instatistical parametric speechsynthesis-from HMM to LSTM-RNN</td>
                            <td width="200"><a href="../pdf/tts_paper/Acoustic_modeling_instatistical_parametric_speechsynthesis-from_HMM_to_LSTM-RNN.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="../pdf/tts_paper/Statistical Parametric Speech SynthesisFromHMMtoLSTMRNN_ppt.pdf">ppt</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Effective Approaches to Attention-based Neural Machine Translation</td>
                            <td width="200"><a href="../pdf/tts_paper/Effective Approaches to Attention-based Neural Machine Translation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">htkbook-3.5</td>
                            <td width="200"><a href="../pdf/tts_paper/htkbook-3.5.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">A study of speaker adaptation for DNN-based speech synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/A study of speaker adaptation for DNN-based speech synthesis.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2014 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks </td>
                            <td width="200"><a href="../pdf/tts_paper/TTS_Synthesis_with_Bidirectional_LSTM_based_Recurrent_Neural_Networks.pdf">pdf</a>
                            </td>
                        </tr>
                    </table>
                    <h3> 2013 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Statical parameteric speech synthesis Using deep neural networks </td>
                            <td width="200"><a href="../pdf/tts_paper/Statical parameteric speech synthesis Using deep neural networks.pdf">pdf</a></td>
                        </tr>
                    </table>
                </div>
            </div>
        </section>
<section class="item card-box" id="row-2">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Vocoder</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">ItôWave: Itô Stochastic Differential Equation Is All You Need For Wave Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.12519.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">End-to-end LPCNet: A Neural Vocoder With Fully-Differentiable LPC Estimation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.11301.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Neural Speech Synthesis on a Shoestring: Improving the Efficiency of LPCNet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.11169.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Phase Vocoder Done Right</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.07382.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">It's Raw! Audio Generation with State-Space Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.09729.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Trainin</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.03751.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">A Neural Vocoder Based Packet Loss Concealment Algorithm</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.14010.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">AdaVocoder: Adaptive Vocoder for Custom Voice</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.09825.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Bunched LPCNet2: Efficient Neural Vocoders Covering Devices from Cloud to Edge</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.14416.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">HiFi++: a Unified Framework for Neural Vocoding, Bandwidth Extension and Speech Enhancement</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.13086.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">iSTFTNet: Fast and Lightweight Mel-Spectrogram Vocoder Incorporating Inverse Short-Time Fourier Transform</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.02395.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Neural Vocoder is All You Need for Speech Super-resolution</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.14941.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.16749.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Parallel Synthesis for Autoregressive Speech Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.11806.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Speaking-Rate-Controllable HiFi-GAN Using Feature Interpolation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.10561.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.09934.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Streamable Neural Audio Synthesis With Non-Causal Convolutions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.07064.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">A Post Auto-regressive GAN Vocoder Focused on Spectrum Fracture</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.06086.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for Binaural Audio Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.14807.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Unified Source-Filter GAN with Harmonic-plus-Noise Source Excitation Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.06053.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">cMelGAN: An Efficient Conditional Generative Model Based on Mel Spectrograms</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.07319.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Avocodo: Generative Adversarial Network for Artifact-free Vocoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.13404.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">BigVGAN: A Universal Neural Vocoder with Large-Scale Training</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.04658.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">GoodBye WaveNet -- A Language Model for Raw Audio with Context of 1/2 Million Samples</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.08297.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">WOLONet: Wave Outlooker for Efficient and High Fidelity Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.09920.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">End-to-End Binaural Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.03697.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Differentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.07282.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Towards Parametric Speech Synthesis Using Gaussian-Markov Model of Spectral Envelope and Wavelet-Based Decomposition of F0</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.07122.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">DDSP-based Singing Vocoders: A New Subtractive-based Synthesizer and A Comprehensive Evaluation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.04756.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Mel Spectrogram Inversion with Stable Pitch</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.12782.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">An Initial study on Birdsong Re-synthesis Using Neural Vocoders</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.10479.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                <tr>
                    <td width="150" align="center">1</a></td>
                    <td width="800">GAN Vocoder: Multi-Resolution Discriminator Is All You Need</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2103.05236.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">2</a></td>
                    <td width="800">Improved parallel WaveGAN vocoder with perceptually weighted spectrogram loss</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2101.07412.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">3</a></td>
                    <td width="800">Universal Neural Vocoding with Parallel WaveNet</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2102.01106.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">4</a></td>
                    <td width="800">LVCNet: Efficient Condition-Dependent Modeling Network for Waveform Generation</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2102.10815.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">5</a></td>
                    <td width="800">High-Quality Vocoding Design with Signal Processing for Speech Synthesis and Voice Conversion</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2101.10278.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">6</a></td>
                    <td width="800">Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform Generation in Multiple Domains</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2011.09631.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">7</a></td>
                    <td width="800">Improve GAN-based Neural Vocoder using Pointwise Relativistic LeastSquare GAN</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2103.14245.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">8</a></td>
                    <td width="800">Unified Source-Filter GAN: Unified Source-filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2104.04668.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">9</a></td>
                    <td width="800">Reconstructing Speech from Real-Time Articulatory MRI Using Neural Vocoders</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2104.11598.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">10</a></td>
                    <td width="800">High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2105.09856.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">11</a></td>
                    <td width="800">a generative model for raw audio using transformer architectures</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.16036.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">12</a></td>
                    <td width="800">WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.08507.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">13</a></td>
                    <td width="800">Advances in Speech Vocoding for Text-to-Speech with Continuous Parameters</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.10481.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">14</a></td>
                    <td width="800">Basis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.13419.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">15</a></td>
                    <td width="800">Catch-A-Waveform: Learning to Generate Audio from a Single Short Example</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.06426.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">16</a></td>
                    <td width="800">Continuous Wavelet Vocoder-based Decomposition of Parametric Speech Waveform Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.06426.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">17</a></td>
                    <td width="800">CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.07431.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">18</a></td>
                    <td width="800">Fre-GAN: Adversarial Frequency-consistent Audio Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.02297.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">19</a></td>
                    <td width="800">Glow-WaveGAN: Learning Speech Representations from GAN-based Variational Auto-Encoder For High Fidelity Flow-based Speech Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.10831.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">20</a></td>
                    <td width="800">Improving the expressiveness of neural vocoding with non-affine Normalizing Flows</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.08649.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">21</a></td>
                    <td width="800">Mathematical Vocoder Algorithm : Modified Spectral Inversion for Efficient Neural Speech Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.03167.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">22</a></td>
                    <td width="800">Relational Data Selection for Data Augmentation of Speaker-dependent Multi-band MelGAN Vocoder</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.05629.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">23</a></td>
                    <td width="800">UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.07889.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">24</a></td>
                    <td width="800">WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.08507.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">25</a></td>
                    <td width="800">A GENERATIVE MODEL FOR RAW AUDIO USING TRANSFORMER ARCHITECTURES</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.08507.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">26</a></td>
                    <td width="800">Neural Waveshaping Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2107.05050.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">27</a></td>
                    <td width="800">A Generative Model for Raw Audio Using Transformer Architectures</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.16036.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">28</a></td>
                    <td width="800">DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis with GANs</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2108.01216.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">29</a></td>
                    <td width="800">Ito^TTS and Ito^Wave: Linear Stochastic Differential Equation Is All You Need For Audio Generation</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2105.07583.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">31</a></td>
                    <td width="800">A Streamwise GAN Vocoder for Wideband Speech Coding at Very Low Bit Rate</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2108.04051.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">32</a></td>
                    <td width="800">FlowVocoder: A small Footprint Neural Vocoder based Normalizing flow for Speech Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2109.13675.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">33</a></td>
                    <td width="800">MSR-NV: Neural vocoder using multiple sampling rates</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2109.13714.pdf">pdf</a></td>
                </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Chunked Autoregressive GAN for Conditional Waveform Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.10139.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">Neural Analysis and Synthesis: Reconstructing Speech from Self-Supervised Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.14513.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">Neural Pitch-Shifting and Time-Stretching with Controllable LPCNet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02360.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">Neural Synthesis of Footsteps Sound Effects with Generative Adversarial Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09605.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">The Mirrornet : Learning Audio Synthesizer Controls Inspired by Sensorimotor Interaction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05695.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">Towards Universal Neural Vocoding with a Multi-band Excited WaveNet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03329.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">40</a></td>
                            <td width="800">High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latenc</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09052.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">41</a></td>
                            <td width="800">RAVE: A variational autoencoder for fast and high-quality neural audio synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.05011.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">42</a></td>
                            <td width="800">VocBench: A Neural Vocoder Benchmark for Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.03099.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">43</a></td>
                            <td width="800">DiffWave: A Versatile Diffusion Model for Audio Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2009.09761.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">multiband melgan</td>
                            <td width="200"><a href="../pdf/tts_paper/multiband melgan.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">FeatherWave An efficient high fidelity neural vocoder with multiband linear prediction</td>
                            <td width="200"><a href="../pdf/tts_paper/FeatherWave An efficient high fidelity neural vocoder with multiband linear prediction.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">parallel wavegan</td>
                            <td width="200"><a href="../pdf/tts_paper/parallel wavegan.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">VocGan</td>
                            <td width="200"><a href="../pdf/tts_paper/VocGAN- A High-Fidelity Real-time Vocoder with a Hierarchically nested Adversarial Network.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">WAVEGRAD- ESTIMATING GRADIENTS FOR WAVEFORM GENERATION</td>
                            <td width="200"><a href="../pdf/tts_paper/WAVEGRAD- ESTIMATING GRADIENTS FOR WAVEFORM GENERATION.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">PARALLEL WAVEGAN- A FAST WAVEFORM GENERATION MODEL BASED ON GENERATIVE ADVERSARIAL NETWORKS WITH MULTI-RESOLUTION SPECTROGRAM</td>
                            <td width="200"><a href="../pdf/tts_paper/PARALLEL WAVEGAN- A FAST WAVEFORM GENERATION MODEL BASED ON GENERATIVE ADVERSARIAL NETWORKS WITH MULTI-RESOLUTION SPECTROGRAM.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">WAVEGRAD  ESTIMATING GRADIENTS FOR WAVEFORM GENERATION</td>
                            <td width="200"><a href="../pdf/tts_paper/WAVEGRAD  ESTIMATING GRADIENTS FOR WAVEFORM GENERATION.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">A Cyclical Post-filtering Approach to Mismatch Refinement of Neural Vocoder for Text-to-speech Systems</td>
                            <td width="200"><a href="../pdf/tts_paper/A Cyclical Post-filtering Approach to Mismatch Refinement of Neural Vocoder for Text-to-speech Systems.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Bunched LPCNet - Vocoder for Low-cost Neural Text-To-Speech Systems</td>
                            <td width="200"><a href="../pdf/tts_paper/Bunched LPCNet - Vocoder for Low-cost Neural Text-To-Speech Systems.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Quasi-Periodic Parallel WaveGAN Vocoder- A Non-autoregressive Pitchdependent Dilated Convolution Model for Parametric Speech Generation</td>
                            <td width="200"><a href="../pdf/tts_paper/Quasi-Periodic Parallel WaveGAN Vocoder- A Non-autoregressive Pitchdependent Dilated Convolution Model for Parametric Speech Generation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Neural Text-to-Speech with a Modeling-by-Generation Excitation Vocoder</td>
                            <td width="200"><a href="../pdf/tts_paper/Neural Text-to-Speech with a Modeling-by-Generation Excitation Vocoder.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Improving Opus Low Bit Rate Quality with Neural Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Improving Opus Low Bit Rate Quality with Neural Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">WG-WaveNet- Real-Time High-Fidelity Speech Synthesis without GPU</td>
                            <td width="200"><a href="../pdf/tts_paper/WG-WaveNet- Real-Time High-Fidelity Speech Synthesis without GPU.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Vocoder-Based Speech Synthesis from Silent Videos</td>
                            <td width="200"><a href="../pdf/tts_paper/Vocoder-Based Speech Synthesis from Silent Videos.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Ultrasound-based Articulatory-to-Acoustic Mapping with WaveGlow Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Ultrasound-based Articulatory-to-Acoustic Mapping with WaveGlow Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Speaker Conditional WaveRNN- Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions</td>
                            <td width="200"><a href="../pdf/tts_paper/Speaker Conditional WaveRNN- Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">GAUSSIAN LPCNET FOR MULTISAMPLE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/GAUSSIAN LPCNET FOR MULTISAMPLE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">UNIVERSAL MELGAN: A ROBUST NEURAL VOCODER FOR HIGH-FIDELITY WAVEFORM GENERATION IN MULTIPLE DOMAINS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.09631.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Improving LPCNet-based Text-to-Speech with Linear Prediction-structured Mixture Density Network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2001.11686.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">What the Future Brings: Investigating the Impact of Lookahead for Incremental Neural TTS</td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/2103.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Lightweight LPCNet-based Neural Vocoder with Tensor Decomposition</td>
                            <td width="200"><a href="http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=247&id=348">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.05646.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">High quality lightweight and adaptable TTS using LPCNet</td>
                            <td width="200"><a href="../pdf/tts_paper/High quality lightweight and adaptable TTS using LPCNet.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">A Neural Vocoder with Hierarchical Generation of Amplitude and Phase Spectra for Statistical Parametric Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/A_Neural_Vocoder_with_Hierarchical_Generation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">RawNet: Fast End-to-End Neural Vocoder</td>
                            <td width="200"><a href="../pdf/tts_paper/RawNet Fast End-to-End Neural Vocoder.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet</td>
                            <td width="200"><a href="../pdf/tts_paper/A Real-Time Wideband Neural Vocoder at 1.6 kbs Using LPCNet.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">Lpcnet improving neural speech synthesis through linear prediction</td>
                            <td width="200"><a href="../pdf/tts_paper/LPCNET: IMPROVING NEURAL SPEECH SYNTHESIS THROUGH LINEAR PREDICTION.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://people.xiph.org/~jm/demo/lpcnet/">demo</a>
                                &nbsp;&nbsp;<a href="https://github.com/drowe67/LPCNet">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Waveglow</td>
                            <td width="200"><a href="../pdf/tts_paper/waveglow.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">melgan</td>
                            <td width="200"><a href="../pdf/tts_paper/MelGAN Generative Adversarial Networks for Conditional Waveform Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">AN INVESTIGATION OF SUBBAND WAVENET VOCODER COVERING ENTIRE AUDIBLE FREQUENCY RANGE WITH LIMITED ACOUSTIC FEATURES</td>
                            <td width="200"><a href="../pdf/tts_paper/AN INVESTIGATION OF SUBBAND WAVENET VOCODER COVERING ENTIRE AUDIBLE FREQUENCY RANGE WITH LIMITED ACOUSTIC FEATURES.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">A Comparison of Recent Neural Vocoders for Speech Signal Reconstruction</td>
                            <td width="200"><a href="https://pdfs.semanticscholar.org/093a/804dc251dbd68b190918e180707bd1f66e4b.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Natural TTS Synthesis by Conditioning Wavennet on MEL spectrogram predictions(tacotron2)</td>
                            <td width="200"><a href="../pdf/tts_paper/Natural_TTS_Synthesis by_Conditioning_Wavennet_on_MEL_spectrogram_predictions.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://github.com/NVIDIA/tacotron2">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Efficient Neural Audio Synthesis (WaveRNN)</td>
                            <td width="200"><a href="../pdf/tts_paper/Efficient Neural Audio Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Improving FFTNet vocoder with noise shaping and subband approaches</td>
                            <td width="200"><a href="../pdf/tts_paper/Improving FFTNet vocoder with noise shaping and subband approaches.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">FFTNET: A REAL-TIME SPEAKER-DEPENDENT NEURAL VOCODER</td>
                            <td width="200"><a href="../pdf/tts_paper/FFTNET: A REAL-TIME SPEAKER-DEPENDENT NEURAL VOCODER.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">SQUEEZEWAVE: EXTREMELY LIGHTWEIGHT VOCODERS FOR ON-DEVICE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/SQUEEZEWAVE: EXTREMELY LIGHTWEIGHT VOCODERS FOR ON-DEVICE SPEECH SYNTHESIS.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://github.com/tianrengao/SqueezeWave">code</a>
                            </td>
                        </tr>
            </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Parallel_WaveNet-Fast_High-Fidelity_Speech_Synthesis.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2016 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Wavenet A Generative Model For Raw Audio</td>
                            <td width="200"><a href="../pdf/tts_paper/Wavenet_A_Generative_Model_For_Raw_Audio.pdf">pdf</a>&nbsp;&nbsp;
                                <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">demo</a>
                                &nbsp;&nbsp;<a href="https://github.com/ibab/tensorflow-wavenet">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Fast Wavenet Geneartion Algorithm</td>
                            <td width="200"><a href="../pdf/tts_paper/Fast_Wavenet_Geneartion_Algorithm.pdf">pdf</a></td>
                        </tr>
            </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-3">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Adap & Multispeaker & Multilingual</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.08124.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Zero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.10375.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">nnSpeech: Speaker-Guided Conditional Variational Autoencoder for Zero-shot Multi-speaker Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.10712.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Voice Filter: Few-shot text-to-speech speaker adaptation using voice conversion as a post-processing module</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.08164.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.03191.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Speaker Adaption with Intuitive Prosodic Features for Statistical Parametric Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.00951.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15447.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">VoiceMe: Personalized voice generation in TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15379.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Applying Feature Underspecified Lexicon Phonological Features in Multilingual Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.07228.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Data-augmented cross-lingual synthesis in a teacher-student framework</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.00061.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Fine-grained Noise Control for Multispeaker Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.05070.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Self supervised learning for robust voice cloning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.03421.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.00990.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.00436.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Few-Shot Cross-Lingual TTS Using Transferable Phoneme Embedding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.15427.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Pronunciation Dictionary-Free Multilingual Speech Synthesis by Combining Unsupervised and Supervised Phonetic Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.00951.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.12132.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">CopyCat2: A Single Model for Multi-Speaker TTS and Many-to-Many Fine-Grained Prosody Transfer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.13443.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Prosody Cloning in Zero-Shot Multispeaker Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.12229.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">AdaVITS: Tiny VITS for Low Computing Resource Speaker Adaptation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.00208.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.15370.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">TDASS: Target Domain Adaptation Speech Synthesis Framework for Multi-speaker Low-Resource TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.11824.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Human-in-the-loop Speaker Adaptation for DNN-based Multi-speaker TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.10256.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">When Is TTS Augmentation Through a Pivot Language Useful?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.09889.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.05913.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">Decoupled Pronunciation and Prosody Modeling in Meta-Learning-Based Multilingual Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.06789.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">ParaTTS: Learning Linguistic and Prosodic Cross-sentence Information in Paragraph-based TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.06484.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Multi-Task Adversarial Training Algorithm for Multi-Speaker Neural Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.12549.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Building Multilingual TTS using Cross-Lingual Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.14039.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">ADASPEECH: ADAPTIVE TEXT TO SPEECH FOR CUSTOM VOICE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.00993.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.04088.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Voice Cloning: a Multi-Speaker Text-to-Speech Synthesis Approach based on Transfer Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.05630.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">CUHK-EE voice cloning system for ICASSP 2021 M2VoC challenge</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.04699.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Real-time Timbre Transfer and Sound Synthesis using DDSP</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.07220.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">The Multi-speaker Multi-style Voice Cloning Challenge 2021</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01818.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">The AS-NU System for the M2VoC Challenge</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.03009.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Exploring Disentanglement with Multilingual and Monolingual VQ-VAE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.01573.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.03153.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Speaker Adaptation with Continuous Vocoder-based DNN-TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.01154.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">GC-TTS: Few-shot Speaker Adaptation with Geometric Constraints</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.06890.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.05426.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Adapting TTS models For New Speakers using Transfer Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05798.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Cloning one's voice using very limited data in the wild</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03347.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Adapting TTS models For New Speakers using Transfer Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05798.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Applying Phonological Features in Multilingual Text-To-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03609.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07192.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07210.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Revisiting IPA-based Cross-lingual Text-to-speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07187.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04482.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Cross-lingual Low Resource Speaker Adaptation Using Phonological Features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09075.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.04040.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">V2C: Visual Voice Cloning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.12890.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Cross lingual Multispeaker Text to Speech under Limited Data Scenario</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 Cross lingual Multispeaker Text to Speech under Limited Data Scenario.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Efficient neural speech synthesis for low resource languages through multilingual modeling</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 Efficient neural speech synthesis for low resource languages through multilingual modeling.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">EndtoEnd Code Switching TTS with Cross Lingual Language Model</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 EndtoEnd Code Switching TTS with Cross Lingual Language Model.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">One Model Many Languages  Meta learning for Multilingual Text to Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 One Model Many Languages  Meta learning for Multilingual Text to Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">SPEAKER ADAPTATION OF A MULTILINGUAL ACOUSTIC MODEL FOR CROSS-LANGUAGE SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 SPEAKER ADAPTATION OF A MULTILINGUAL ACOUSTIC MODEL FOR CROSS-LANGUAGE SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</td>
                            <td width="800">Multilingual speech synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Multilingual speech synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</td>
                            <td width="800">Domain-adversarial training of multi-speaker TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Domain adversarial training of multi speaker TTS .pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Focusing on Attention Prosody Transfer and Adaptative Optimization Strategy for Multi Speaker End to End Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Focusing on Attention Prosody Transfer and Adaptative Optimization Strategy for Multi Speaker End to End Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Zero Shot Multi Speaker Text To Speech with State of the art Neural Speaker Embeddings</td>
                            <td width="200"><a href="../pdf/tts_paper/Zero Shot Multi Speaker Text To Speech with State of the art Neural Speaker Embeddings.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Multi-speaker Text-to-speech Synthesis Using Deep Gaussian Processes</td>
                            <td width="200"><a href="../pdf/tts_paper/Multi-speaker Text-to-speech Synthesis Using Deep Gaussian Processes.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Phonological Features for 0-shot Multilingual Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Phonological Features for 0-shot Multilingual Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Semi-supervised Learning for Multi-speaker Text-to-speech Synthesis Using Discrete Speech Representation</td>
                            <td width="200"><a href="../pdf/tts_paper/Semi-supervised Learning for Multi-speaker Text-to-speech Synthesis Using Discrete Speech Representation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">USING IPA-BASED TACOTRON FOR DATA EFFICIENT CROSS-LINGUAL SPEAKER ADAPTATION AND PRONUNCIATION ENHANCEMENT</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.06392.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Cross lingual Multi speaker Texttospeech Synthesis for Voice Cloning without Using Parallel Corpus for Unseen Speakers</td>
                            <td width="200"><a href="../pdf/tts_paper/2019 Cross lingual Multi speaker Texttospeech Synthesis for Voice Cloning without Using Parallel Corpus for Unseen Speakers.pdf ">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Learning to Speak Fluently in a Foreign Language Multilingual Speech Synthesis and Cross Language Voice Cloning</td>
                            <td width="200"><a href="../pdf/tts_paper/2019 Learning to Speak Fluently in a Foreign Language Multilingual Speech Synthesis and Cross Language Voice Cloning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">个性化语音合成中说话人特征不同嵌入方式的研究</td>
                            <td width="200"><a href="../pdf/tts_paper/2019 个性化语音合成中说话人特征不同嵌入方式的研究.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Cross lingual Multispeaker Text To Speech Synthesis Using Neural Speaker Embedding</td>
                            <td width="200"><a href="../pdf/tts_paper/Cross lingual Multispeaker Text To Speech Synthesis Using Neural Speaker Embedding.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">Automatic Multispeaker Voice Cloning</td>
                            <td width="200"><a href="../pdf/tts_paper/Automatic Multispeaker Voice Cloning.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://google.github.io/tacotron/publications/speaker_adaptation/">demo</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</td>
                            <td width="800">Training Multi-Speaker Neural Text-to-Speech Systems using Speaker-Imbalanced Speech Corpora</td>
                            <td width="200"><a href="../pdf/tts_paper/Training Multi-Speaker Neural Text-to-Speech Systems using Speaker-Imbalanced Speech Corpora.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Speaker adaptation in DNN-based speech synthesis using d-vectors</td>
                            <td width="200"><a href="../pdf/tts_paper/Speaker adaptation in DNNbased speech synthesis using dvectors.pdf">pdf</a>
                            </td>
                        </tr>
          </table>
            <h3> 2016 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Speaker Representations for Speaker Adaptation in Multiple Speakers’BLSTM-RNN-based Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Speaker Representations for Speaker Adaptation in Multiple Speakers’BLSTM-RNN-based Speech Synthesis.pdf">pdf</a></td>
                        </tr>
          </table>
            <h3> 2015 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Multi-speaker modeling and speaker adaptation for dnn-based tts synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Multispeaker modeling and speaker adaptation for dnnbased tts synthesis.pdf">pdf</a>
                            </td>
                        </tr>
          </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-4">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Expressive TTS</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Disentangling Style and Speaker Attributes for TTS Style Transfer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.09472.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">MsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.06460.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Distribution augmentation for low-resource expressive text-to-speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.06409.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Cross-speaker style transfer for text-to-speech using data augmentation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.05083.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Towards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.12201.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.10020.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Towards Multi-Scale Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.02743.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">StyleWaveGAN: Style-based synthesis of drum sounds with extensive controls using generative adversarial networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.00907.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.15439.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.07211.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.12040.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Expressive, Variable, and Controllable Duration Modelling in TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.14165.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis based on Disentanglement between Prosody and Timbre</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.14866.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.15067.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Self-supervised Context-aware Style Representation for Expressive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.12559.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.14643.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.12262.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.06000.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">PoeticTTS -- Controllable Poetry Reading for Literary Studies</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.05549.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Cross-speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.01198.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Speech Synthesis with Mixed Emotions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.05890.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Towards Cross-speaker Reading Style Transfer on Audiobook Dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.05359.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">The Role of Voice Persona in Expressive Communication:An Argument for Relevance in Speech Synthesis Design</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.02855.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Whispered and Lombard Neural Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.05313.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Expressive Neural Voice Cloning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00151.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Model architectures to extrapolate emotional expressions in DNN-based text-to-speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.10345.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Analysis and Assessment of Controllability of an Expressive Deep Learning-based TTS system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.04097.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">STYLER: Style Modeling with Rapidity and Robustness via SpeechDecomposition for Expressive and Controllable Neural Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.09474.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Expressive Text-to-Speech using Style Tag</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00436.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01408.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Towards Multi-Scale Style Control for Expressive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.03521.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">AdaSpeech 2: Adaptive Text to Speech with Untranscribed Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.09715.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Exploring emotional prototypes in a high dimensional TTS latent space</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.01891.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Global Rhythm Style Transfer Without Text Transcriptions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08519.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Improving Performance of Seen and Unseen Speech Style Transfer in End-to-end Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10003.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Non-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.12896.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Spoken Style Learning with Multi-modal Hierarchical Context Encoding for Conversational Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06233.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">UniTTS: Residual Learning of Unified Embedding Space for Speech Style Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.11171.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.12562.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.02530.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Daft-Exprt: Robust Prosody Transfer Across Speakers for Expressive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.02271.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Information Sieve: Content Leakage Reduction in End-to-End Prosody For Expressive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.01831.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Enhancing audio quality for expressive Neural Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.06270.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Emotional Speech Synthesis for Companion Robot to Imitate Professional Caregiver Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.12787.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Controllable cross-speaker emotion transfer for end-to-end speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.06733.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Cross-speaker Emotion Transfer Based on Speaker Condition Layer Normalization and Semi-Supervised Training in Text-To-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04153.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">GANtron: Emotional Speech Synthesis with Generative Adversarial Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03390.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">Improving Emotional Speech Synthesis by Using SUS-Constrained VAE and Text Encoder Aggregation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09780.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">StrengthNet: Deep Learning-based Emotion Strength Assessment for Emotional Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03156.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Fine-grained style control in Transformer-based Text-to-speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06306.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Using multiple reference audios and style embedding constraints for speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04451.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">Emotional Prosody Control for Speech Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.04730.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Meta-Voice: Fast few-shot style transfer for expressive voice cloning using meta learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07218.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Word-Level Style Control for Expressive, Non-attentive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10173.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">Multi-speaker Multi-style Text-to-speech Synthesis With Single-speaker Single-style Training Data Scenarios</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.12743.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">Multi-speaker Emotional Text-to-speech Synthesizer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.03557.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Controllable Neural Prosody Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Controllable Neural Prosody Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">FULLY-HIERARCHICAL FINE-GRAINED PROSODY MODELING FOR INTERPRETABLE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/FULLY-HIERARCHICAL FINE-GRAINED PROSODY MODELING FOR INTERPRETABLE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Flowtron- an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Flowtron- an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Enhancing Speech Intelligibility in Text-To-Speech Synthesis using Speaking Style Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Enhancing Speech Intelligibility in Text-To-Speech Synthesis using Speaking Style Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Controllable Emotion Transfer For End-to-End Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08679.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Fine-grained Emotion Strength Transfer, Control and Prediction for Emotional Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08477.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">MULTI-REFERENCE NEURAL TTS STYLIZATION WITH ADVERSARIAL CYCLE CONSISTENCY</td>
                            <td width="200"><a href="../pdf/tts_paper/MULTI-REFERENCE NEURAL TTS STYLIZATION WITH ADVERSARIAL CYCLE CONSISTENCY.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Multi-reference Tacotron by Intercross Training for Style Disentangling, Transfer and Control in Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Multi-reference Tacotron by Intercross Training for Style Disentangling, Transfer and Control in Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">MELLOTRON- MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS</td>
                            <td width="200"><a href="../pdf/tts_paper/MELLOTRON- MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">HIERARCHICAL GENERATIVE MODELING FOR CONTROLLABLE SPEECH SYNTHESIS.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/HIERARCHICAL GENERATIVE MODELING FOR CONTROLLABLE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron</td>
                            <td width="200"><a href="../pdf/tts_paper/Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Style Tokens- Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Style Tokens- Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">PREDICTING EXPRESSIVE SPEAKING STYLE FROM TEXT IN END-TO-END SPEECH SYNTHESIS.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/PREDICTING EXPRESSIVE SPEAKING STYLE FROM TEXT IN END-TO-END SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
            </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-5">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Voice Conversion</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Invertible Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.10687.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Emotion Intensity and its Control for Emotional Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.03967.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">IQDUBBING: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.00269.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Noise-robust voice conversion with domain adversarial training</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.10693.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">DRVC: A Framework of Any-to-Any Voice Conversion with Self-Supervised Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.10976.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">AVQVC: One-shot Voice Conversion by Vector Quantization with applying contrastive learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.1002.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15873.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Analysis of Voice Conversion and Code-Switching Synthesis Using VQ-VAE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.14640.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">DGC-vector: A new speaker embedding for zero-shot voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.09722.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Disentangleing Content and Fine-grained Prosody Information via Hybrid ASR Bottleneck Features for Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.12813.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">DRVC: A Framework of Any-to-Any Voice Conversion with Self-Supervised Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.10976.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Efficient Non-Autoregressive GAN Voice Conversion using VQWav2vec Features and Dynamic Convolution</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.17172.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Enhancing Zero-Shot Many to Many Voice Conversion with Self-Attention VAE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.16037.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">HiFi-VC: High Quality ASR-Based Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.16937.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Robust Disentangled Variational Speech Representation Learning for Zero-shot Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.16705.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">SpeechSplit 2.0: Unsupervised speech disentanglement for voice conversion Without tuning autoencoder Bottlen</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.14156.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Text-free non-parallel many-to-many voice conversion using normalising flows</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.08009.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Enhanced exemplar autoencoder with cycle consistency loss in any-to-one voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.03847.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Time Domain Adversarial Voice Conversion for ADD 2022</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.08692.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Enhanced exemplar autoencoder with cycle consistency loss in any-to-one voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.03847.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Towards Improved Zero-shot Voice Conversion with Conditional DSVAE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.05227.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">End-to-End Zero-Shot Voice Style Transfer with Location-Variable Convolutions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.09784.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">An Evaluation of Three-Stage Voice Conversion Framework for Noisy and Reverberant Conditions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.15155.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">End-to-End Voice Conversion with Information Perturbation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.07569.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">Identifying Source Speakers for Voice Conversion based Spoofing Attacks on Speaker Verification Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.09103.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">Speak Like a Dog: Human to Non-human creature Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.0478.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Speak Like a Professional: Increasing Speech Intelligibility by Mimicking Professional Announcer Voice with Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.13021.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Streaming non-autoregressive model for any-to-many voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.07288.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">Subband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.06057.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">A Comparative Study of Self-supervised Speech Representation Based Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.04356.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Glow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and Any-to-any Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.01832.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">GlowVC: Mel-spectrogram space disentangling model for language-independent text-free voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.01454.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.00756.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.08757.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">TGAVC: Improving Autoencoder Voice Conversion with Text-Guided and Adversarial Training</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.04035.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Rhythm</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.11866.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">Boosting Star-GANs for Voice Conversion with Contrastive Discriminator</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.10088.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">DeID-VC: Speaker De-identification via Zero-shot Pseudo Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.0453.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">Investigation into Target Speaking Rate Adaptation for Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.01978.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">EMOCAT: LANGUAGE-AGNOSTIC EMOTIONAL VOICE CONVERSION</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.05695.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Building Multilingual TTS using Cross-Lingual Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.14039.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">High-Quality Vocoding Design with Signal Processing for Speech Synthesis and Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.10278.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Hierarchical disentangled representation learning for singing voice conversio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.06842.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Adversarially learning disentangled speech representations for robust multi-factor voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00184.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Towards Natural and Controllable Cross-Lingual Voice Conversion Based on Neural TTS Model and Phonetic Posteriorgram</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.01991.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Investigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.11420.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">crank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.02858.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">MaskCycleGAN-VC: Learning Non-parallel Voice Conversion with Filling in Frames</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.12841.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Axial Residual Networks for CycleGAN-based Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.08075.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Improving Zero-shot Voice Style Transfer via Disentangled Representation Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.09420.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">CycleDRUMS: Automatic Drum Arrangement For Bass Lines Using CycleGAN</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00353.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00931.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.02901.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">StarGAN-based Emotional Voice Conversion for Japanese Phrases</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01807.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">NoiseVC: Towards High Quality Zero-Shot Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06074.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Non-autoregressive sequence-to-sequence voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06793.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">FastS2S-VC: Streaming Non-Autoregressive Sequence-to-Sequence Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06900.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">NoiseVC: Towards High Quality Zero-Shot Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06074.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Non-autoregressive sequence-to-sequence voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06793.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Building Bilingual and Code-Switched Voice Conversion with Limited
Training Data Using Embedding Consistency Loss</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.10832.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Towards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.07283.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">An Adaptive Learning based Generative Adversarial Network for One-To-One Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.12159.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.09856.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.01786.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.13871.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Emotional Voice Conversion: Theory, Databases and ESD</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.14762.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.13479.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.01415.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Enriching Source Style Transfer in Recognition-Synthesis based Non-Parallel Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08741.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Improving robustness of one-shot voice conversion with deep discriminative speaker encoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10406.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">NVC-Net: End-to-End Adversarial Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.00992.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">Pathological voice adaptation with autoencoder-based voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08427.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Voicy: Zero-Shot Non-Parallel Voice Conversion in Noisy Reverberant Environments</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08873.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10132.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">StarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for Natural-Sounding Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.10394.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">On Prosody Modeling for ASR+TTS based Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.09477.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.08361.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">Many-to-Many Voice Conversion based Feature Disentanglement using Variational Autoencoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.06642.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">40</a></td>
                            <td width="800">Expressive Voice Conversion: A Joint Framework for Speaker Identity and Emotional Style Transfer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.03748.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">41</a></td>
                            <td width="800">Improving robustness of one-shot voice conversion with deep discriminative speaker encoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10406.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">42</a></td>
                            <td width="800">StarGAN-VC+ASR: StarGAN-based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.04395.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">43</a></td>
                            <td width="800">Unet-TTS: Improving Unseen Speaker and Style Transfer in One-shot Voice Cloning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.11115.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">44</a></td>
                            <td width="800">Noisy-to-Noisy Voice Conversion Framework with Denoising Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.10608.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">45</a></td>
                            <td width="800">Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.03551.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">46</a></td>
                            <td width="800">Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.13821.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">47</a></td>
                            <td width="800">Decoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.01164.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">48</a></td>
                            <td width="800">MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02500.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">49</a></td>
                            <td width="800">S3PRL-VC: Open-source Voice Conversion Framework with Self-supervised Speech Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06280.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">50</a></td>
                            <td width="800">Sequence-To-Sequence Voice Conversion using F0 and Time Conditioning and Adversarial Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03744.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">51</a></td>
                            <td width="800">Speech Enhancement-assisted Stargan Voice Conversion in Noisy Environments</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09923.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">52</a></td>
                            <td width="800">Toward Degradation-Robust Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07537.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">53</a></td>
                            <td width="800">Towards Identity Preserving Normal to Dysarthric Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08213.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">54</a></td>
                            <td width="800">A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.02392.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">55</a></td>
                            <td width="800">AC-VC: Non-parallel Low Latency Phonetic Posteriorgrams Based Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.06601.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">56</a></td>
                            <td width="800">Attention-Guided Generative Adversarial Network for Whisper to Normal Speech Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.01342.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">57</a></td>
                            <td width="800">CycleTransGAN-EVC: A CycleGAN-based Emotional Voice Conversion Model with Transformer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.15159.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">58</a></td>
                            <td width="800">Direct Noisy Speech Modeling for Noisy-to-Noisy Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07116.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">59</a></td>
                            <td width="800">One-shot Voice Conversion For Style Transfer Based On Speaker Adaptation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.12277.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">60</a></td>
                            <td width="800">SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.03811.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">61</a></td>
                            <td width="800">Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.04424.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">62</a></td>
                            <td width="800">Conditional Deep Hierarchical Variational Autoencoder for Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.02796.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">63</a></td>
                            <td width="800">YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.02418.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data</td>
                            <td width="200"><a href="../pdf/tts_paper/Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">An Overview of Voice Conversion and its Challenges: From Statistical Modeling to Deep Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2008.03648.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Converting Anyone’s Emotion- Towards Speaker-Independent Emotional Voice Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Converting Anyone’s Emotion- Towards Speaker-Independent Emotional Voice Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">SEEN AND UNSEEN EMOTIONAL STYLE TRANSFER FOR VOICE CONVERSION WITH A NEW EMOTIONAL SPEECH DATASET</td>
                            <td width="200"><a href="../pdf/tts_paper/SEEN AND UNSEEN EMOTIONAL STYLE TRANSFER FOR VOICE CONVERSION WITH A NEW EMOTIONAL SPEECH DATASET.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">ANY-TO-ONE SEQUENCE-TO-SEQUENCE VOICE CONVERSION USING SELF-SUPERVISED DISCRETE SPEECH REPRESENTATIONS</td>
                            <td width="200"><a href="../pdf/tts_paper/ANY-TO-ONE SEQUENCE-TO-SEQUENCE VOICE CONVERSION USING SELF-SUPERVISED DISCRETE SPEECH REPRESENTATIONS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">GAZEV- GAN-Based Zero-Shot Voice Conversion over Non-parallel Speech Corpus</td>
                            <td width="200"><a href="../pdf/tts_paper/GAZEV- GAN-Based Zero-Shot Voice Conversion over Non-parallel Speech Corpus.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">TOWARDS LOW-RESOURCE STARGAN VOICE CONVERSION USING WEIGHT ADAPTIVE INSTANCE NORMALIZATION</td>
                            <td width="200"><a href="../pdf/tts_paper/TOWARDS LOW-RESOURCE STARGAN VOICE CONVERSION USING WEIGHT ADAPTIVE INSTANCE NORMALIZATION.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">CycleGAN-VC3- Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/CycleGAN-VC3- Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Accent and Speaker Disentanglement in Many-to-many Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08609.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">AUTOVC- Zero-Shot Voice Style Transfer with Only Autoencoder Loss</td>
                            <td width="200"><a href="../pdf/tts_paper/AUTOVC- Zero-Shot Voice Style Transfer with Only Autoencoder Loss.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">An Overview of Voice Conversion Systems</td>
                            <td width="200"><a href="../pdf/tts_paper/An Overview of Voice Conversion Systems.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Non-Parallel Sequence-to-Sequence Voice Conversion with Disentangled Linguistic and Speaker Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1906.10508.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">An Overview of Voice Conversion Systems</td>
                            <td width="200"><a href="../pdf/tts_paper/An Overview of Voice Conversion Systems.pdf">pdf</a></td>
                        </tr>
            </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-6">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Sing Synthesis</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Improving Adversarial Waveform Generation based Singing Voice Conversion with Harmonic Signals</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.10130.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">partitura: A Python Package for Handling Symbolic Musical Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.13144.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.10936.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.07429.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">MR-SVS: Singing Voice Synthesis with Multi-Reference Encoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.03864.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Quantized GAN for Complex Music Generation from Dance Videos</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.00604.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Expressive Singing Synthesis Using Local Style Token and Dual-path Pitch Encoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.03249.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Learn2Sing 2.0: Diffusion and Mutual Information-Based Target Speaker SVS by Learning from Singing Teacher</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.16408.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Music Generation Using an LSTM</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.12105.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.17001.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">U-Singer: Multi-Singer Singing Voice Synthesizer that Controls Emotional Intensity</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.00931.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.10750.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Singing-Tacotron: Global duration control attention and dynamic filter for End-to-end singing voice synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.07907.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Deep Performer: Score-to-Audio Music Performance Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.06034.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Learning the Beauty in Songs: Neural Singing Voice Beautifier</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.13277.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">SUSing: SU-net for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.11841.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Muskits: an End-to-End Music Processing Toolkit for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.04029.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center"><18/a></td>
                            <td width="800">A Hierarchical Speaker Representation Framework for One-shot Singing Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.13762.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Adversarial Multi-Task Learning for Disentangling Timbre and Pitch in Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.11558.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Multi-instrument Music Synthesis with Spectrogram Diffusion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.05408.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">HouseX: A Fine-grained House Music Dataset and its Potential in the Music Industry</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.11690.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">WeSinger 2: Fully Parallel Singing Voice Synthesis via Multi-Singer Conditional Adversarial Training</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.01886.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">What is missing in deep music generation? A study of repetition and structure in popular music</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.001820.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">A New Corpus for Computational Music Research and A Novel Method for Musical Structure Analysis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.14747.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">MeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.14345.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">Leveraging Symmetrical Convolutional Transformer Networks for Speech to Singing Voice Style Transfer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.1241.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Musika! Fast Infinite Waveform Music Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.08706.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Mandarin Singing Voice Synthesis with Denoising Diffusion Probabilistic Wasserstein GAN</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.10446.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">musicaiz: A Python Library for Symbolic Music Generation, Analysis and Visualization</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.07974.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.07144.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">SongDriver: Real-time Music Accompaniment Generation without Logical Latency nor Exposure Bias</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.06054.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">What is missing in deep music generation? A study of repetition and structure in popular music</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.00182.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Anyone GAN Sing</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.11058.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Latent Space Explorations of Singing Voice Synthesis using DDSP</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.07197.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Learning to Generate Music With Sentiment</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.06125.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Hierarchical disentangled representation learning for singing voice conversio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.06842.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.12292.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">DiffSinger: Diffusion Acoustic Model for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.02446.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">LoopNet: Musical Loop Synthesis Conditioned On Intuitive Musical Parameters</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.10371.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.12292.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Music Generation using Deep Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.09046.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">MLP Singer: Towards Rapid Parallel Korean Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.07886.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">N-Singer: A Non-Autoregressive Korean Singing Voice Synthesis System for Pronunciation Enhancement</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15205.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.02776.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">A Unified Model for Zero-shot Music Source Separation, Transcription and Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.03456.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">An Empirical Study on End-to-End Singing Voice Synthesis with Encoder-Decoder Architectures</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.03008.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">A Melody-Unsupervision Model for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06546.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02511.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">DeepA: A Deep Neural Analyzer For Speech And Singing Vocoding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06434.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Enhanced Memory Network: The novel network structure for Symbolic Music Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03392.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE using Mel-spectrograms</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04005.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">KaraTuner: Towards end to end natural pitch correction for singing voice in karaoke</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09121.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Pitch Preservation In Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05033.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07468.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Towards High-fidelity Singing Voice Conversion with Acoustic Reference and Contrastive Predictive Coding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04754.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">A Melody-Unsupervision Model for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06546.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02511.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">A-Muze-Net: Music Generation by Composing the Harmony based on the Generated Melody</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.12986.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Learning To Generate Piano Music With Sustain Pedals</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.01216.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Rapping-Singing Voice Synthesis based on Phoneme-level Prosody Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09146.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">Symbolic Music Loop Generation with VQ-VAE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07657.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Video Background Music Generation with Controllable Music Transformer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.08380.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Zero-shot Singing Technique Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.08839.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">Evaluating Deep Music Generation Methods Using Data Augmentation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.00052.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.10358.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">EmotionBox: a music-element-driven emotional music generation system using Recurrent Neural Network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.08561.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">HIFISINGER TOWARDS HIGH FIDELITY NEURAL SINGING VOICE SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/HIFISINGER TOWARDS HIGH FIDELITY NEURAL SINGING VOICE SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">ByteSing A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder Decoder Acoustic Models and WaveRNN Vocoders</td>
                            <td width="200"><a href="../pdf/tts_paper/ByteSing A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder Decoder Acoustic Models and WaveRNN Vocoders.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">DurIAN SC Duration Informed Attention Network based Singing Voice Conversion System</td>
                            <td width="200"><a href="../pdf/tts_paper/DurIAN SC Duration Informed Attention Network based Singing Voice Conversion System.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Jukebox A Generative Model for Music</td>
                            <td width="200"><a href="../pdf/tts_paper/Jukebox A Generative Model for Music.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">XiaoiceSing- A High-Quality and Integrated Singing Voice Synthesis System</td>
                            <td width="200"><a href="../pdf/tts_paper/XiaoiceSing- A High-Quality and Integrated Singing Voice Synthesis System.pdf">pdf</a></td>
                        </tr>
                            <td width="150" align="center">6</td>
                            <td width="800">Speech-to-Singing Conversion based on Boundary Equilibrium GAN</td>
                            <td width="200"><a href="../pdf/tts_paper/Speech-to-Singing Conversion based on Boundary Equilibrium GAN.pdf">pdf</a></td>
                        </tr>
                        </tr>
                            <td width="150" align="center">7</td>
                            <td width="800">A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.06801.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">MELLOTRON- MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS</td>
                            <td width="200"><a href="../pdf/tts_paper/MELLOTRON- MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS.pdf">pdf</a></td>
                        </tr>
            </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-7">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Talking Head</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Multi-modal data fusion of Voice and EMG data for Robotic Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.02237.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Stitch it in Time: GAN-Based Facial Editing of Real Videos</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.08361.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.05986.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.00791.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Improving Cross-lingual Speech Synthesis with Triplet Training Scheme</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.10729.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">VCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge transfer from voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.09081.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">CALM: Contrastive Aligned Audio-Language Multirate and Multimodal Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.03587.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Recent Advances and Challenges in Deep Audio-Visual Correlation Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2202.13673.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Freeform Body Motion Generation from Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.02291.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Transformer-based Multimodal Information Fusion for Facial Expression Analysis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.12367.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.12756.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.02090.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Lip to Speech Synthesis with Visual Context Attentional GAN</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.01726.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Residual-guided Personalized Speech Synthesis based on Face Image</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.01672.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Multi-modality Associative Bridging through Memory: Speech Sound Recollected from Face Video</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.01265.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Text/Speech-Driven Full-Body Animation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.15573.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Talking Face Generation with Multilingual TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.06421.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">A Novel Speech-Driven Lip-Sync Model with CNN and LSTM</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.00916.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">FlexLip: A Controllable Text-to-Lip System</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.03206.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Learning Speaker-specific Lip-to-Speech Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.02050.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.07458.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.13038.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center"><23/a></td>
                            <td width="800">Audio Input Generates Continuous Frames to Synthesize Facial Video Using Generative Adiversarial Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.08813.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.03800.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">StableFace: Analyzing and Improving Motion Stability for Talking Face Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2208.13717.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">Facial Landmark Predictions with Applications to Metaverse</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.14698.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">AutoLV: Automatic Lecture Video Generator</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.08795.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Continuously Controllable Facial Expression Editing in Talking Face Videos</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.08289.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">TIMIT-TTS: a Text-to-Speech Dataset for Multimodal Synthetic Media Detection</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.08000.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Talking Head from Speech Audio using a Pre-trained Image Generator</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.04252.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Lip-to-Speech Synthesis for Arbitrary Speakers in the Wild</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.00642.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Generating coherent spontaneous speech and gesture from text</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.05684.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Creating Song From Lip and Tongue Videos With a Convolutional Vocoder</td>
                            <td width="200"><a href="https://www.researchgate.net/publication/348439109_Creating_Song_From_Lip_and_Tongue_Videos_With_a_Convolutional_Vocoder">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">SPEAK WITH YOUR HANDS Using Continuous Hand Gestures to control Articulatory Speech Synthesizer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.01640.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">What is Multimodality?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.06304.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.08223.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.10299.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Text2Video: Text-driven Talking-head Video Synthesis with Phonetic Dictionary</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.14631.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Recent Advances and Trends in Multimodal Deep Learning: A Review</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.11087.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Rethinking the constraints of multimodal fusion: case study in Weakly-Supervised Audio-Visual Video Parsing</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.14430.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.12306.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.04185.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">NWT: Towards natural audio-to-video generation with representation learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.04283.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.14014.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.09293.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">A Survey on Audio Synthesis and Audio-Visual Multimodal Processing</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.00443.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Integrated Speech and Gesture Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.11436.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.04325.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.08020.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.10595.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Audio-to-Image Cross-Modal Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.13354.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08580.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Talking Head Generation with Audio and Speech Related Facial Action Units</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09951.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">LiMuSE: Lightweight Multi-modal Speaker Extraction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.04063.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">Metric-based multimodal meta-learning for human movement identification via footstep recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07979.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">FaceFormer: Speech-Driven 3D Facial Animation with Transformers</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.05329.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">Joint Audio-Text Model for Expressive Speech-Driven 3D Facial Animation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.02214.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">PoseKernelLifter: Metric Lifting of 3D Human Pose using Sound</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.00216.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">What comprises a good talking head video generation? A Survey and Benchmark</td>
                            <td width="200"><a href="../pdf/tts_paper/What comprises a good talking head video generation? A Survey and Benchmark.pdf">pdf</a>
                            &nbsp;&nbsp;<a href="https://github.com/lelechen63/talking-head-generation-survey">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models</td>
                            <td width="200"><a href="../pdf/tts_paper/A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Large-scale multilingual audio visual dubbing</td>
                            <td width="200"><a href="../pdf/tts_paper/Large-scale multilingual audio visual dubbing.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">(talking head) Text-based Editing of Talking-head Video</td>
                            <td width="200"><a href="../pdf/tts_paper/Text-based Editing of Talking-head Video.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://www.ohadf.com/projects/text-based-editing/">vedio</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</td>
                            <td width="200"><a href="../pdf/tts_paper/Talking Face Generation by Adversarially Disentangled Audio-Visual Representation.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS">code</a>
                                &nbsp;&nbsp;<a href="https://www.youtube.com/watch?v=-J2zANwdjcQ">demo</a>
                            </td>
                        </tr>
             </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-8">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Robust TTS</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800"></td>
                            <td width="200"><a href="">pdf</a></td>
                        </tr>
                    </table>

            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Noise Robust TTS for Low Resource Speakers using Pre-trained Model and Speech Enhancement</td>
                            <td width="200"><a href="../pdf/tts_paper/Noise Robust TTS for Low Resource Speakers using Pre-trained Model and Speech Enhancement.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial Training</td>
                            <td width="200"><a href="../pdf/tts_paper/Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial Training.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Neural Text to Speech Adaptation from Low Quality Public Recordings</td>
                            <td width="200"><a href="../pdf/tts_paper/Neural Text to Speech Adaptation from Low Quality Public Recordings.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization</td>
                            <td width="200"><a href="../pdf/tts_paper/32102.pdf">pdf</a></td>
                        </tr>
            </table>
        </div>
    </div>
</section>

</section>
<section class="item card-box" id="row-9">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Front End</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.10716.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Polyphone disambiguation and accent prediction using pre-trained language models in Japanese TTS front-end</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.09427.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">An End-to-end Chinese Text Normalization Model based on Rule-guided Flat-Lattice Transformer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.16954.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.10430.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.15917.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural Machine Translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.04922.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Automatic Prosody Annotation with Pre-Trained Text-Speech Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.07956.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.13703.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center"><9/a></td>
                            <td width="800">A Polyphone BERT for Polyphone Disambiguation in Mandarin Chinese</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.12089.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Detection of Prosodic Boundaries in Speech Using Wav2Vec 2.0</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.15032.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Non-Standard Vietnamese Word Detection and Normalization for Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.02971.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021</h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00621.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Grapheme-to-Phoneme Transformer Model for Transfer Learning Dialects</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.04091.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Proteno: Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.07777.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Phrase break prediction with bidirectional encoder representations in Japanese text-to-speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.12395.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">A Unified Transformer-based Framework for Duplex Text Normalization</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.09889.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">A UNIFIED SEQUENCE-TO-SEQUENCE FRONT-END MODEL FOR MANDARIN TEXT-TO-SPEECH SYNTHESIS.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/A UNIFIED SEQUENCE-TO-SEQUENCE FRONT-END MODEL FOR MANDARIN TEXT-TO-SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">A HYBRID TEXT NORMALIZATION SYSTEM USING MULTI-HEAD SELF-ATTENTION FOR MANDARIN.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/A HYBRID TEXT NORMALIZATION SYSTEM USING MULTI-HEAD SELF-ATTENTION FOR MANDARIN.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">A Mask-based Model for Mandarin Chinese Polyphone Disambiguation</td>
                            <td width="200"><a href="../pdf/tts_paper/A Mask-based Model for Mandarin Chinese Polyphone Disambiguation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Unified Mandarin TTS Front-end Based on Distilled BERT Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.15404.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">A Mandarin Prosodic Boundary Prediction Model Based on Multi Task Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/A Mandarin Prosodic Boundary Prediction Model Based on Multi Task Learning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Token Level Ensemble Distillation for Grapheme to Phoneme Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Token Level Ensemble Distillation for Grapheme to Phoneme Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Pre trained Text Representations for Improving Front End Text Processing in Mandarin Text to Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Pre trained Text Representations for Improving Front End Text Processing in Mandarin Text to Speech Synthesis.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Mandarin Prosody Prediction Based on Attention Mechanism and Multi-model Ensemble</td>
                            <td width="200"><a href="../pdf/tts_paper/Mandarin Prosody Prediction Based on Attention Mechanism and Multimodel Ensemble.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2016 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Improving Prosodic Boundaries Prediction for Mandarin Speech Synthesis by Using Enhanced Embedding Feature and Model Fusion Approach</td>
                            <td width="200"><a href="../pdf/tts_paper/Improving Prosodic Boundaries Prediction for Mandarin Speech Synthesis by Using Enhanced Embedding Feature and Model Fusion Approach.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2015 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">AUTOMATIC PROSODY PREDICTION FOR CHINESE SPEECH SYNTHESIS USING BLSTM-RNN AND EMBEDDING FEATURES</td>
                            <td width="200"><a href="../pdf/tts_paper/AUTOMATIC PROSODY PREDICTION FOR CHINESE SPEECH SYNTHESIS USING BLSTM-RNN AND EMBEDDING FEATURES.pdf">pdf</a></td>
                        </tr>
            </table>
        </div>
    </div>
</section>

<section class="item card-box" id="row-10">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Alignment</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800"></td>
                            <td width="200"><a href="">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Triple M: A Practical Neural Text-to-speech System With Multi-guidance Attention And Multi-band Multi-time Lpcnet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00247.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Multi-rate attention architecture for fast streamable Text-to-speech spectrum modeling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00705.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">LOCATION-RELATIVE ATTENTION MECHANISMS FOR ROBUST LONG-FORM SPEECH SYNTHESIS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1910.10288.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Attentron- Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding</td>
                            <td width="200"><a href="../pdf/tts_paper/Attentron- Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Peking Opera Synthesis via Duration Informed Attention Network</td>
                            <td width="200"><a href="../pdf/tts_paper/Peking Opera Synthesis via Duration Informed Attention Network.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Understanding Self-Attention of Self-Supervised Audio Transformers</td>
                            <td width="200"><a href="../pdf/tts_paper/Understanding Self-Attention of Self-Supervised Audio Transformers.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Initial investigation of an encoder-decoder end-to-end TTS framework using marginalization of monotonic hard latent alignments</td>
                            <td width="200"><a href="../pdf/tts_paper/Initial investigation of an encoder-decoder end-to-end TTS framework using marginalization of monotonic hard latent alignments.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1906.00672.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">MONOTONIC CHUNKWISE ATTENTION</td>
                            <td width="200"><a href="../pdf/tts_paper/MONOTONIC CHUNKWISE ATTENTION.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">FORWARD ATTENTION IN SEQUENCE-TO-SEQUENCE ACOUSTIC MODELING FOR SPEECH SYNTHESIS.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/FORWARD ATTENTION IN SEQUENCE-TO-SEQUENCE ACOUSTIC MODELING FOR SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Online and Linear-Time Attention by Enforcing Monotonic Alignments</td>
                            <td width="200"><a href="../pdf/tts_paper/Online and Linear-Time Attention by Enforcing Monotonic Alignments.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Attention Is All You Need</td>
                            <td width="200"><a href="../pdf/tts_paper/Attention Is All You Need">pdf</a></td>
                        </tr>
            </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-11">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Dual Learning</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800"></td>
                            <td width="200"><a href="">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Exploring Machine Speech Chain for Domain Adaptation and Few-Shot Speaker Adaptation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.03815.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">LRSpeech- Extremely Low-Resource Speech Synthesis and Recognition</td>
                            <td width="200"><a href="../pdf/tts_paper/LRSpeech- Extremely Low-Resource Speech Synthesis and Recognition.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Almost Unsupervised Text to Speech and Automatic Speech Recognition</td>
                            <td width="200"><a href="../pdf/tts_paper/Almost Unsupervised Text to Speech and Automatic Speech Recognition.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Machine Speech Chain with One-shot Speaker Adaptation</td>
                            <td width="200"><a href="../pdf/tts_paper/Machine Speech Chain with One-shot Speaker Adaptation.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Listening while Speaking- Speech Chain by Deep Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/Listening while Speaking- Speech Chain by Deep Learning.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-12">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>EEG</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800"></td>
                            <td width="200"><a href="">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">On Interfacing the Brain with Quantum Computers: An Approach to Listen to the Logic of the Mind</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.03887.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Advancing Speech Synthesis using EEG</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2004.04731.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center"2>2</td>
                            <td width="800">Speech Synthesis using EEG</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2002.12756.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Predicting Different Acoustic Features from EEG and towards direct synthesis of Audio Waveform from EEG</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.01262.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-13">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>S2S</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">CVSS Corpus and Massively Multilingual Speech-to-Speech Translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.03713.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Creating Speech-to-Speech Corpus from Dubbed Series</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.03601.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.13339.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.02967.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Leveraging Pseudo-labeled Data to Improve Direct Speech-to-Speech Translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.08993.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.12523.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Assessing Evaluation Metrics for Speech-to-Speech Translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.13877.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Direct simultaneous speech to speech translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08250.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Incremental Speech Synthesis For Speech-To-Speech Translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08214.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Textless Speech-to-Speech Translation on Real Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.08352.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-14">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Other</strong>
            </div>
            <!--获取内容列表-->
                    <h3> 2022 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.10896">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2201.05771.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Residual-Guided Non-Intrusive Speech Quality Assessment</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.11499.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Robotic Speech Synthesis: Perspectives on Interactions, Scenarios, and Ethics</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.09599.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.14757.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">The VoiceMOS Challenge 2022</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2203.11389.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Improving Self-Supervised Learning-based MOS Prediction Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.11030.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">LibriS2S: A German-English Speech-to-Speech Translation Corpus</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.10593.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Enhancement of Pitch Controllability using Timbre-Preserving Pitch Augmentation in FastPitch</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.05753.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Fusion of Self-supervised Learned Models for MOS Prediction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.04855.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Karaoker: Alignment-free singing voice synthesis with speech training data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.04127.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Arabic Text-To-Speech (TTS) Data Preparation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.03255.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.03219.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.03040.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">A Comparison of Deep Learning MOS Predictors for Speech Synthesis Quality</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.02249.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.02152.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality Assessment</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.01345.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Into-TTS : Intonation Template based Prosody Control System</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2204.01271.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel's Weekly Video Podcasts</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.12194.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Macedonian Speech Synthesis for Assistive Technology Applications</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.09198.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">TuGeBiC: A Turkish German Bilingual Code-Switching Corpus</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2205.00868.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Audio Similarity is Unreliable as a Proxy for Audio Quality</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.13411.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Comparison of Speech Representations for the MOS Prediction System</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.13817.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">SAQAM: Spatial Audio Quality Assessment Metric</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.12297.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">Speech Quality Assessment through MOS using Non-Matching References</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.12285.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">The ZevoMOS entry to VoiceMOS Challenge 2022</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.07448.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Wideband Audio Waveform Evaluation Networks: Efficient, Accurate Estimation of Speech Qualities</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2206.13272.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">EEG2Mel: Reconstructing Sound from Brain Responses to Music</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.13845.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.03546.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2207.01063.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Evaluating generative audio systems and their metrics</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.00130.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">Predicting pairwise preferences between TTS audio stimuli using parallel ratings data and anti-symmetric twin neural networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.11003.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">MnTTS: An Open-Source Mongolian Text-to-Speech Synthesis Dataset and Accompanied Baseline</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.10848.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">ESPnet-ONNX: Bridging a Gap Between Research and Production</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.09756.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2209.06358.pdf">pdf</a></td>
                        </tr>
                    </table>
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.00110.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Hi-Fi Multi-Speaker English TTS Dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01497.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">ProsoBeast Prosody Annotation Tool</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.02397.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.08459.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">Deep Learning Based Assessment of Synthetic Speech Naturalness</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.11673.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Deep Learning Based Assessment of Synthetic Speech Naturalness</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.11673.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Speaker disentanglement in video-to-speech conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.09652.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Voice of Your Brain: Cognitive Representations of Imagined Speech,Overt Speech, and Speech Perception Based on EEG</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.14787.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">ADEPT: A Dataset for Evaluating Prosody Transfer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08321.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.09317.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">HUI-Audio-Corpus-German: A high quality TTS dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06309.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Mixtures of Deep Neural Experts for Automated Speech Scoring</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.12475.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08468.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Adaptation of Tacotron2-based Text-To-Speech for Articulatory-to-Acoustic Mapping using Ultrasound Tongue Imaging</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.12051.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Extending Text-to-Speech Synthesis with Articulatory Movement Prediction using Ultrasound Tongue Imaging</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.05550.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Speech Synthesis from Text and Ultrasound Tongue Image-based Articulatory Input</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.02003.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">An Objective Evaluation Framework for Pathological Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.00308.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Digital Einstein Experience: Fast Text-to-Speech for Conversational AI</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.10658.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Translatotron 2: Robust direct speech-to-speech translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.08661.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Direct speech-to-speech translation with discrete units</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.05604.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Fighting Game Commentator with Pitch and Loudness Adjustment Utilizing Highlight Cues</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.08112.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.05684.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">"Hello, It's Me": Deep Learning-based Speech Synthesis Attacks in the Real World</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.09598.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">FMFCC-A: A Challenging Mandarin Dataset for Synthetic Speech Detection</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09441.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">AQP: An Open Modular Python Platform for Objective Speech and Audio Quality Metrics</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.13589.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">Generalization Ability of MOS Prediction Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02635.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">LDNet: Unified Listener Dependent Modeling in MOS Prediction for Synthetic Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09103.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Objective Measures of Perceptual Audio Quality Reviewed: An Evaluation of Their Application Domain Dependence</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.11438.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.14203.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Cross-lingual Low Resource Speaker Adaptation Using Phonological Features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09075.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Visualising and Explaining Deep Learning Models for Speech Quality Prediction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2112.06219.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>

<!--页脚-->
<footer class="footer">
    <div class="container">
        <div class="rwo">
            <div class="col-md-12">
                <p>
                    本站内容源自互联网，如有内容侵犯了你的权益，请联系删除相关内容，联系邮箱：yongqiangli@alumni.hust.edu.cn
                </p>
                <!--代码源自小呆导航的开源代码，遵循MIT协议，此处保留源代码的声明-->
                <p>
                    Copyright © 2015-2035 li yongqiang All Rights Reserved
                </p>
            </div>
        </div>
    </div>
</footer>
</div>
<!--内容区域-->
</div>
<div id="get-top" title="回到顶部">
    <i class="icon icon-arrow-up"></i>
</div>

<!-- jQuery (ZUI中的Javascript组件依赖于jQuery) -->
<script src="http://code.jquery.com/jquery-1.11.0.min.js"></script>

<script>
    window.onscroll = function(){
//回到顶部
var sllTop = document.documentElement.scrollTop||document.body.scrollTop;
if(sllTop>240){
  $('#get-top').css('display','block')
}else{
  $('#get-top').css('display','none')
}
}
$('#get-top').click(function(){ 
  $('body,html').animate({
    scrollTop: 0
  }, 800);//点击回到顶部按钮，数字越小越快
})
//判断用户使用的设备
var deviceVal  = browserRedirect();
function browserRedirect() {
  var sUserAgent = navigator.userAgent.toLowerCase();
  var bIsIpad = sUserAgent.match(/ipad/i) == "ipad";
  var bIsIphoneOs = sUserAgent.match(/iphone os/i) == "iphone os";
  var bIsMidp = sUserAgent.match(/midp/i) == "midp";
  var bIsUc7 = sUserAgent.match(/rv:1.2.3.4/i) == "rv:1.2.3.4";
  var bIsUc = sUserAgent.match(/ucweb/i) == "ucweb";
  var bIsAndroid = sUserAgent.match(/android/i) == "android";
  var bIsCE = sUserAgent.match(/windows ce/i) == "windows ce";
  var bIsWM = sUserAgent.match(/windows mobile/i) == "windows mobile";
  if (bIsIpad || bIsIphoneOs || bIsMidp || bIsUc7 || bIsUc || bIsAndroid || bIsCE || bIsWM) {
    return 'phone';
} else {
    return 'pc';
}
}
$('.nav-btn').on('click', function () {
    $('.nav').toggleClass('showNav');
    $(this).toggleClass('animated2');
});

</script>
</div>
</body>
</html>
