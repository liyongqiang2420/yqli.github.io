<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <!--头部信息-->
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <!--title keywords description 请改为自己的-->
    <title>低调奋进</title>

    <!--网站favicon可以没有或者改为自己的-->
    <!--<link rel="shortcut icon" type="image/x-icon" href="http://www.bituplink.com/wp-content/uploads/favicon.png"/>-->

    <!--CSS 若不需要变动样式不用改-->
    <link href="plugin/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/zui/1.8.1/css/zui.min.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" type="text/css" href="../css/common.css" />
    <link href="../img/logo.ico" rel="shortcut icon" />
    <script src="plugin/jquery.min.js"></script>
    <script src="plugin/bootstrap/js/bootstrap.min.js"></script>
</head>
<body id="nav_body">
<!--[if lt IE 10]>
<div class="alert alert-danger">
    您正在使用 
    <strong>过时的</strong> 浏览器. 请更换一个更好的浏览器来提升用户体验.
</div>
<![endif]--><!--头部导航条-->
<div id="content">
    <div class="w_header">
      <div class="container">
        <div class="w_header_top">
          <a href="../index.html" class="w_logo"></a>
          <span class="w_header_nav">
              <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="speech.html" class="active">Speech & ML</a></li>
                <li><a href="pro.html">Programming</a></li>
                <li><a href="moodList.html">Life</a></li>
                <li><a href="tools.html">Tool</a></li>
                <li><a href="about.html">About</a></li>
            </ul>
        </span>
    </div>
</div>
</div>

<!--左侧Director，导航跳转-->
<div class="left-bar">
    <div class="header">
        <h2>Director</h2>
    </div>
    <div class="menu" id="menu">
        <ul class="scrollcontent">
            <!--左侧Director，按照需要修改和添加，参考已有的修改名称和href-->
            <li><a href="#row-1">General TTS</a></li>
            <li><a href="#row-2">Vocoder</a></li>
            <li><a href="#row-3">Adap & multi-lingual</a></li>
            <li><a href="#row-4">Expressive TTS</a></li>
            <li><a href="#row-5">Voice Conversion</a></li>
            <li><a href="#row-6">Sing Synthesis</a></li>
            <li><a href="#row-7">Talking Head</a></li>
            <li><a href="#row-8">Robust TTS</a></li>
            <li><a href="#row-9">Front End</a></li>
            <li><a href="#row-10">Alignment</a></li>
            <li><a href="#row-11">Dual learning</a></li>
            <li><a href="#row-12">EEG</a></li>
            <li><a href="#row-13">S2S</a></li>
            <li><a href="#row-14">Other</a></li>
        </ul>
    </div>
</div>
<!--内容-->
<div class="main">
    <div class="container content-box">
        <!--导航分类范例1，请根据自己的需求进行修改-->
        <section class="item card-box" id="row-1">
            <div class="container-fluid">
                <div class="row">
                    <div class="item-tit">
                        <strong>Journal and conference on speech</strong>
                        <table width="1150" border="1">
                            <tr>
                                <td width="150" align="center">CCF-A</a></td>
                                <td width="1000">NeuraIPS&nbsp;&nbsp;&nbsp;AAAI&nbsp;&nbsp;&nbsp;IJAI&nbsp;&nbsp;&nbsp;ACMMM </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">CCF-B</a></td>
                                <td width="1000">ICASSP&nbsp;&nbsp;&nbsp;COLING&nbsp;&nbsp;&nbsp;SpeechCom&nbsp;&nbsp;&nbsp;TSLP&nbsp;&nbsp;&nbsp;TASLP&nbsp;&nbsp;&nbsp;JSLHR&nbsp;&nbsp;&nbsp;TMM&nbsp;&nbsp;&nbsp;TOMCCAP&nbsp;&nbsp;&nbsp;ICME </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">CCF-C</a></td>
                                <td width="1000">INTERSPEECH&nbsp;&nbsp;&nbsp;ICPR </td>
                            </tr>
                            <tr>
                                <td width="150" align="center">other</a></td>
                                <td width="1000">ICLR </td>
                            </tr>
                        </table>
                    </div>
                    <div class="item-tit">
                        <strong>on going</strong>
                        <table width="1150" border="1">
                            <tr>
                                <td width="150" align="center">2021</a></td>
                                <td width="800">speech synthese </td>
                                <td width="200"><a href="https://docs.google.com/spreadsheets/d/1wLjhN1RITE39NjrmvP2zKNP7JmilImPgPANCvXl8BK8/edit?usp=sharing">pdf</a></td>
                            </tr>
                        </table>
                    </div>
                    <div class="item-tit">
                        <strong>General TTS</strong>
                    </div>
                    <!--获取内容列表-->
                    <h3> 2021 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Triple M: A Practical Neural Text-to-speech System With Multi-guidance Attention And Multi-band Multi-time Lpcnet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00247.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">VARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep VAE with Residual Attention</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.06431.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">LightSpeech: Lightweight and Fast Text to Speech with Neural Architecture Search</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.04040.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech </td>
                            <td width="200"><a href="https://openreview.net/pdf?id=o3iritJHLfO">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">ADASPEECH: ADAPTIVE TEXT TO SPEECH FOR CUSTOM VOICE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.00993.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Building Multilingual TTS using Cross-Lingual Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.14039.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Supervised and Unsupervised Approaches for Controlling Narrow Lexical Focus in Sequence-to-Sequence Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.09940.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Mixture Density Network for Phone-Level Prosody Modelling in Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00851.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.09914.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Data-Efficient Training Strategies for Neural TTS Systems</td>
                            <td width="200"><a href="https://dl.acm.org/doi/abs/10.1145/3430984.3431034">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Multilingual Byte2Speech Text-To-Speech Models Are Few-shot Spoken Language Learners</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.03541.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Text-to-speech for the hearing impaired</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.02174.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Continual Speaker Adaptation for Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.14512.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.14574.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.15060.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.05557.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Fast DCTTS: Efficient Deep Convolutional Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00624.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01409.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Multi-rate attention architecture for fast streamable Text-to-speech spectrum modeling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00705.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Flavored Tacotron: Conditional Learning for Prosodic-linguistic Features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.04050.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Speech Resynthesis from Discrete Disentangled Self-Supervised Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00355.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Dependency Parsing based Semantic Representation Learning with Graph Neural Network for Enhancing Expressiveness of Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06835.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Review of end-to-end speech synthesis technology based on deep learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.09995.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">dependency Parsing based Semantic Representation Learning with Graph Neural Network for Enhancing Expressiveness of Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06835.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">TalkNet 2: Non-Autoregressive Depth-Wise Separable Convolutional Model Stanislav Beliaev, Boris Ginsburgfor Speech Synthesis with Explicit Pitch and Duration Prediction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.08189.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Signal Representations for Synthesizing Audio Textures with Generative Adversarial Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.07390.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">SpeechNet: A Universal Modularized Model for Speech Processing Tasks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.03070.pdf">pdf</a>
                            &nbsp;&nbsp;<a href="https://mp.weixin.qq.com/s/858zn1pkTrL8mgN7GGw1Ug">blog</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">How do Voices from Past Speech Synthesis Challenges Compare Today?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.02373.pdf">pdf</a>
                            &nbsp;&nbsp;<a href="https://mp.weixin.qq.com/s?__biz=MzAxNjY3NjQwOQ==&mid=2247485167&idx=1&sn=159993dd0921ac04baec32d055ca0738&chksm=9bf065b9ac87ecafbb1d7d8e311494f8583d77a47ea9753c6afd22ca6ea0fb007a4bf9d60101&cur_album_id=1595929281711390722&scene=190#rd">blog</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Learning Robust Latent Representations for Controllable Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.04458.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">MASS: Multi-task Anthropomorphic Speech Synthesis Framework</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.04124.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">VQCPC-GAN: Variable-length Adversarial Audio Synthesis using Vector-Quantized Contrastive Predictive Coding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.01531.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">SpeechNet: A Universal Modularized Model for Speech Processing Tasks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.03070.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.06337.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">Ito^TTS and Ito^Wave: Linear Stochastic Differential Equation Is All You Need For Audio Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.07583.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">Diverse and Controllable Speech Synthesis with GMM-Based Phone-Level Prosody Modelling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.13086.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">A learned conditional prior for the VAE acoustic space of a TTS system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10229.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">A Survey on Neural Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15561.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">An objective evaluation of the effects of recording conditions and speaker characteristics in multi-speaker deep neural speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.01812.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">40</a></td>
                            <td width="800">Byakto Speech: Real-time long speech synthesis with convolutional neural network: Transfer learning from English to Bangla</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.03937.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">41</a></td>
                            <td width="800">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06103.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">42</a></td>
                            <td width="800">Controllable Context-aware Conversational Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10828.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">43</a></td>
                            <td width="800">Multi-Scale Spectrogram Modelling for Neural Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15649.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">44</a></td>
                            <td width="800">Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08352.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">45</a></td>
                            <td width="800">FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15123.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">46</a></td>
                            <td width="800">GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15153.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">47</a></td>
                            <td width="800">Hierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15144.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">48</a></td>
                            <td width="800">Improving multi-speaker TTS prosody variance with a residual encoder and normalizing flows</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.05762.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">49</a></td>
                            <td width="800">Non-native English lexicon creation for bilingual speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10870.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">50</a></td>
                            <td width="800">Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.02830.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">51</a></td>
                            <td width="800">Speaker verification-derived loss and data augmentation for DNN-based multispeaker speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.01789.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">52</a></td>
                            <td width="800">Speech BERT Embedding For Improving Prosody in Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.04312.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">53</a></td>
                            <td width="800">WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.09660.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">54</a></td>
                            <td width="800">Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.13479.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">55</a></td>
                            <td width="800">VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.03298.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">56</a></td>
                            <td width="800">Location, Location: Enhancing the Evaluation of Text-to-Speech Synthesis Using the Rapid Prosody Transcription Paradigm</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.02527.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">57</a></td>
                            <td width="800">Federated Learning with Dynamic Transformer for Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.08795.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">58</a></td>
                            <td width="800">Effective and Differentiated Use of Control Information for Multi-speaker Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/abs/2107.03065">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">59</a></td>
                            <td width="800">End to End Bangla Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.00500.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">60</a></td>
                            <td width="800">Perceptually Guided End-to-End Text-to-Speech With MOS Prediction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.01174.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">61</a></td>
                            <td width="800">One TTS Alignment To Rule Them All</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.10447.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">62</a></td>
                            <td width="800">Combining speakers of multiple languages to improve quality of neural voices</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.07737.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">63</a></td>
                            <td width="800">DeepEigen: Learning-based Modal Sound Synthesis with Acoustic Transfer Maps</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.07425.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">64</a></td>
                            <td width="800">Neural HMMs are all you need (for high-quality attention-free TTS)</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.13320.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">65</a></td>
                            <td width="800">PortaSpeech: Portable and High-Quality Generative Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.15166.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">66</a></td>
                            <td width="800">Nana-HDR: A Non-attentive Non-autoregressive Hybrid Model for TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.13673.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">67</a></td>
                            <td width="800">Low-Latency Incremental Text-to-Speech Synthesis with Distilled Context Prediction Network</td>
                            <td width="200"><a href=https://arxiv.org/pdf/2109.10724.pdf"">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">68</a></td>
                            <td width="800">An Audio Synthesis Framework Derived from Industrial Process Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.10455.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">69</a></td>
                            <td width="800">On-device neural speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.08710.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">70</a></td>
                            <td width="800">fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.06912.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">71</a></td>
                            <td width="800">A study on the efficacy of model pre-training in developing neural text-to-speech system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03857.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">72</a></td>
                            <td width="800">DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.12612.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">73</a></td>
                            <td width="800">Discrete acoustic space for an efficient sampling in neural text-to-speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.12539.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">74</a></td>
                            <td width="800">EdiTTS: Score-based Editing for Controllable Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02584.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">75</a></td>
                            <td width="800">Emphasis control for parallel neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03012.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">76</a></td>
                            <td width="800">Environment Aware Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03887.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">77</a></td>
                            <td width="800">ESPnet2-TTS: Extending the Edge of TTS Research</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07840.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">78</a></td>
                            <td width="800">FedSpeech: Federated Text-to-Speech with Continual Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07216.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">79</a></td>
                            <td width="800">Hierarchical prosody modeling and control in non-autoregressive parallel neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02952.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">80</a></td>
                            <td width="800">Mixer-TTS: non-autoregressive, fast and compact text-to-speech model conditioned on language model embeddings</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03584.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">81</a></td>
                            <td width="800">Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09698.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">82</a></td>
                            <td width="800">On the Interplay Between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.01147.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">83</a></td>
                            <td width="800">PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04486.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">84</a></td>
                            <td width="800">Prosody-TTS: An end-to-end speech synthesis system with prosody control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02854.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">85</a></td>
                            <td width="800">A study on the efficacy of model pre-training in developing neural text-to-speech system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03857.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">86</a></td>
                            <td width="800">Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10882.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">87</a></td>
                            <td width="800">Guided-TTS:Text-to-Speech with Untranscribed Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.11755.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">88</a></td>
                            <td width="800">Improved Prosodic Clustering for Multispeaker and Speaker-independent Phoneme-level Prosody Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10168.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">89</a></td>
                            <td width="800">Improving Prosody for Unseen Texts in Speech Synthesis by Utilizing Linguistic Information and Noisy Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07549.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">90</a></td>
                            <td width="800">More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10139.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">91</a></td>
                            <td width="800">Prosodic Clustering for Phoneme-level Prosody Control in End-to-End Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10177.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">92</a></td>
                            <td width="800">RefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.00962.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">93</a></td>
                            <td width="800">Speaker Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.05095.pdf">pdf</a></td>
                        </tr>
                    </table>

                    <h3> 2020 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">INTERACTIVE TEXT-TO-SPEECH VIA SEMI-SUPERVISED STYLE TRANSFER LEARNING</td>
                            <td width="200"><a href="../pdf/tts_paper/INTERACTIVE TEXT-TO-SPEECH VIA SEMI-SUPERVISED STYLE TRANSFER LEARNING.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">SQUEEZEWAVE EXTREMELY LIGHTWEIGHT VOCODERS FOR ON DEVICE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/SQUEEZEWAVE EXTREMELY LIGHTWEIGHT VOCODERS FOR ON DEVICE SPEECH SYNTHESIS.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://tianrengao.github.io/SqueezeWaveDemo/">demo</a>
                                &nbsp;&nbsp;<a href="https://github.com/tianrengao/SqueezeWave">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">LOCATION RELATIVE ATTENTION MECHANISMS FOR ROBUST LONG FORM SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/LOCATION RELATIVE ATTENTION MECHANISMS FOR ROBUST LONG FORM SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">End to End Adversarial Text to Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/End to End Adversarial Text to Speech.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://deepmind.com/research/publications/End-to-End-Adversarial-Text-to-Speech">demo</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">FastSpeech 2 Fast and High Quality End to End Text to Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/FastSpeech 2 Fast and High Quality End to End Text to Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Deep Representation Learning in Speech Processing Challenges Recent Advances and Future Trends</td>
                            <td width="200"><a href="../pdf/tts_paper/Deep Representation Learning in Speech Processing Challenges Recent Advances and Future Trends.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Flowtron an Autoregressive Flowbased Generative Network for TexttoSpeech Synthesis.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/Flowtron an Autoregressive Flowbased Generative Network for TexttoSpeech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">JDI-T- Jointly trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment</td>
                            <td width="200"><a href="../pdf/tts_paper/JDI-T- Jointly trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">FastPitch- Parallel Text-to-speech with Pitch Prediction.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/FastPitch- Parallel Text-to-speech with Pitch Prediction.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Glow-TTS- A Generative Flow for Text-to-Speech via Monotonic Alignment Search.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/Glow-TTS- A Generative Flow for Text-to-Speech via Monotonic Alignment Search.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">FLOW-TTS: A NON-AUTOREGRESSIVE NETWORK FOR TEXT TO SPEECH BASED ON FLOW</td>
                            <td width="200"><a href="../pdf/tts_paper/FLOW-TTS: A NON-AUTOREGRESSIVE NETWORK FOR TEXT TO SPEECH BASED ON FLOW.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">SpeedySpeech- Efficient Neural Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/SpeedySpeech- Efficient Neural Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">End-to-End Adversarial Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/End-to-End Adversarial Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Controllable Neural Prosody Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Controllable Neural Prosody Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling</td>
                            <td width="200"><a href="../pdf/tts_paper/Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Exploring TTS without T Using Biologically/Psychologically Motivated Neural Network Modules (ZeroSpeech 2020)</td>
                            <td width="200"><a href="../pdf/tts_paper/Exploring TTS without T Using Biologically/Psychologically Motivated Neural Network Modules (ZeroSpeech 2020).pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">From Speaker Verification to Multispeaker Speech Synthesis, Deep Transfer with Feedback Constraint</td>
                            <td width="200"><a href="../pdf/tts_paper/From Speaker Verification to Multispeaker Speech Synthesis, Deep Transfer with Feedback Constraint.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Incremental Text to Speech for Neural Sequence-to-Sequence Models using Reinforcement Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/Incremental Text to Speech for Neural Sequence-to-Sequence Models using Reinforcement Learning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit</td>
                            <td width="200"><a href="../pdf/tts_paper/Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages</td>
                            <td width="200"><a href="../pdf/tts_paper/Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Speaking Speed Control of End-to-End Speech Synthesis using Sentence-Level Conditioning</td>
                            <td width="200"><a href="../pdf/tts_paper/Speaking Speed Control of End-to-End Speech Synthesis using Sentence-Level Conditioning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">NON-ATTENTIVE TACOTRON- ROBUST AND CONTROLLABLE NEURAL TTS SYNTHESIS INCLUDING UNSUPERVISED DURATION MODELING</td>
                            <td width="200"><a href="../pdf/tts_paper/NON-ATTENTIVE TACOTRON- ROBUST AND CONTROLLABLE NEURAL TTS SYNTHESIS INCLUDING UNSUPERVISED DURATION MODELING.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">PARALLEL TACOTRON- NON-AUTOREGRESSIVE AND CONTROLLABLE TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/PARALLEL TACOTRON- NON-AUTOREGRESSIVE AND CONTROLLABLE TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">TTS-BY-TTS- TTS-DRIVEN DATA AUGMENTATION FOR FAST AND HIGH-QUALITY SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/TTS-BY-TTS- TTS-DRIVEN DATA AUGMENTATION FOR FAST AND HIGH-QUALITY SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">SPEECH SYNTHESIS AND CONTROL USING DIFFERENTIABLE DSP</td>
                            <td width="200"><a href="../pdf/tts_paper/SPEECH SYNTHESIS AND CONTROL USING DIFFERENTIABLE DSP.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">FEATHERTTS- ROBUST AND EFFICIENT ATTENTION BASED NEURAL TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/FEATHERTTS- ROBUST AND EFFICIENT ATTENTION BASED NEURAL TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">GRAPHSPEECH: SYNTAX-AWARE GRAPH ATTENTION NETWORK FOR NEURAL SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/GRAPHSPEECH- SYNTAX-AWARE GRAPH ATTENTION NETWORK FOR NEURAL SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">HIERARCHICAL PROSODY MODELING FOR NON-AUTOREGRESSIVE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/HIERARCHICAL PROSODY MODELING FOR NON-AUTOREGRESSIVE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">DEVICETTS: A SMALL-FOOTPRINT, FAST, STABLE NETWORK FOR ON-DEVICE TEXT-TO-SPEECH</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.15311.pdf">pdf</a></td>
                        </tr>
                        </tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">PRETRAINING STRATEGIES, WAVEFORM MODEL CHOICE, AND ACOUSTIC CONFIGURATIONS FOR MULTI-SPEAKER END-TO-END SPEECH SYNTHESIS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.04839.pdf">pdf</a></td>
                        </tr>
                        </tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">Fast and lightweight on-device TTS with Tacotron2 and LPCNet</td>
                            <td width="200"><a href="https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2169.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2019 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">2019</td>
                            <td width="800"> isca 2019 speech </td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2019/">papers</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Deep Text-to-Speech System with Seq2Seq Model</td>
                            <td width="200"><a href="../pdf/tts_paper/Deep_Text-to-Speech_System_with_Seq2Seq_Model.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">FastSpeech: Fast, Robust and Controllable Text to Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/FastSpeech_Fast_Robust_and_Controllable_Text_to_Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Neural Speech Synthesis with Transformer Network</td>
                            <td width="200"><a href="../pdf/tts_paper/Neural_Speech_Synthesis_with_Transformer_Network.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="../pdf/tts_paper/tts transformer .pptx">ppt</a>
                                &nbsp;&nbsp;<a href="https://neuraltts.github.io/transformertts/">demo</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Parallel Neural Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/Parallel Neural Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/libriTTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</td>
                            <td width="800">Forward-Backward Decoding for Regularizing End-to-End TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Forward-Backward Decoding for Regularizing End-to-End TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</td>
                            <td width="800">Self-attention Based Prosodic Boundary Prediction for Chinese Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Self-attention Based Prosodic Boundary Prediction for Chinese Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Guide to Speech Synthesis with Deep Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/Guide to Speech Synthesis with Deep Learning.pdf">ppt</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">tts tutorial part1 part2</td>
                            <td width="200"><a href="../pdf/tts_paper/tts tutorial part1.pdf">ppt1</a>
                                &nbsp;&nbsp;<a href="../pdf/tts_paper/tts_tutorial part2.pdf">ppt2</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">maximizing mutual information for tacotron</td>
                            <td width="200"><a href="../pdf/tts_paper/MAXIMIZING MUTUAL INFORMATION FOR TACOTRON.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</td>
                            <td width="800">durlan</td>
                            <td width="200"><a href="../pdf/tts_paper/DURIAN DURATION INFORMED ATTENTION NETWORK FOR MULTIMODAL SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</td>
                            <td width="800">Non-Autoregressive Neural Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/Non-Autoregressive Neural Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Tacotron-based acoustic model using phoneme alignment for practical neural text-to-speech systems</td>
                            <td width="200"><a href="../pdf/tts_paper/Tacotron-based acoustic model using phoneme alignment for practical neural text-to-speech systems.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2018 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">2018</td>
                            <td width="800"> isca 2018 speech </td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2018/">papers</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Deep voice 3: Scaling text-to-speech with convolutional sequence learning</td>
                            <td width="200"><a href="../pdf/tts_paper/deep_voice3.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">ClariNet Parallel Wave Generation in End-to-End Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/ClariNet Parallel Wave Generation in End-to-End Text-to-Speech.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Linear Networks Based Speaker Adaptation For Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Linear Networks Based Speaker Adaptation For Speech Synthesis.pdf">pdf</a>
                            </td>
                        </tr>
                    </table>    
                    <h3> 2017 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">2017</td>
                            <td width="800"> isca 2017 speech </td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2017/">papers</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Tacotron: Towards End-to-End Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/tacotron.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://google.github.io/tacotron/index.html">page</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Char2Wav: End-to-End Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Char2Wav-End-to-End_Speech_Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Deep Voice: Real-time Neural Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/Deep_Voice-Real-time_Neural_Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Deep Voice 2: Multi-Speaker Neural Text-to-Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/DeepVoice2-Multi-Speaker_Neural_Text-to-Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">VoiceLoop voice fitting and synthesis via a phonological loop</td>
                            <td width="200"><a href="../pdf/tts_paper/VOICELOOP- VOICE FITTING AND SYNTHESIS VIA A PHONOLOGICAL LOOP.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Attention Is All You Need</td>
                            <td width="200"><a href="../pdf/tts_paper/Attention Is All You Need.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2016 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">2016</td>
                            <td width="800"> isca 2016 speech </td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2016/">papers</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices</td>
                            <td width="200"><a href="../pdf/tts_paper/Fast_Compact_and_High_Quality_LSTM-RNN_Based_Statistical_Parametric.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Merlin: An Open Source Neural Network Speech Synthesis System</td>
                            <td width="200"><a href="../pdf/tts_paper/Merlin_An_Open_Source_Neural_Network_Speech_Synthesis_System.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2015 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Acoustic modeling instatistical parametric speechsynthesis-from HMM to LSTM-RNN</td>
                            <td width="200"><a href="../pdf/tts_paper/Acoustic_modeling_instatistical_parametric_speechsynthesis-from_HMM_to_LSTM-RNN.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="../pdf/tts_paper/Statistical Parametric Speech SynthesisFromHMMtoLSTMRNN_ppt.pdf">ppt</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Effective Approaches to Attention-based Neural Machine Translation</td>
                            <td width="200"><a href="../pdf/tts_paper/Effective Approaches to Attention-based Neural Machine Translation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">htkbook-3.5</td>
                            <td width="200"><a href="../pdf/tts_paper/htkbook-3.5.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">A study of speaker adaptation for DNN-based speech synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/A study of speaker adaptation for DNN-based speech synthesis.pdf">pdf</a></td>
                        </tr>
                    </table>
                    <h3> 2014 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks </td>
                            <td width="200"><a href="../pdf/tts_paper/TTS_Synthesis_with_Bidirectional_LSTM_based_Recurrent_Neural_Networks.pdf">pdf</a>
                            </td>
                        </tr>
                    </table>
                    <h3> 2013 </h3>
                    <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Statical parameteric speech synthesis Using deep neural networks </td>
                            <td width="200"><a href="../pdf/tts_paper/Statical parameteric speech synthesis Using deep neural networks.pdf">pdf</a></td>
                        </tr>
                    </table>
                </div>
            </div>
        </section>
<section class="item card-box" id="row-2">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Vocoder</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                <tr>
                    <td width="150" align="center">1</a></td>
                    <td width="800">GAN Vocoder: Multi-Resolution Discriminator Is All You Need</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2103.05236.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">2</a></td>
                    <td width="800">Improved parallel WaveGAN vocoder with perceptually weighted spectrogram loss</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2101.07412.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">3</a></td>
                    <td width="800">Universal Neural Vocoding with Parallel WaveNet</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2102.01106.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">4</a></td>
                    <td width="800">LVCNet: Efficient Condition-Dependent Modeling Network for Waveform Generation</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2102.10815.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">5</a></td>
                    <td width="800">High-Quality Vocoding Design with Signal Processing for Speech Synthesis and Voice Conversion</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2101.10278.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">6</a></td>
                    <td width="800">Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform Generation in Multiple Domains</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2011.09631.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">7</a></td>
                    <td width="800">Improve GAN-based Neural Vocoder using Pointwise Relativistic LeastSquare GAN</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2103.14245.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">8</a></td>
                    <td width="800">Unified Source-Filter GAN: Unified Source-filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2104.04668.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">9</a></td>
                    <td width="800">Reconstructing Speech from Real-Time Articulatory MRI Using Neural Vocoders</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2104.11598.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">10</a></td>
                    <td width="800">High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2105.09856.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">11</a></td>
                    <td width="800">a generative model for raw audio using transformer architectures</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.16036.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">12</a></td>
                    <td width="800">WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.08507.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">13</a></td>
                    <td width="800">Advances in Speech Vocoding for Text-to-Speech with Continuous Parameters</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.10481.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">14</a></td>
                    <td width="800">Basis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.13419.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">15</a></td>
                    <td width="800">Catch-A-Waveform: Learning to Generate Audio from a Single Short Example</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.06426.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">16</a></td>
                    <td width="800">Continuous Wavelet Vocoder-based Decomposition of Parametric Speech Waveform Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.06426.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">17</a></td>
                    <td width="800">CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.07431.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">18</a></td>
                    <td width="800">Fre-GAN: Adversarial Frequency-consistent Audio Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.02297.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">19</a></td>
                    <td width="800">Glow-WaveGAN: Learning Speech Representations from GAN-based Variational Auto-Encoder For High Fidelity Flow-based Speech Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.10831.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">20</a></td>
                    <td width="800">Improving the expressiveness of neural vocoding with non-affine Normalizing Flows</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.08649.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">21</a></td>
                    <td width="800">Mathematical Vocoder Algorithm : Modified Spectral Inversion for Efficient Neural Speech Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.03167.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">22</a></td>
                    <td width="800">Relational Data Selection for Data Augmentation of Speaker-dependent Multi-band MelGAN Vocoder</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.05629.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">23</a></td>
                    <td width="800">UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.07889.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">24</a></td>
                    <td width="800">WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.08507.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">25</a></td>
                    <td width="800">A GENERATIVE MODEL FOR RAW AUDIO USING TRANSFORMER ARCHITECTURES</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.08507.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">26</a></td>
                    <td width="800">Neural Waveshaping Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2107.05050.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">27</a></td>
                    <td width="800">A Generative Model for Raw Audio Using Transformer Architectures</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2106.16036.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">28</a></td>
                    <td width="800">DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis with GANs</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2108.01216.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">29</a></td>
                    <td width="800">Ito^TTS and Ito^Wave: Linear Stochastic Differential Equation Is All You Need For Audio Generation</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2105.07583.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">31</a></td>
                    <td width="800">A Streamwise GAN Vocoder for Wideband Speech Coding at Very Low Bit Rate</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2108.04051.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">32</a></td>
                    <td width="800">FlowVocoder: A small Footprint Neural Vocoder based Normalizing flow for Speech Synthesis</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2109.13675.pdf">pdf</a></td>
                </tr>
                <tr>
                    <td width="150" align="center">33</a></td>
                    <td width="800">MSR-NV: Neural vocoder using multiple sampling rates</td>
                    <td width="200"><a href="https://arxiv.org/pdf/2109.13714.pdf">pdf</a></td>
                </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Chunked Autoregressive GAN for Conditional Waveform Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.10139.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">Neural Analysis and Synthesis: Reconstructing Speech from Self-Supervised Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.14513.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">Neural Pitch-Shifting and Time-Stretching with Controllable LPCNet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02360.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">Neural Synthesis of Footsteps Sound Effects with Generative Adversarial Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09605.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">The Mirrornet : Learning Audio Synthesizer Controls Inspired by Sensorimotor Interaction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05695.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">Towards Universal Neural Vocoding with a Multi-band Excited WaveNet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03329.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">40</a></td>
                            <td width="800">High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latenc</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09052.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">41</a></td>
                            <td width="800">RAVE: A variational autoencoder for fast and high-quality neural audio synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.05011.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">multiband melgan</td>
                            <td width="200"><a href="../pdf/tts_paper/multiband melgan.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">FeatherWave An efficient high fidelity neural vocoder with multiband linear prediction</td>
                            <td width="200"><a href="../pdf/tts_paper/FeatherWave An efficient high fidelity neural vocoder with multiband linear prediction.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">parallel wavegan</td>
                            <td width="200"><a href="../pdf/tts_paper/parallel wavegan.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">VocGan</td>
                            <td width="200"><a href="../pdf/tts_paper/VocGAN- A High-Fidelity Real-time Vocoder with a Hierarchically nested Adversarial Network.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">WAVEGRAD- ESTIMATING GRADIENTS FOR WAVEFORM GENERATION</td>
                            <td width="200"><a href="../pdf/tts_paper/WAVEGRAD- ESTIMATING GRADIENTS FOR WAVEFORM GENERATION.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">PARALLEL WAVEGAN- A FAST WAVEFORM GENERATION MODEL BASED ON GENERATIVE ADVERSARIAL NETWORKS WITH MULTI-RESOLUTION SPECTROGRAM</td>
                            <td width="200"><a href="../pdf/tts_paper/PARALLEL WAVEGAN- A FAST WAVEFORM GENERATION MODEL BASED ON GENERATIVE ADVERSARIAL NETWORKS WITH MULTI-RESOLUTION SPECTROGRAM.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">WAVEGRAD  ESTIMATING GRADIENTS FOR WAVEFORM GENERATION</td>
                            <td width="200"><a href="../pdf/tts_paper/WAVEGRAD  ESTIMATING GRADIENTS FOR WAVEFORM GENERATION.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">A Cyclical Post-filtering Approach to Mismatch Refinement of Neural Vocoder for Text-to-speech Systems</td>
                            <td width="200"><a href="../pdf/tts_paper/A Cyclical Post-filtering Approach to Mismatch Refinement of Neural Vocoder for Text-to-speech Systems.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Bunched LPCNet - Vocoder for Low-cost Neural Text-To-Speech Systems</td>
                            <td width="200"><a href="../pdf/tts_paper/Bunched LPCNet - Vocoder for Low-cost Neural Text-To-Speech Systems.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Quasi-Periodic Parallel WaveGAN Vocoder- A Non-autoregressive Pitchdependent Dilated Convolution Model for Parametric Speech Generation</td>
                            <td width="200"><a href="../pdf/tts_paper/Quasi-Periodic Parallel WaveGAN Vocoder- A Non-autoregressive Pitchdependent Dilated Convolution Model for Parametric Speech Generation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Neural Text-to-Speech with a Modeling-by-Generation Excitation Vocoder</td>
                            <td width="200"><a href="../pdf/tts_paper/Neural Text-to-Speech with a Modeling-by-Generation Excitation Vocoder.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Improving Opus Low Bit Rate Quality with Neural Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Improving Opus Low Bit Rate Quality with Neural Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">WG-WaveNet- Real-Time High-Fidelity Speech Synthesis without GPU</td>
                            <td width="200"><a href="../pdf/tts_paper/WG-WaveNet- Real-Time High-Fidelity Speech Synthesis without GPU.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Vocoder-Based Speech Synthesis from Silent Videos</td>
                            <td width="200"><a href="../pdf/tts_paper/Vocoder-Based Speech Synthesis from Silent Videos.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Ultrasound-based Articulatory-to-Acoustic Mapping with WaveGlow Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Ultrasound-based Articulatory-to-Acoustic Mapping with WaveGlow Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Speaker Conditional WaveRNN- Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions</td>
                            <td width="200"><a href="../pdf/tts_paper/Speaker Conditional WaveRNN- Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">GAUSSIAN LPCNET FOR MULTISAMPLE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/GAUSSIAN LPCNET FOR MULTISAMPLE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">UNIVERSAL MELGAN: A ROBUST NEURAL VOCODER FOR HIGH-FIDELITY WAVEFORM GENERATION IN MULTIPLE DOMAINS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.09631.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Improving LPCNet-based Text-to-Speech with Linear Prediction-structured Mixture Density Network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2001.11686.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">What the Future Brings: Investigating the Impact of Lookahead for Incremental Neural TTS</td>
                            <td width="200"><a href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/2103.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Lightweight LPCNet-based Neural Vocoder with Tensor Decomposition</td>
                            <td width="200"><a href="http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=247&id=348">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2010.05646.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">High quality lightweight and adaptable TTS using LPCNet</td>
                            <td width="200"><a href="../pdf/tts_paper/High quality lightweight and adaptable TTS using LPCNet.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">A Neural Vocoder with Hierarchical Generation of Amplitude and Phase Spectra for Statistical Parametric Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/A_Neural_Vocoder_with_Hierarchical_Generation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">RawNet: Fast End-to-End Neural Vocoder</td>
                            <td width="200"><a href="../pdf/tts_paper/RawNet Fast End-to-End Neural Vocoder.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet</td>
                            <td width="200"><a href="../pdf/tts_paper/A Real-Time Wideband Neural Vocoder at 1.6 kbs Using LPCNet.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">Lpcnet improving neural speech synthesis through linear prediction</td>
                            <td width="200"><a href="../pdf/tts_paper/LPCNET: IMPROVING NEURAL SPEECH SYNTHESIS THROUGH LINEAR PREDICTION.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://people.xiph.org/~jm/demo/lpcnet/">demo</a>
                                &nbsp;&nbsp;<a href="https://github.com/drowe67/LPCNet">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Waveglow</td>
                            <td width="200"><a href="../pdf/tts_paper/waveglow.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">melgan</td>
                            <td width="200"><a href="../pdf/tts_paper/MelGAN Generative Adversarial Networks for Conditional Waveform Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">AN INVESTIGATION OF SUBBAND WAVENET VOCODER COVERING ENTIRE AUDIBLE FREQUENCY RANGE WITH LIMITED ACOUSTIC FEATURES</td>
                            <td width="200"><a href="../pdf/tts_paper/AN INVESTIGATION OF SUBBAND WAVENET VOCODER COVERING ENTIRE AUDIBLE FREQUENCY RANGE WITH LIMITED ACOUSTIC FEATURES.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">A Comparison of Recent Neural Vocoders for Speech Signal Reconstruction</td>
                            <td width="200"><a href="https://pdfs.semanticscholar.org/093a/804dc251dbd68b190918e180707bd1f66e4b.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Natural TTS Synthesis by Conditioning Wavennet on MEL spectrogram predictions(tacotron2)</td>
                            <td width="200"><a href="../pdf/tts_paper/Natural_TTS_Synthesis by_Conditioning_Wavennet_on_MEL_spectrogram_predictions.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://github.com/NVIDIA/tacotron2">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Efficient Neural Audio Synthesis (WaveRNN)</td>
                            <td width="200"><a href="../pdf/tts_paper/Efficient Neural Audio Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Improving FFTNet vocoder with noise shaping and subband approaches</td>
                            <td width="200"><a href="../pdf/tts_paper/Improving FFTNet vocoder with noise shaping and subband approaches.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">FFTNET: A REAL-TIME SPEAKER-DEPENDENT NEURAL VOCODER</td>
                            <td width="200"><a href="../pdf/tts_paper/FFTNET: A REAL-TIME SPEAKER-DEPENDENT NEURAL VOCODER.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">SQUEEZEWAVE: EXTREMELY LIGHTWEIGHT VOCODERS FOR ON-DEVICE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/SQUEEZEWAVE: EXTREMELY LIGHTWEIGHT VOCODERS FOR ON-DEVICE SPEECH SYNTHESIS.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://github.com/tianrengao/SqueezeWave">code</a>
                            </td>
                        </tr>
            </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Parallel_WaveNet-Fast_High-Fidelity_Speech_Synthesis.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2016 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Wavenet A Generative Model For Raw Audio</td>
                            <td width="200"><a href="../pdf/tts_paper/Wavenet_A_Generative_Model_For_Raw_Audio.pdf">pdf</a>&nbsp;&nbsp;
                                <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">demo</a>
                                &nbsp;&nbsp;<a href="https://github.com/ibab/tensorflow-wavenet">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Fast Wavenet Geneartion Algorithm</td>
                            <td width="200"><a href="../pdf/tts_paper/Fast_Wavenet_Geneartion_Algorithm.pdf">pdf</a></td>
                        </tr>
            </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-3">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Adap & Multispeaker & Multilingual</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Building Multilingual TTS using Cross-Lingual Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.14039.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">ADASPEECH: ADAPTIVE TEXT TO SPEECH FOR CUSTOM VOICE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.00993.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.04088.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Voice Cloning: a Multi-Speaker Text-to-Speech Synthesis Approach based on Transfer Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.05630.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">CUHK-EE voice cloning system for ICASSP 2021 M2VoC challenge</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.04699.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Real-time Timbre Transfer and Sound Synthesis using DDSP</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.07220.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">The Multi-speaker Multi-style Voice Cloning Challenge 2021</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01818.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">The AS-NU System for the M2VoC Challenge</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.03009.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Exploring Disentanglement with Multilingual and Monolingual VQ-VAE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.01573.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.03153.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Speaker Adaptation with Continuous Vocoder-based DNN-TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.01154.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">GC-TTS: Few-shot Speaker Adaptation with Geometric Constraints</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.06890.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.05426.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Adapting TTS models For New Speakers using Transfer Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05798.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Cloning one's voice using very limited data in the wild</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03347.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Adapting TTS models For New Speakers using Transfer Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05798.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Applying Phonological Features in Multilingual Text-To-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03609.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07192.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07210.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Revisiting IPA-based Cross-lingual Text-to-speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07187.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04482.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Cross-lingual Low Resource Speaker Adaptation Using Phonological Features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09075.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.04040.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">V2C: Visual Voice Cloning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.12890.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Cross lingual Multispeaker Text to Speech under Limited Data Scenario</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 Cross lingual Multispeaker Text to Speech under Limited Data Scenario.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Efficient neural speech synthesis for low resource languages through multilingual modeling</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 Efficient neural speech synthesis for low resource languages through multilingual modeling.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">EndtoEnd Code Switching TTS with Cross Lingual Language Model</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 EndtoEnd Code Switching TTS with Cross Lingual Language Model.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">One Model Many Languages  Meta learning for Multilingual Text to Speech</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 One Model Many Languages  Meta learning for Multilingual Text to Speech.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">SPEAKER ADAPTATION OF A MULTILINGUAL ACOUSTIC MODEL FOR CROSS-LANGUAGE SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/2020 SPEAKER ADAPTATION OF A MULTILINGUAL ACOUSTIC MODEL FOR CROSS-LANGUAGE SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</td>
                            <td width="800">Multilingual speech synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Multilingual speech synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</td>
                            <td width="800">Domain-adversarial training of multi-speaker TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Domain adversarial training of multi speaker TTS .pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Focusing on Attention Prosody Transfer and Adaptative Optimization Strategy for Multi Speaker End to End Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Focusing on Attention Prosody Transfer and Adaptative Optimization Strategy for Multi Speaker End to End Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Zero Shot Multi Speaker Text To Speech with State of the art Neural Speaker Embeddings</td>
                            <td width="200"><a href="../pdf/tts_paper/Zero Shot Multi Speaker Text To Speech with State of the art Neural Speaker Embeddings.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Multi-speaker Text-to-speech Synthesis Using Deep Gaussian Processes</td>
                            <td width="200"><a href="../pdf/tts_paper/Multi-speaker Text-to-speech Synthesis Using Deep Gaussian Processes.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Phonological Features for 0-shot Multilingual Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Phonological Features for 0-shot Multilingual Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Semi-supervised Learning for Multi-speaker Text-to-speech Synthesis Using Discrete Speech Representation</td>
                            <td width="200"><a href="../pdf/tts_paper/Semi-supervised Learning for Multi-speaker Text-to-speech Synthesis Using Discrete Speech Representation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">USING IPA-BASED TACOTRON FOR DATA EFFICIENT CROSS-LINGUAL SPEAKER ADAPTATION AND PRONUNCIATION ENHANCEMENT</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.06392.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Cross lingual Multi speaker Texttospeech Synthesis for Voice Cloning without Using Parallel Corpus for Unseen Speakers</td>
                            <td width="200"><a href="../pdf/tts_paper/2019 Cross lingual Multi speaker Texttospeech Synthesis for Voice Cloning without Using Parallel Corpus for Unseen Speakers.pdf ">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Learning to Speak Fluently in a Foreign Language Multilingual Speech Synthesis and Cross Language Voice Cloning</td>
                            <td width="200"><a href="../pdf/tts_paper/2019 Learning to Speak Fluently in a Foreign Language Multilingual Speech Synthesis and Cross Language Voice Cloning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">个性化语音合成中说话人特征不同嵌入方式的研究</td>
                            <td width="200"><a href="../pdf/tts_paper/2019 个性化语音合成中说话人特征不同嵌入方式的研究.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Cross lingual Multispeaker Text To Speech Synthesis Using Neural Speaker Embedding</td>
                            <td width="200"><a href="../pdf/tts_paper/Cross lingual Multispeaker Text To Speech Synthesis Using Neural Speaker Embedding.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">Automatic Multispeaker Voice Cloning</td>
                            <td width="200"><a href="../pdf/tts_paper/Automatic Multispeaker Voice Cloning.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://google.github.io/tacotron/publications/speaker_adaptation/">demo</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</td>
                            <td width="800">Training Multi-Speaker Neural Text-to-Speech Systems using Speaker-Imbalanced Speech Corpora</td>
                            <td width="200"><a href="../pdf/tts_paper/Training Multi-Speaker Neural Text-to-Speech Systems using Speaker-Imbalanced Speech Corpora.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Speaker adaptation in DNN-based speech synthesis using d-vectors</td>
                            <td width="200"><a href="../pdf/tts_paper/Speaker adaptation in DNNbased speech synthesis using dvectors.pdf">pdf</a>
                            </td>
                        </tr>
          </table>
            <h3> 2016 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Speaker Representations for Speaker Adaptation in Multiple Speakers’BLSTM-RNN-based Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Speaker Representations for Speaker Adaptation in Multiple Speakers’BLSTM-RNN-based Speech Synthesis.pdf">pdf</a></td>
                        </tr>
          </table>
            <h3> 2015 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Multi-speaker modeling and speaker adaptation for dnn-based tts synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Multispeaker modeling and speaker adaptation for dnnbased tts synthesis.pdf">pdf</a>
                            </td>
                        </tr>
          </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-4">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Expressive TTS</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Whispered and Lombard Neural Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.05313.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Expressive Neural Voice Cloning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00151.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Model architectures to extrapolate emotional expressions in DNN-based text-to-speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.10345.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Analysis and Assessment of Controllability of an Expressive Deep Learning-based TTS system</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.04097.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">STYLER: Style Modeling with Rapidity and Robustness via SpeechDecomposition for Expressive and Controllable Neural Text to Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.09474.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Expressive Text-to-Speech using Style Tag</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00436.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01408.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Towards Multi-Scale Style Control for Expressive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.03521.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">AdaSpeech 2: Adaptive Text to Speech with Untranscribed Data</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.09715.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Exploring emotional prototypes in a high dimensional TTS latent space</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.01891.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Global Rhythm Style Transfer Without Text Transcriptions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08519.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Improving Performance of Seen and Unseen Speech Style Transfer in End-to-end Neural TTS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10003.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Non-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.12896.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Spoken Style Learning with Multi-modal Hierarchical Context Encoding for Conversational Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06233.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">UniTTS: Residual Learning of Unified Embedding Space for Speech Style Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.11171.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.12562.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.02530.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Daft-Exprt: Robust Prosody Transfer Across Speakers for Expressive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.02271.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Information Sieve: Content Leakage Reduction in End-to-End Prosody For Expressive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.01831.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Enhancing audio quality for expressive Neural Text-to-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.06270.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Emotional Speech Synthesis for Companion Robot to Imitate Professional Caregiver Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.12787.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Controllable cross-speaker emotion transfer for end-to-end speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.06733.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Cross-speaker Emotion Transfer Based on Speaker Condition Layer Normalization and Semi-Supervised Training in Text-To-Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04153.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">GANtron: Emotional Speech Synthesis with Generative Adversarial Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03390.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">Improving Emotional Speech Synthesis by Using SUS-Constrained VAE and Text Encoder Aggregation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09780.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">StrengthNet: Deep Learning-based Emotion Strength Assessment for Emotional Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03156.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Fine-grained style control in Transformer-based Text-to-speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06306.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Using multiple reference audios and style embedding constraints for speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04451.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">Emotional Prosody Control for Speech Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.04730.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Meta-Voice: Fast few-shot style transfer for expressive voice cloning using meta learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07218.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Word-Level Style Control for Expressive, Non-attentive Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.10173.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Controllable Neural Prosody Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Controllable Neural Prosody Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">FULLY-HIERARCHICAL FINE-GRAINED PROSODY MODELING FOR INTERPRETABLE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/FULLY-HIERARCHICAL FINE-GRAINED PROSODY MODELING FOR INTERPRETABLE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Flowtron- an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Flowtron- an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Enhancing Speech Intelligibility in Text-To-Speech Synthesis using Speaking Style Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Enhancing Speech Intelligibility in Text-To-Speech Synthesis using Speaking Style Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Controllable Emotion Transfer For End-to-End Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08679.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Fine-grained Emotion Strength Transfer, Control and Prediction for Emotional Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08477.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">MULTI-REFERENCE NEURAL TTS STYLIZATION WITH ADVERSARIAL CYCLE CONSISTENCY</td>
                            <td width="200"><a href="../pdf/tts_paper/MULTI-REFERENCE NEURAL TTS STYLIZATION WITH ADVERSARIAL CYCLE CONSISTENCY.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Multi-reference Tacotron by Intercross Training for Style Disentangling, Transfer and Control in Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Multi-reference Tacotron by Intercross Training for Style Disentangling, Transfer and Control in Speech Synthesis.pdf">pdf</a></td>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Controllable Neural Prosody Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Controllable Neural Prosody Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">FULLY-HIERARCHICAL FINE-GRAINED PROSODY MODELING FOR INTERPRETABLE SPEECH SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/FULLY-HIERARCHICAL FINE-GRAINED PROSODY MODELING FOR INTERPRETABLE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Flowtron- an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Flowtron- an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Enhancing Speech Intelligibility in Text-To-Speech Synthesis using Speaking Style Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Enhancing Speech Intelligibility in Text-To-Speech Synthesis using Speaking Style Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Controllable Emotion Transfer For End-to-End Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08679.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Fine-grained Emotion Strength Transfer, Control and Prediction for Emotional Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08477.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">MULTI-REFERENCE NEURAL TTS STYLIZATION WITH ADVERSARIAL CYCLE CONSISTENCY</td>
                            <td width="200"><a href="../pdf/tts_paper/MULTI-REFERENCE NEURAL TTS STYLIZATION WITH ADVERSARIAL CYCLE CONSISTENCY.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Multi-reference Tacotron by Intercross Training for Style Disentangling, Transfer and Control in Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Multi-reference Tacotron by Intercross Training for Style Disentangling, Transfer and Control in Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">MELLOTRON- MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS</td>
                            <td width="200"><a href="../pdf/tts_paper/MELLOTRON- MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">HIERARCHICAL GENERATIVE MODELING FOR CONTROLLABLE SPEECH SYNTHESIS.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/HIERARCHICAL GENERATIVE MODELING FOR CONTROLLABLE SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron</td>
                            <td width="200"><a href="../pdf/tts_paper/Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Style Tokens- Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Style Tokens- Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">PREDICTING EXPRESSIVE SPEAKING STYLE FROM TEXT IN END-TO-END SPEECH SYNTHESIS.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/PREDICTING EXPRESSIVE SPEAKING STYLE FROM TEXT IN END-TO-END SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
            </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-5">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Voice Conversion</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">EMOCAT: LANGUAGE-AGNOSTIC EMOTIONAL VOICE CONVERSION</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.05695.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Building Multilingual TTS using Cross-Lingual Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.14039.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">High-Quality Vocoding Design with Signal Processing for Speech Synthesis and Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.10278.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Hierarchical disentangled representation learning for singing voice conversio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.06842.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Adversarially learning disentangled speech representations for robust multi-factor voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00184.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Towards Natural and Controllable Cross-Lingual Voice Conversion Based on Neural TTS Model and Phonetic Posteriorgram</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.01991.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Investigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.11420.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">crank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.02858.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">MaskCycleGAN-VC: Learning Non-parallel Voice Conversion with Filling in Frames</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.12841.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Axial Residual Networks for CycleGAN-based Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.08075.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">Improving Zero-shot Voice Style Transfer via Disentangled Representation Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.09420.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">CycleDRUMS: Automatic Drum Arrangement For Bass Lines Using CycleGAN</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00353.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00931.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.02901.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">StarGAN-based Emotional Voice Conversion for Japanese Phrases</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01807.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">NoiseVC: Towards High Quality Zero-Shot Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06074.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">Non-autoregressive sequence-to-sequence voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06793.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">FastS2S-VC: Streaming Non-Autoregressive Sequence-to-Sequence Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06900.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">NoiseVC: Towards High Quality Zero-Shot Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06074.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Non-autoregressive sequence-to-sequence voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.06793.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Building Bilingual and Code-Switched Voice Conversion with Limited
Training Data Using Embedding Consistency Loss</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.10832.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Towards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.07283.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">An Adaptive Learning based Generative Adversarial Network for One-To-One Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.12159.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.09856.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.01786.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.13871.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Emotional Voice Conversion: Theory, Databases and ESD</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.14762.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.13479.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.01415.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Enriching Source Style Transfer in Recognition-Synthesis based Non-Parallel Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08741.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Improving robustness of one-shot voice conversion with deep discriminative speaker encoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10406.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">32</a></td>
                            <td width="800">NVC-Net: End-to-End Adversarial Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.00992.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">33</a></td>
                            <td width="800">Pathological voice adaptation with autoencoder-based voice conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08427.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">34</a></td>
                            <td width="800">Voicy: Zero-Shot Non-Parallel Voice Conversion in Noisy Reverberant Environments</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08873.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">35</a></td>
                            <td width="800">VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10132.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">36</a></td>
                            <td width="800">StarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for Natural-Sounding Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.10394.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">37</a></td>
                            <td width="800">On Prosody Modeling for ASR+TTS based Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.09477.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">38</a></td>
                            <td width="800">An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.08361.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">39</a></td>
                            <td width="800">Many-to-Many Voice Conversion based Feature Disentanglement using Variational Autoencoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.06642.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">40</a></td>
                            <td width="800">Expressive Voice Conversion: A Joint Framework for Speaker Identity and Emotional Style Transfer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.03748.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">41</a></td>
                            <td width="800">Improving robustness of one-shot voice conversion with deep discriminative speaker encoder</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.10406.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">42</a></td>
                            <td width="800">StarGAN-VC+ASR: StarGAN-based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.04395.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">43</a></td>
                            <td width="800">Unet-TTS: Improving Unseen Speaker and Style Transfer in One-shot Voice Cloning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.11115.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">44</a></td>
                            <td width="800">Noisy-to-Noisy Voice Conversion Framework with Denoising Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.10608.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">45</a></td>
                            <td width="800">Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.03551.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">46</a></td>
                            <td width="800">Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.13821.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">47</a></td>
                            <td width="800">Decoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.01164.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">48</a></td>
                            <td width="800">MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02500.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">49</a></td>
                            <td width="800">S3PRL-VC: Open-source Voice Conversion Framework with Self-supervised Speech Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06280.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">50</a></td>
                            <td width="800">Sequence-To-Sequence Voice Conversion using F0 and Time Conditioning and Adversarial Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03744.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">51</a></td>
                            <td width="800">Speech Enhancement-assisted Stargan Voice Conversion in Noisy Environments</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09923.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">52</a></td>
                            <td width="800">Toward Degradation-Robust Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07537.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">53</a></td>
                            <td width="800">Towards Identity Preserving Normal to Dysarthric Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08213.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">54</a></td>
                            <td width="800">A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.02392.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">55</a></td>
                            <td width="800">AC-VC: Non-parallel Low Latency Phonetic Posteriorgrams Based Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.06601.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">56</a></td>
                            <td width="800">Attention-Guided Generative Adversarial Network for Whisper to Normal Speech Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.01342.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">57</a></td>
                            <td width="800">CycleTransGAN-EVC: A CycleGAN-based Emotional Voice Conversion Model with Transformer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.15159.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">58</a></td>
                            <td width="800">Direct Noisy Speech Modeling for Noisy-to-Noisy Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07116.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">59</a></td>
                            <td width="800">One-shot Voice Conversion For Style Transfer Based On Speaker Adaptation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.12277.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">60</a></td>
                            <td width="800">SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.03811.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data</td>
                            <td width="200"><a href="../pdf/tts_paper/Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">An Overview of Voice Conversion and its Challenges: From Statistical Modeling to Deep Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2008.03648.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Converting Anyone’s Emotion- Towards Speaker-Independent Emotional Voice Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Converting Anyone’s Emotion- Towards Speaker-Independent Emotional Voice Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">SEEN AND UNSEEN EMOTIONAL STYLE TRANSFER FOR VOICE CONVERSION WITH A NEW EMOTIONAL SPEECH DATASET</td>
                            <td width="200"><a href="../pdf/tts_paper/SEEN AND UNSEEN EMOTIONAL STYLE TRANSFER FOR VOICE CONVERSION WITH A NEW EMOTIONAL SPEECH DATASET.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">ANY-TO-ONE SEQUENCE-TO-SEQUENCE VOICE CONVERSION USING SELF-SUPERVISED DISCRETE SPEECH REPRESENTATIONS</td>
                            <td width="200"><a href="../pdf/tts_paper/ANY-TO-ONE SEQUENCE-TO-SEQUENCE VOICE CONVERSION USING SELF-SUPERVISED DISCRETE SPEECH REPRESENTATIONS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">GAZEV- GAN-Based Zero-Shot Voice Conversion over Non-parallel Speech Corpus</td>
                            <td width="200"><a href="../pdf/tts_paper/GAZEV- GAN-Based Zero-Shot Voice Conversion over Non-parallel Speech Corpus.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">TOWARDS LOW-RESOURCE STARGAN VOICE CONVERSION USING WEIGHT ADAPTIVE INSTANCE NORMALIZATION</td>
                            <td width="200"><a href="../pdf/tts_paper/TOWARDS LOW-RESOURCE STARGAN VOICE CONVERSION USING WEIGHT ADAPTIVE INSTANCE NORMALIZATION.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">CycleGAN-VC3- Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/CycleGAN-VC3- Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Accent and Speaker Disentanglement in Many-to-many Voice Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.08609.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">AUTOVC- Zero-Shot Voice Style Transfer with Only Autoencoder Loss</td>
                            <td width="200"><a href="../pdf/tts_paper/AUTOVC- Zero-Shot Voice Style Transfer with Only Autoencoder Loss.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">An Overview of Voice Conversion Systems</td>
                            <td width="200"><a href="../pdf/tts_paper/An Overview of Voice Conversion Systems.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Non-Parallel Sequence-to-Sequence Voice Conversion with Disentangled Linguistic and Speaker Representations</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1906.10508.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">An Overview of Voice Conversion Systems</td>
                            <td width="200"><a href="../pdf/tts_paper/An Overview of Voice Conversion Systems.pdf">pdf</a></td>
                        </tr>
            </table>


        </div>
    </div>
</section>
<section class="item card-box" id="row-6">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Sing Synthesis</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Anyone GAN Sing</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.11058.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Latent Space Explorations of Singing Voice Synthesis using DDSP</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.07197.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Learning to Generate Music With Sentiment</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.06125.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Hierarchical disentangled representation learning for singing voice conversio</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.06842.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</a></td>
                            <td width="800">Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.12292.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">DiffSinger: Diffusion Acoustic Model for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.02446.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">LoopNet: Musical Loop Synthesis Conditioned On Intuitive Musical Parameters</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.10371.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.12292.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Music Generation using Deep Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.09046.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">MLP Singer: Towards Rapid Parallel Korean Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.07886.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">N-Singer: A Non-Autoregressive Korean Singing Voice Synthesis System for Pronunciation Enhancement</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.15205.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.02776.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">A Unified Model for Zero-shot Music Source Separation, Transcription and Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.03456.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">An Empirical Study on End-to-End Singing Voice Synthesis with Encoder-Decoder Architectures</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.03008.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">A Melody-Unsupervision Model for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06546.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02511.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">DeepA: A Deep Neural Analyzer For Speech And Singing Vocoding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06434.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Enhanced Memory Network: The novel network structure for Symbolic Music Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.03392.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE using Mel-spectrograms</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04005.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">KaraTuner: Towards end to end natural pitch correction for singing voice in karaoke</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09121.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Pitch Preservation In Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.05033.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.07468.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">Towards High-fidelity Singing Voice Conversion with Acoustic Reference and Contrastive Predictive Coding</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.04754.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">A Melody-Unsupervision Model for Singing Voice Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.06546.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02511.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">A-Muze-Net: Music Generation by Composing the Harmony based on the Generated Melody</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.12986.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">Learning To Generate Piano Music With Sustain Pedals</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.01216.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Rapping-Singing Voice Synthesis based on Phoneme-level Prosody Control</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09146.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">Symbolic Music Loop Generation with VQ-VAE</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07657.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Video Background Music Generation with Controllable Music Transformer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.08380.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">31</a></td>
                            <td width="800">Zero-shot Singing Technique Conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.08839.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">HIFISINGER TOWARDS HIGH FIDELITY NEURAL SINGING VOICE SYNTHESIS</td>
                            <td width="200"><a href="../pdf/tts_paper/HIFISINGER TOWARDS HIGH FIDELITY NEURAL SINGING VOICE SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">ByteSing A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder Decoder Acoustic Models and WaveRNN Vocoders</td>
                            <td width="200"><a href="../pdf/tts_paper/ByteSing A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder Decoder Acoustic Models and WaveRNN Vocoders.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">DurIAN SC Duration Informed Attention Network based Singing Voice Conversion System</td>
                            <td width="200"><a href="../pdf/tts_paper/DurIAN SC Duration Informed Attention Network based Singing Voice Conversion System.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Jukebox A Generative Model for Music</td>
                            <td width="200"><a href="../pdf/tts_paper/Jukebox A Generative Model for Music.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">XiaoiceSing- A High-Quality and Integrated Singing Voice Synthesis System</td>
                            <td width="200"><a href="../pdf/tts_paper/XiaoiceSing- A High-Quality and Integrated Singing Voice Synthesis System.pdf">pdf</a></td>
                        </tr>
                            <td width="150" align="center">6</td>
                            <td width="800">Speech-to-Singing Conversion based on Boundary Equilibrium GAN</td>
                            <td width="200"><a href="../pdf/tts_paper/Speech-to-Singing Conversion based on Boundary Equilibrium GAN.pdf">pdf</a></td>
                        </tr>
                        </tr>
                            <td width="150" align="center">7</td>
                            <td width="800">A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2011.06801.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">MELLOTRON- MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS</td>
                            <td width="200"><a href="../pdf/tts_paper/MELLOTRON- MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS.pdf">pdf</a></td>
                        </tr>
            </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-7">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Talking Head</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Generating coherent spontaneous speech and gesture from text</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.05684.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Creating Song From Lip and Tongue Videos With a Convolutional Vocoder</td>
                            <td width="200"><a href="https://www.researchgate.net/publication/348439109_Creating_Song_From_Lip_and_Tongue_Videos_With_a_Convolutional_Vocoder">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">SPEAK WITH YOUR HANDS Using Continuous Hand Gestures to control Articulatory Speech Synthesizer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.01640.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">What is Multimodality?</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.06304.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.08223.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</td>
                            <td width="800">Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.10299.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Text2Video: Text-driven Talking-head Video Synthesis with Phonetic Dictionary</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.14631.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Recent Advances and Trends in Multimodal Deep Learning: A Review</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.11087.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">Rethinking the constraints of multimodal fusion: case study in Weakly-Supervised Audio-Visual Video Parsing</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.14430.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.12306.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.04185.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">NWT: Towards natural audio-to-video generation with representation learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.04283.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.14014.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.09293.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">A Survey on Audio Synthesis and Audio-Visual Multimodal Processing</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.00443.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Integrated Speech and Gesture Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.11436.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.04325.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.08020.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.10595.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Audio-to-Image Cross-Modal Generation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.13354.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08580.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">Talking Head Generation with Audio and Speech Related Facial Action Units</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09951.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">LiMuSE: Lightweight Multi-modal Speaker Extraction</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.04063.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">Metric-based multimodal meta-learning for human movement identification via footstep recognition</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.07979.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">What comprises a good talking head video generation? A Survey and Benchmark</td>
                            <td width="200"><a href="../pdf/tts_paper/What comprises a good talking head video generation? A Survey and Benchmark.pdf">pdf</a>
                            &nbsp;&nbsp;<a href="https://github.com/lelechen63/talking-head-generation-survey">code</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models</td>
                            <td width="200"><a href="../pdf/tts_paper/A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Large-scale multilingual audio visual dubbing</td>
                            <td width="200"><a href="../pdf/tts_paper/Large-scale multilingual audio visual dubbing.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">(talking head) Text-based Editing of Talking-head Video</td>
                            <td width="200"><a href="../pdf/tts_paper/Text-based Editing of Talking-head Video.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://www.ohadf.com/projects/text-based-editing/">vedio</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</td>
                            <td width="200"><a href="../pdf/tts_paper/Talking Face Generation by Adversarially Disentangled Audio-Visual Representation.pdf">pdf</a>
                                &nbsp;&nbsp;<a href="https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS">code</a>
                                &nbsp;&nbsp;<a href="https://www.youtube.com/watch?v=-J2zANwdjcQ">demo</a>
                            </td>
                        </tr>
             </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-8">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Robust TTS</strong>
            </div>
            <!--获取内容列表-->

            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS</td>
                            <td width="200"><a href="../pdf/tts_paper/Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Noise Robust TTS for Low Resource Speakers using Pre-trained Model and Speech Enhancement</td>
                            <td width="200"><a href="../pdf/tts_paper/Noise Robust TTS for Low Resource Speakers using Pre-trained Model and Speech Enhancement.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial Training</td>
                            <td width="200"><a href="../pdf/tts_paper/Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial Training.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Neural Text to Speech Adaptation from Low Quality Public Recordings</td>
                            <td width="200"><a href="../pdf/tts_paper/Neural Text to Speech Adaptation from Low Quality Public Recordings.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization</td>
                            <td width="200"><a href="../pdf/tts_paper/32102.pdf">pdf</a></td>
                        </tr>
            </table>
        </div>
    </div>
</section>

</section>
<section class="item card-box" id="row-9">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Front End</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021</h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised Learning</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00621.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Grapheme-to-Phoneme Transformer Model for Transfer Learning Dialects</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.04091.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Proteno: Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.07777.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">Phrase break prediction with bidirectional encoder representations in Japanese text-to-speech synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.12395.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">A Unified Transformer-based Framework for Duplex Text Normalization</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.09889.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">A UNIFIED SEQUENCE-TO-SEQUENCE FRONT-END MODEL FOR MANDARIN TEXT-TO-SPEECH SYNTHESIS.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/A UNIFIED SEQUENCE-TO-SEQUENCE FRONT-END MODEL FOR MANDARIN TEXT-TO-SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">A HYBRID TEXT NORMALIZATION SYSTEM USING MULTI-HEAD SELF-ATTENTION FOR MANDARIN.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/A HYBRID TEXT NORMALIZATION SYSTEM USING MULTI-HEAD SELF-ATTENTION FOR MANDARIN.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">A Mask-based Model for Mandarin Chinese Polyphone Disambiguation</td>
                            <td width="200"><a href="../pdf/tts_paper/A Mask-based Model for Mandarin Chinese Polyphone Disambiguation.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Unified Mandarin TTS Front-end Based on Distilled BERT Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2012.15404.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">A Mandarin Prosodic Boundary Prediction Model Based on Multi Task Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/A Mandarin Prosodic Boundary Prediction Model Based on Multi Task Learning.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Token Level Ensemble Distillation for Grapheme to Phoneme Conversion</td>
                            <td width="200"><a href="../pdf/tts_paper/Token Level Ensemble Distillation for Grapheme to Phoneme Conversion.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Pre trained Text Representations for Improving Front End Text Processing in Mandarin Text to Speech Synthesis</td>
                            <td width="200"><a href="../pdf/tts_paper/Pre trained Text Representations for Improving Front End Text Processing in Mandarin Text to Speech Synthesis.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Mandarin Prosody Prediction Based on Attention Mechanism and Multi-model Ensemble</td>
                            <td width="200"><a href="../pdf/tts_paper/Mandarin Prosody Prediction Based on Attention Mechanism and Multimodel Ensemble.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2016 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Improving Prosodic Boundaries Prediction for Mandarin Speech Synthesis by Using Enhanced Embedding Feature and Model Fusion Approach</td>
                            <td width="200"><a href="../pdf/tts_paper/Improving Prosodic Boundaries Prediction for Mandarin Speech Synthesis by Using Enhanced Embedding Feature and Model Fusion Approach.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2015 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">AUTOMATIC PROSODY PREDICTION FOR CHINESE SPEECH SYNTHESIS USING BLSTM-RNN AND EMBEDDING FEATURES</td>
                            <td width="200"><a href="../pdf/tts_paper/AUTOMATIC PROSODY PREDICTION FOR CHINESE SPEECH SYNTHESIS USING BLSTM-RNN AND EMBEDDING FEATURES.pdf">pdf</a></td>
                        </tr>
            </table>
        </div>
    </div>
</section>

<section class="item card-box" id="row-10">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Alignment</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Triple M: A Practical Neural Text-to-speech System With Multi-guidance Attention And Multi-band Multi-time Lpcnet</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2102.00247.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Multi-rate attention architecture for fast streamable Text-to-speech spectrum modeling</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.00705.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">LOCATION-RELATIVE ATTENTION MECHANISMS FOR ROBUST LONG-FORM SPEECH SYNTHESIS</td>
                            <td width="200"><a href="https://arxiv.org/pdf/1910.10288.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Attentron- Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding</td>
                            <td width="200"><a href="../pdf/tts_paper/Attentron- Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</a></td>
                            <td width="800">Peking Opera Synthesis via Duration Informed Attention Network</td>
                            <td width="200"><a href="../pdf/tts_paper/Peking Opera Synthesis via Duration Informed Attention Network.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</a></td>
                            <td width="800">Understanding Self-Attention of Self-Supervised Audio Transformers</td>
                            <td width="200"><a href="../pdf/tts_paper/Understanding Self-Attention of Self-Supervised Audio Transformers.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2019 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Initial investigation of an encoder-decoder end-to-end TTS framework using marginalization of monotonic hard latent alignments</td>
                            <td width="200"><a href="../pdf/tts_paper/Initial investigation of an encoder-decoder end-to-end TTS framework using marginalization of monotonic hard latent alignments.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS </td>
                            <td width="200"><a href="https://arxiv.org/pdf/1906.00672.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">MONOTONIC CHUNKWISE ATTENTION</td>
                            <td width="200"><a href="../pdf/tts_paper/MONOTONIC CHUNKWISE ATTENTION.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">FORWARD ATTENTION IN SEQUENCE-TO-SEQUENCE ACOUSTIC MODELING FOR SPEECH SYNTHESIS.pdf</td>
                            <td width="200"><a href="../pdf/tts_paper/FORWARD ATTENTION IN SEQUENCE-TO-SEQUENCE ACOUSTIC MODELING FOR SPEECH SYNTHESIS.pdf">pdf</a></td>
                        </tr>
            </table>
            <h3> 2017 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</a></td>
                            <td width="800">Online and Linear-Time Attention by Enforcing Monotonic Alignments</td>
                            <td width="200"><a href="../pdf/tts_paper/Online and Linear-Time Attention by Enforcing Monotonic Alignments.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</a></td>
                            <td width="800">Attention Is All You Need</td>
                            <td width="200"><a href="../pdf/tts_paper/Attention Is All You Need">pdf</a></td>
                        </tr>
            </table>

        </div>
    </div>
</section>
<section class="item card-box" id="row-11">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Dual Learning</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Exploring Machine Speech Chain for Domain Adaptation and Few-Shot Speaker Adaptation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.03815.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">LRSpeech- Extremely Low-Resource Speech Synthesis and Recognition</td>
                            <td width="200"><a href="../pdf/tts_paper/LRSpeech- Extremely Low-Resource Speech Synthesis and Recognition.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Almost Unsupervised Text to Speech and Automatic Speech Recognition</td>
                            <td width="200"><a href="../pdf/tts_paper/Almost Unsupervised Text to Speech and Automatic Speech Recognition.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
            <h3> 2018 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Machine Speech Chain with One-shot Speaker Adaptation</td>
                            <td width="200"><a href="../pdf/tts_paper/Machine Speech Chain with One-shot Speaker Adaptation.pdf">pdf</a>
                            </td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Listening while Speaking- Speech Chain by Deep Learning</td>
                            <td width="200"><a href="../pdf/tts_paper/Listening while Speaking- Speech Chain by Deep Learning.pdf">pdf</a>
                            </td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-12">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>EEG</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">On Interfacing the Brain with Quantum Computers: An Approach to Listen to the Logic of the Mind</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2101.03887.pdf">pdf</a></td>
                        </tr>
             </table>
            <h3> 2020 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Advancing Speech Synthesis using EEG</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2004.04731.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center"2>2</td>
                            <td width="800">Speech Synthesis using EEG</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2002.12756.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Predicting Different Acoustic Features from EEG and towards direct synthesis of Audio Waveform from EEG</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2006.01262.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-13">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>S2S</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">Assessing Evaluation Metrics for Speech-to-Speech Translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.13877.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Direct simultaneous speech to speech translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08250.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">Incremental Speech Synthesis For Speech-To-Speech Translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.08214.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>
<section class="item card-box" id="row-14">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Other</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
<section class="item card-box" id="row-13">
    <div class="container-fluid">
        <div class="row">
            <div class="item-tit">
                <strong>Other</strong>
            </div>
            <!--获取内容列表-->
            <h3> 2021 </h3>
            <table width="1150" border="1">
                        <tr>
                            <td width="150" align="center">1</td>
                            <td width="800">MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2103.00110.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">2</td>
                            <td width="800">Hi-Fi Multi-Speaker English TTS Dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.01497.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">3</td>
                            <td width="800">ProsoBeast Prosody Annotation Tool</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.02397.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">4</td>
                            <td width="800">KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.08459.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">5</td>
                            <td width="800">Deep Learning Based Assessment of Synthetic Speech Naturalness</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.11673.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">6</a></td>
                            <td width="800">Deep Learning Based Assessment of Synthetic Speech Naturalness</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2104.11673.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">7</a></td>
                            <td width="800">Speaker disentanglement in video-to-speech conversion</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.09652.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">8</a></td>
                            <td width="800">Voice of Your Brain: Cognitive Representations of Imagined Speech,Overt Speech, and Speech Perception Based on EEG</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2105.14787.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">9</a></td>
                            <td width="800">ADEPT: A Dataset for Evaluating Prosody Transfer</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08321.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">10</a></td>
                            <td width="800">EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.09317.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">11</a></td>
                            <td width="800">HUI-Audio-Corpus-German: A high quality TTS dataset</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.06309.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">12</a></td>
                            <td width="800">Mixtures of Deep Neural Experts for Automated Speech Scoring</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.12475.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">13</a></td>
                            <td width="800">RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2106.08468.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">14</a></td>
                            <td width="800">Adaptation of Tacotron2-based Text-To-Speech for Articulatory-to-Acoustic Mapping using Ultrasound Tongue Imaging</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.12051.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">15</a></td>
                            <td width="800">Extending Text-to-Speech Synthesis with Articulatory Movement Prediction using Ultrasound Tongue Imaging</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.05550.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">16</a></td>
                            <td width="800">Speech Synthesis from Text and Ultrasound Tongue Image-based Articulatory Input</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.02003.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">17</a></td>
                            <td width="800">An Objective Evaluation Framework for Pathological Speech Synthesis</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.00308.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">18</a></td>
                            <td width="800">Digital Einstein Experience: Fast Text-to-Speech for Conversational AI</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.10658.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">19</a></td>
                            <td width="800">Translatotron 2: Robust direct speech-to-speech translation</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.08661.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">20</a></td>
                            <td width="800">Direct speech-to-speech translation with discrete units</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2107.05604.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">21</a></td>
                            <td width="800">Fighting Game Commentator with Pitch and Loudness Adjustment Utilizing Highlight Cues</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.08112.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">22</a></td>
                            <td width="800">RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2108.05684.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">23</a></td>
                            <td width="800">"Hello, It's Me": Deep Learning-based Speech Synthesis Attacks in the Real World</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2109.09598.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">24</a></td>
                            <td width="800">FMFCC-A: A Challenging Mandarin Dataset for Synthetic Speech Detection</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09441.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">25</a></td>
                            <td width="800">AQP: An Open Modular Python Platform for Objective Speech and Audio Quality Metrics</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.13589.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">26</a></td>
                            <td width="800">Generalization Ability of MOS Prediction Networks</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.02635.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">27</a></td>
                            <td width="800">LDNet: Unified Listener Dependent Modeling in MOS Prediction for Synthetic Speech</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.09103.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">28</a></td>
                            <td width="800">Objective Measures of Perceptual Audio Quality Reviewed: An Evaluation of Their Application Domain Dependence</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2110.11438.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">29</a></td>
                            <td width="800">How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.14203.pdf">pdf</a></td>
                        </tr>
                        <tr>
                            <td width="150" align="center">30</a></td>
                            <td width="800">Cross-lingual Low Resource Speaker Adaptation Using Phonological Features</td>
                            <td width="200"><a href="https://arxiv.org/pdf/2111.09075.pdf">pdf</a></td>
                        </tr>
             </table>
        </div>
    </div>
</section>

<!--页脚-->
<footer class="footer">
    <div class="container">
        <div class="rwo">
            <div class="col-md-12">
                <p>
                    本站内容源自互联网，如有内容侵犯了你的权益，请联系删除相关内容，联系邮箱：yongqiangli@alumni.hust.edu.cn
                </p>
                <!--代码源自小呆导航的开源代码，遵循MIT协议，此处保留源代码的声明-->
                <p>
                    Copyright © 2018-2021 li yongqiang All Rights Reserved
                </p>
            </div>
        </div>
    </div>
</footer>
</div>
<!--内容区域-->
</div>
<div id="get-top" title="回到顶部">
    <i class="icon icon-arrow-up"></i>
</div>

<!-- jQuery (ZUI中的Javascript组件依赖于jQuery) -->
<script src="http://code.jquery.com/jquery-1.11.0.min.js"></script>

<script>
    window.onscroll = function(){
//回到顶部
var sllTop = document.documentElement.scrollTop||document.body.scrollTop;
if(sllTop>240){
  $('#get-top').css('display','block')
}else{
  $('#get-top').css('display','none')
}
}
$('#get-top').click(function(){ 
  $('body,html').animate({
    scrollTop: 0
  }, 800);//点击回到顶部按钮，数字越小越快
})
//判断用户使用的设备
var deviceVal  = browserRedirect();
function browserRedirect() {
  var sUserAgent = navigator.userAgent.toLowerCase();
  var bIsIpad = sUserAgent.match(/ipad/i) == "ipad";
  var bIsIphoneOs = sUserAgent.match(/iphone os/i) == "iphone os";
  var bIsMidp = sUserAgent.match(/midp/i) == "midp";
  var bIsUc7 = sUserAgent.match(/rv:1.2.3.4/i) == "rv:1.2.3.4";
  var bIsUc = sUserAgent.match(/ucweb/i) == "ucweb";
  var bIsAndroid = sUserAgent.match(/android/i) == "android";
  var bIsCE = sUserAgent.match(/windows ce/i) == "windows ce";
  var bIsWM = sUserAgent.match(/windows mobile/i) == "windows mobile";
  if (bIsIpad || bIsIphoneOs || bIsMidp || bIsUc7 || bIsUc || bIsAndroid || bIsCE || bIsWM) {
    return 'phone';
} else {
    return 'pc';
}
}
$('.nav-btn').on('click', function () {
    $('.nav').toggleClass('showNav');
    $(this).toggleClass('animated2');
});

</script>
</div>
</body>
</html>
